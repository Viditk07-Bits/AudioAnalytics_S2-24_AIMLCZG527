{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viditk07-Bits/AudioAnalytics_S2-24_AIMLCZG527/blob/main/AA_Assignment2_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EN9uWs6-RQ"
      },
      "source": [
        "# Music Information Retrieval System\n",
        "\n",
        "## Assignment Objective\n",
        "This assignment implements a comprehensive Music Information Retrieval (MIR) system using Large Language Models (LLMs) and deep learning techniques. It includes music recommendation, genre classification, and semantic search applications, combining audio analysis with natural language processing.\n",
        "\n",
        "## Dataset Setup\n",
        "Using the Free Music Archive (FMA) dataset with audio files, metadata, and synthetic user data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task A1, A2, A3, A5, C5: Music Genre Classification, Recommendation, and Retrieval\n",
        "# Implements audio and text-based genre classification, transformer-based audio classification,\n",
        "# content-based music discovery, personalized recommendation, and comprehensive evaluation.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, average_precision_score, ndcg_score\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import hashlib\n",
        "import subprocess\n",
        "import logging\n",
        "import random\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from datasets import Dataset  # Added for fine-tuning\n",
        "\n",
        "# Setup logging for detailed debugging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/fma/data\"\n",
        "AUDIO_PATH = os.path.join(DATA_PATH, \"audio_files\")\n",
        "METADATA_PATH = os.path.join(DATA_PATH, \"metadata/tracks.csv\")\n",
        "ARTISTS_PATH = os.path.join(DATA_PATH, \"metadata/artists.csv\")\n",
        "GENRES_PATH = os.path.join(DATA_PATH, \"metadata/genres.csv\")\n",
        "LYRICS_PATH = os.path.join(DATA_PATH, \"lyrics\")\n",
        "USER_DATA_PATH = os.path.join(DATA_PATH, \"user_data/ratings.csv\")\n",
        "TAGS_PATH = os.path.join(DATA_PATH, \"descriptions/tags.csv\")\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "TEMP_DIR = \"/content/fma\"\n",
        "NUM_EPOCHS_REC = 5\n",
        "NUM_EPOCHS_CLS = 10\n",
        "BATCH_SIZE = 32\n",
        "MAX_TRACKS = 500\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def setup_fma_dataset():\n",
        "    \"\"\"Task A1, A3, A5: Download and set up FMA dataset with tracks, genres, ratings, and synthetic lyrics.\"\"\"\n",
        "    print(\"=== Dataset Setup ===\")\n",
        "    logging.info(\"Starting dataset setup...\")\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "    os.makedirs(DATA_PATH, exist_ok=True)\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n",
        "    os.makedirs(LYRICS_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(USER_DATA_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(TAGS_PATH), exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        os.chdir(TEMP_DIR)\n",
        "        logging.info(\"Changed to TEMP_DIR: %s\", TEMP_DIR)\n",
        "        if not os.path.exists(os.path.join(TEMP_DIR, \"fma\")):\n",
        "            logging.info(\"Cloning FMA repository...\")\n",
        "            print(\"Cloning FMA repository...\")\n",
        "            subprocess.run([\"git\", \"clone\", \"https://github.com/mdeff/fma.git\"], check=True, capture_output=True, text=True)\n",
        "        os.chdir(os.path.join(TEMP_DIR, \"fma\"))\n",
        "        logging.info(\"Changed to FMA directory\")\n",
        "\n",
        "        # Define paths for FMA dataset files\n",
        "        fma_small_zip = \"fma_small.zip\"\n",
        "        fma_metadata_zip = \"fma_metadata.zip\"\n",
        "\n",
        "        # Check if files already exist to avoid redundant downloads\n",
        "        if os.path.exists(fma_small_zip):\n",
        "            logging.info(\"fma_small.zip already exists, skipping download.\")\n",
        "            print(\"fma_small.zip already exists, skipping download.\")\n",
        "        else:\n",
        "            logging.info(\"Downloading fma_small.zip...\")\n",
        "            print(\"Downloading fma_small.zip...\")\n",
        "            subprocess.run([\"wget\", \"-O\", fma_small_zip, \"https://os.unil.cloud.switch.ch/fma/fma_small.zip\"], check=True, capture_output=True, text=True)\n",
        "\n",
        "        if os.path.exists(fma_metadata_zip):\n",
        "            logging.info(\"fma_metadata.zip already exists, skipping download.\")\n",
        "            print(\"fma_metadata.zip already exists, skipping download.\")\n",
        "        else:\n",
        "            logging.info(\"Downloading fma_metadata.zip...\")\n",
        "            print(\"Downloading fma_metadata.zip...\")\n",
        "            subprocess.run([\"wget\", \"-O\", fma_metadata_zip, \"https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\"], check=True, capture_output=True, text=True)\n",
        "\n",
        "        # Verify checksums\n",
        "        def sha1_checksum(file_path):\n",
        "            sha1 = hashlib.sha1()\n",
        "            with open(file_path, 'rb') as f:\n",
        "                while chunk := f.read(8192):\n",
        "                    sha1.update(chunk)\n",
        "            return sha1.hexdigest()\n",
        "\n",
        "        logging.info(\"Verifying checksums...\")\n",
        "        print(\"Verifying checksums...\")\n",
        "        if sha1_checksum(fma_small_zip) != \"ade154f733639d52e35e32f5593efe5be76c6d70\":\n",
        "            logging.warning(\"fma_small.zip checksum failed. Proceeding anyway.\")\n",
        "        if sha1_checksum(fma_metadata_zip) != \"f0df49ffe5f2a6008d7dc83c6915b31835dfe733\":\n",
        "            logging.warning(\"fma_metadata.zip checksum failed. Proceeding anyway.\")\n",
        "\n",
        "        # Unzip datasets\n",
        "        if not os.path.exists(os.path.join(DATA_PATH, \"fma_small\")):\n",
        "            logging.info(\"Unzipping fma_small.zip...\")\n",
        "            print(\"Unzipping fma_small.zip...\")\n",
        "            try:\n",
        "                subprocess.run([\"unzip\", \"-q\", fma_small_zip, \"-d\", DATA_PATH], check=True, capture_output=True, text=True)\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                logging.error(\"Unzip fma_small.zip failed: %s\", e.stderr)\n",
        "                raise\n",
        "        if not os.path.exists(os.path.join(DATA_PATH, \"fma_metadata\")):\n",
        "            logging.info(\"Unzipping fma_metadata.zip...\")\n",
        "            print(\"Unzipping fma_metadata.zip...\")\n",
        "            try:\n",
        "                subprocess.run([\"unzip\", \"-q\", fma_metadata_zip, \"-d\", DATA_PATH], check=True, capture_output=True, text=True)\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                logging.error(\"Unzip fma_metadata.zip failed: %s\", e.stderr)\n",
        "                raise\n",
        "\n",
        "        # Move MP3 files to AUDIO_PATH if not already present\n",
        "        logging.info(\"Checking and moving MP3 files...\")\n",
        "        print(\"Checking and moving MP3 files...\")\n",
        "        mp3_count = 0\n",
        "        existing_mp3s = set(f.name for f in Path(AUDIO_PATH).glob(\"*.mp3\"))\n",
        "        mp3_files = list(Path(DATA_PATH).rglob(\"*.mp3\"))\n",
        "        logging.info(\"MP3 files found: %s\", [str(f) for f in mp3_files[:10]])\n",
        "        for mp3_file in mp3_files:\n",
        "            target = os.path.join(AUDIO_PATH, mp3_file.name)\n",
        "            if mp3_file.name not in existing_mp3s:\n",
        "                os.rename(mp3_file, target)\n",
        "                mp3_count += 1\n",
        "        logging.info(\"Moved %d MP3 files.\", mp3_count)\n",
        "        print(f\"Moved {mp3_count} MP3 files.\")\n",
        "        logging.info(\"Audio files after move: %s\", os.listdir(AUDIO_PATH)[:10])\n",
        "\n",
        "        # Process metadata with diverse genres\n",
        "        logging.info(\"Processing metadata...\")\n",
        "        print(\"Processing metadata...\")\n",
        "        tracks = pd.read_csv(os.path.join(DATA_PATH, \"fma_metadata\", \"tracks.csv\"), index_col=0, header=[0, 1])\n",
        "        genres = pd.read_csv(os.path.join(DATA_PATH, \"fma_metadata\", \"genres.csv\"))\n",
        "\n",
        "        df_artists = tracks['artist'][['name']].reset_index().rename(columns={'track_id': 'artist_id', 'name': 'artist_name'})\n",
        "        df_artists['artist_id'] = df_artists['artist_id'].astype(str).str.zfill(6)\n",
        "        df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "        logging.info(\"Created artists.csv\")\n",
        "\n",
        "        df_genres = genres[['genre_id', 'title']].rename(columns={'title': 'genre_name'})\n",
        "        df_genres.to_csv(GENRES_PATH, index=False)\n",
        "        logging.info(\"Created genres.csv\")\n",
        "\n",
        "        # Filter tracks to match available audio files\n",
        "        audio_ids = {f.stem for f in Path(AUDIO_PATH).glob(\"*.mp3\")}\n",
        "        df_tracks = tracks['track'][['title', 'genre_top']].reset_index()\n",
        "        df_tracks['track_id'] = df_tracks['track_id'].astype(str).str.zfill(6)\n",
        "        df_tracks = df_tracks[df_tracks['track_id'].isin(audio_ids)]\n",
        "        df_tracks['artist_id'] = df_tracks['track_id']\n",
        "        df_tracks['genre_id'] = df_tracks['genre_top'].map(df_genres.set_index('genre_name')['genre_id'])\n",
        "        df_tracks = df_tracks.dropna().groupby('genre_top').head(50).head(MAX_TRACKS)\n",
        "        df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "        logging.info(\"Created tracks.csv\")\n",
        "\n",
        "        # Generate synthetic ratings\n",
        "        ratings = pd.DataFrame({\n",
        "            'user_id': [f\"user_{i%10+1}\" for i in range(len(df_tracks))],\n",
        "            'track_id': df_tracks['track_id'],\n",
        "            'rating': np.random.uniform(1, 5, len(df_tracks))\n",
        "        })\n",
        "        ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "        logging.info(\"Created ratings.csv\")\n",
        "\n",
        "        # Generate synthetic tags\n",
        "        pd.DataFrame({\n",
        "            'track_id': df_tracks['track_id'],\n",
        "            'tag': [random.choice(['rock', 'pop', 'jazz', 'hip-hop', 'folk']) for _ in range(len(df_tracks))]\n",
        "        }).to_csv(TAGS_PATH, index=False)\n",
        "        logging.info(\"Created tags.csv\")\n",
        "\n",
        "        # Generate synthetic lyrics\n",
        "        for track_id in df_tracks['track_id']:\n",
        "            with open(os.path.join(LYRICS_PATH, f\"{track_id}.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"Synthetic lyrics for track {track_id} in {df_tracks[df_tracks['track_id'] == track_id]['genre_top'].iloc[0]}\")\n",
        "        logging.info(\"Created synthetic lyrics\")\n",
        "\n",
        "        print(\"Dataset setup completed successfully.\")\n",
        "        print(f\"Tracks shape: {df_tracks.shape}, Genres shape: {df_genres.shape}, Ratings shape: {ratings.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Dataset setup failed: %s. Creating synthetic dataset.\", str(e))\n",
        "        print(f\"Dataset setup failed: {str(e)}. Creating synthetic dataset.\")\n",
        "        create_synthetic_dataset()\n",
        "\n",
        "def create_synthetic_dataset():\n",
        "    \"\"\"Task A1, A3, A5: Create synthetic dataset if FMA download fails.\"\"\"\n",
        "    logging.info(\"Creating synthetic dataset...\")\n",
        "    print(\"Creating synthetic dataset...\")\n",
        "    df_tracks = pd.DataFrame({\n",
        "        'track_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "        'title': [f\"Track_{i}\" for i in range(1, MAX_TRACKS + 1)],\n",
        "        'artist_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "        'genre_id': [random.randint(1, 10) for _ in range(MAX_TRACKS)],\n",
        "        'genre_top': [random.choice(['Rock', 'Pop', 'Jazz', 'Classical', 'Hip-Hop', 'Electronic', 'Folk', 'Blues', 'Country', 'Reggae']) for _ in range(MAX_TRACKS)]\n",
        "    })\n",
        "    df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "\n",
        "    df_artists = pd.DataFrame({\n",
        "        'artist_id': df_tracks['artist_id'],\n",
        "        'artist_name': [f\"Artist_{i}\" for i in range(1, MAX_TRACKS + 1)]\n",
        "    })\n",
        "    df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "\n",
        "    df_genres = pd.DataFrame({\n",
        "        'genre_id': range(1, 11),\n",
        "        'genre_name': ['Rock', 'Pop', 'Jazz', 'Classical', 'Hip-Hop', 'Electronic', 'Folk', 'Blues', 'Country', 'Reggae']\n",
        "    })\n",
        "    df_genres.to_csv(GENRES_PATH, index=False)\n",
        "\n",
        "    ratings = pd.DataFrame({\n",
        "        'user_id': [f\"user_{i%10+1}\" for i in range(len(df_tracks))],\n",
        "        'track_id': df_tracks['track_id'],\n",
        "        'rating': np.random.uniform(1, 5, len(df_tracks))\n",
        "    })\n",
        "    ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "\n",
        "    pd.DataFrame({\n",
        "        'track_id': df_tracks['track_id'],\n",
        "        'tag': [random.choice(['rock', 'pop', 'jazz', 'hip-hop', 'folk']) for _ in range(len(df_tracks))]\n",
        "    }).to_csv(TAGS_PATH, index=False)\n",
        "\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    for track_id in df_tracks['track_id']:\n",
        "        with open(os.path.join(AUDIO_PATH, f\"{track_id}.mp3\"), 'w') as f:\n",
        "            f.write(\"\")\n",
        "\n",
        "    for track_id in df_tracks['track_id']:\n",
        "        with open(os.path.join(LYRICS_PATH, f\"{track_id}.txt\"), 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Synthetic lyrics for track {track_id} in {df_tracks[df_tracks['track_id'] == track_id]['genre_top'].iloc[0]}\")\n",
        "\n",
        "    print(f\"Synthetic dataset created: Tracks shape: {df_tracks.shape}\")\n",
        "    logging.info(\"Synthetic dataset created: Tracks shape: %s\", df_tracks.shape)\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    \"\"\"Task A1.1, A2: Extract audio features (MFCCs, chroma, spectral centroid, tempo) using Librosa.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(audio_path):\n",
        "            logging.warning(\"Audio file %s does not exist.\", audio_path)\n",
        "            return np.zeros(26)\n",
        "        if os.path.getsize(audio_path) < 100:\n",
        "            logging.warning(\"Skipping %s: File too small or empty.\", audio_path)\n",
        "            return np.zeros(26)\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=22050)\n",
        "        except Exception as load_e:\n",
        "            logging.warning(\"Librosa load failed for %s: %s\", audio_path, str(load_e))\n",
        "            return np.zeros(26)\n",
        "        if len(y) == 0:\n",
        "            logging.warning(\"Skipping %s: Empty audio data.\", audio_path)\n",
        "            return np.zeros(26)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        tempo = librosa.feature.tempo(y=y, sr=sr)\n",
        "        tempo = float(tempo[0]) if isinstance(tempo, np.ndarray) else float(tempo)\n",
        "        features = np.concatenate([\n",
        "            np.mean(mfccs, axis=1),\n",
        "            np.mean(chroma, axis=1),\n",
        "            np.mean(spectral_centroid, axis=1),\n",
        "            [tempo]\n",
        "        ])\n",
        "        logging.info(\"Extracted features for %s: %s\", audio_path, features[:5])\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Error processing %s: %s\", audio_path, str(e))\n",
        "        return np.zeros(26)\n",
        "\n",
        "def load_fma_data(audio_path, metadata_path, artists_path, genres_path, lyrics_path, tags_path):\n",
        "    \"\"\"Task A1, A3, A5: Load FMA dataset, process audio, metadata, lyrics, and tags.\"\"\"\n",
        "    logging.info(\"Loading FMA data...\")\n",
        "    print(\"Loading FMA data...\")\n",
        "    if not os.path.exists(metadata_path):\n",
        "        logging.error(\"Metadata not found. Creating synthetic dataset.\")\n",
        "        create_synthetic_dataset()\n",
        "\n",
        "    try:\n",
        "        df_tracks = pd.read_csv(metadata_path)\n",
        "        df_metadata = df_tracks[['track_id', 'title', 'artist_id', 'genre_id']].dropna()\n",
        "        df_artists = pd.read_csv(artists_path)[['artist_id', 'artist_name']]\n",
        "        df_genres = pd.read_csv(genres_path)[['genre_id', 'genre_name']]\n",
        "        df_metadata = pd.merge(df_metadata, df_artists, on='artist_id', how='left')\n",
        "        df_metadata = pd.merge(df_metadata, df_genres, on='genre_id', how='left')\n",
        "        df_metadata = df_metadata[['track_id', 'artist_name', 'title', 'genre_name']].dropna()\n",
        "        df_metadata.columns = ['track_id', 'artist_name', 'title', 'genre']\n",
        "        df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "        logging.info(\"Metadata shape: %s\", df_metadata.shape)\n",
        "        print(f\"Metadata shape: {df_metadata.shape}\")\n",
        "        print(f\"Genre distribution:\\n{df_metadata['genre'].value_counts()}\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading metadata: %s. Using synthetic metadata.\", str(e))\n",
        "        print(f\"Error loading metadata: {str(e)}. Using synthetic metadata.\")\n",
        "        df_metadata = pd.DataFrame({\n",
        "            'track_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "            'artist_name': [f\"Artist_{i}\" for i in range(1, MAX_TRACKS + 1)],\n",
        "            'title': [f\"Track_{i}\" for i in range(1, MAX_TRACKS + 1)],\n",
        "            'genre': [random.choice(['Rock', 'Pop', 'Jazz', 'Classical', 'Hip-Hop', 'Electronic', 'Folk', 'Blues', 'Country', 'Reggae']) for _ in range(MAX_TRACKS)]\n",
        "        })\n",
        "\n",
        "    features = []\n",
        "    audio_files = list(Path(audio_path).glob(\"*.mp3\"))\n",
        "    valid_track_ids = set(df_metadata['track_id'].astype(str).str.zfill(6).tolist())\n",
        "    logging.info(\"Found %d audio files.\", len(audio_files))\n",
        "    print(f\"Found {len(audio_files)} audio files.\")\n",
        "    print(f\"Valid track IDs (first 10): {list(valid_track_ids)[:10]}\")\n",
        "    print(f\"Audio file IDs (first 10): {[f.stem for f in audio_files[:10]]}\")\n",
        "    print(f\"Matching IDs: {len(set([f.stem for f in audio_files]) & valid_track_ids)}\")\n",
        "    valid_audio_count = 0\n",
        "    valid_audio_files = []\n",
        "    for audio_file in audio_files:\n",
        "        track_id = audio_file.stem\n",
        "        if track_id in valid_track_ids:\n",
        "            logging.info(\"Processing audio: %s\", audio_file)\n",
        "            audio_features = extract_audio_features(audio_file)\n",
        "            if not np.all(audio_features == 0):\n",
        "                valid_audio_count += 1\n",
        "                valid_audio_files.append(str(audio_file))\n",
        "            features.append([track_id] + audio_features.tolist())\n",
        "    logging.info(\"Processed %d valid audio files.\", valid_audio_count)\n",
        "    print(f\"Processed {valid_audio_count} valid audio files.\")\n",
        "    if valid_audio_files:\n",
        "        print(f\"Valid audio files (first 5): {valid_audio_files[:5]}\")\n",
        "        logging.info(\"Valid audio files (first 5): %s\", valid_audio_files[:5])\n",
        "\n",
        "    feature_columns = ['track_id'] + [f'mfcc_{i+1}' for i in range(13)] + [f'chroma_{i+1}' for i in range(12)] + ['spectral_centroid', 'tempo']\n",
        "    df_features = pd.DataFrame(features, columns=feature_columns).dropna()\n",
        "    if df_features.empty or valid_audio_count == 0:\n",
        "        logging.warning(\"No valid audio features. Using synthetic features.\")\n",
        "        print(\"No valid audio features. Using synthetic features.\")\n",
        "        df_features = pd.DataFrame({\n",
        "            'track_id': df_metadata['track_id'],\n",
        "            **{col: [0.0] * len(df_metadata) for col in feature_columns[1:]}\n",
        "        })\n",
        "    else:\n",
        "        df_features = df_features[df_features['track_id'].isin(valid_track_ids)]\n",
        "        logging.info(\"Features shape: %s\", df_features.shape)\n",
        "        print(f\"Features shape: {df_features.shape}\")\n",
        "\n",
        "    lyrics_dict = {}\n",
        "    for lyric_file in Path(lyrics_path).glob(\"*.txt\"):\n",
        "        track_id = lyric_file.stem\n",
        "        if track_id in df_metadata['track_id'].values:\n",
        "            try:\n",
        "                with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                    lyrics_dict[track_id] = f.read().strip() or 'music'\n",
        "            except Exception as e:\n",
        "                logging.warning(\"Error reading lyrics %s: %s\", track_id, str(e))\n",
        "                lyrics_dict[track_id] = 'music'\n",
        "\n",
        "    if tags_path and os.path.exists(tags_path):\n",
        "        try:\n",
        "            df_tags = pd.read_csv(tags_path)\n",
        "            for _, row in df_tags.iterrows():\n",
        "                track_id = str(row['track_id']).zfill(6)\n",
        "                if track_id in df_metadata['track_id'].values:\n",
        "                    tag = str(row['tag'])\n",
        "                    lyrics_dict[track_id] = lyrics_dict.get(track_id, '') + \" \" + tag\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error loading tags: %s\", str(e))\n",
        "\n",
        "    if not lyrics_dict:\n",
        "        logging.warning(\"No lyrics found. Using synthetic lyrics.\")\n",
        "        print(\"No lyrics found. Using synthetic lyrics.\")\n",
        "        for track_id in df_metadata['track_id']:\n",
        "            lyrics_dict[track_id] = f\"Synthetic lyrics for {track_id} in {df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]}\"\n",
        "\n",
        "    logging.info(\"Lyrics dict size: %d\", len(lyrics_dict))\n",
        "    print(f\"Lyrics dict size: {len(lyrics_dict)}\")\n",
        "    return df_metadata, df_features, lyrics_dict\n",
        "\n",
        "def generate_text_embeddings(lyrics_dict):\n",
        "    \"\"\"Task A1.2, A3: Generate semantic embeddings for lyrics using Sentence-Transformers.\"\"\"\n",
        "    logging.info(\"Generating text embeddings...\")\n",
        "    print(\"Generating text embeddings...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    embeddings = {}\n",
        "    for track_id, text in lyrics_dict.items():\n",
        "        embeddings[track_id] = model.encode(text, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    logging.info(\"Text embeddings generated for %d tracks.\", len(embeddings))\n",
        "    print(f\"Text embeddings generated for {len(embeddings)} tracks.\")\n",
        "    return embeddings\n",
        "\n",
        "def analyze_linguistic_patterns(df_metadata, lyrics_dict):\n",
        "    \"\"\"Task A1.2: Analyze linguistic patterns across genres (top words per genre).\"\"\"\n",
        "    print(\"\\nLinguistic Analysis: Top 5 words per genre\")\n",
        "    genre_words = {g: [] for g in df_metadata['genre'].unique()}\n",
        "    for track_id, text in lyrics_dict.items():\n",
        "        genre = df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]\n",
        "        genre_words[genre].extend(text.split())\n",
        "    for genre, words in genre_words.items():\n",
        "        top_words = Counter(words).most_common(5)\n",
        "        print(f\"{genre} top words: {top_words}\")\n",
        "        logging.info(\"%s top words: %s\", genre, top_words)\n",
        "\n",
        "class HybridRecommender(nn.Module):\n",
        "    \"\"\"Task A5: Content-based recommender combining audio and text features.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        return torch.sigmoid(self.fc(combined))\n",
        "\n",
        "def train_recommender(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Task A5: Train the content-based recommender.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, ratings in train_loader:\n",
        "        audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, ratings.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_recommender(model, test_loader, df_metadata, test_indices, genres, k=10):\n",
        "    \"\"\"Task C5: Evaluate recommender with Precision@K, MAP, NDCG, and diversity.\"\"\"\n",
        "    model.eval()\n",
        "    precisions, maps, ndcgs, diversities = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, ratings in test_loader:\n",
        "            audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            binary_ratings = (ratings > 0.5).float()\n",
        "            top_k = torch.topk(outputs, k=min(k, outputs.size(0)), dim=0).indices.flatten()\n",
        "            relevant = binary_ratings[top_k]\n",
        "            precision = relevant.mean().item()\n",
        "            precisions.append(precision)\n",
        "            if binary_ratings.sum() > 0:\n",
        "                map_score = average_precision_score(binary_ratings.cpu().numpy(), outputs.cpu().numpy())\n",
        "                ndcg = ndcg_score(binary_ratings.cpu().numpy().reshape(1, -1), outputs.cpu().numpy().reshape(1, -1), k=k)\n",
        "            else:\n",
        "                map_score, ndcg = 0.0, 0.0\n",
        "            maps.append(map_score)\n",
        "            ndcgs.append(ndcg)\n",
        "            top_k_ids = df_metadata.iloc[test_indices].iloc[top_k.cpu()]['track_id']\n",
        "            top_k_genres = df_metadata[df_metadata['track_id'].isin(top_k_ids)]['genre']\n",
        "            diversity = len(set(top_k_genres)) / len(genres) if len(genres) > 0 else 1.0\n",
        "            diversities.append(diversity)\n",
        "    return {\n",
        "        'precision@k': np.mean(precisions) if precisions else 0.0,\n",
        "        'map': np.mean(maps) if maps else 0.0,\n",
        "        'ndcg@k': np.mean(ndcgs) if ndcgs else 0.0,\n",
        "        'diversity': np.mean(diversities) if diversities else 1.0\n",
        "    }\n",
        "\n",
        "def collaborative_filtering(user_data, df_metadata, k=10):\n",
        "    \"\"\"Task A5: Collaborative filtering using NMF.\"\"\"\n",
        "    print(\"\\nTraining Collaborative Filtering Model...\")\n",
        "    # Filter user_data to only include tracks present in df_metadata\n",
        "    valid_track_ids = set(df_metadata['track_id'].astype(str).str.zfill(6))\n",
        "    user_data = user_data[user_data['track_id'].isin(valid_track_ids)]\n",
        "\n",
        "    if user_data.empty:\n",
        "        logging.error(\"No valid user data after filtering. Returning zero metrics.\")\n",
        "        print(\"No valid user data after filtering. Returning zero metrics.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    # Split user data into train/test\n",
        "    train_data, test_data = train_test_split(user_data, test_size=0.2, random_state=42)\n",
        "    logging.info(\"CF data split: train=%d, test=%d\", len(train_data), len(test_data))\n",
        "    print(f\"CF data split: train={len(train_data)}, test={len(test_data)}\")\n",
        "\n",
        "    # Create user-item matrix for training\n",
        "    user_item_matrix = train_data.pivot(index='user_id', columns='track_id', values='rating').fillna(0)\n",
        "    if user_item_matrix.empty:\n",
        "        logging.error(\"Empty user-item matrix. Returning zero metrics.\")\n",
        "        print(\"Empty user-item matrix. Returning zero metrics.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    # Apply NMF\n",
        "    nmf = NMF(n_components=20, random_state=42)\n",
        "    user_features = nmf.fit_transform(user_item_matrix)\n",
        "    item_features = nmf.components_\n",
        "    predictions = np.dot(user_features, item_features)\n",
        "    predicted_ratings = pd.DataFrame(predictions, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
        "\n",
        "    # Create test user-item matrix\n",
        "    test_matrix = test_data.pivot(index='user_id', columns='track_id', values='rating').fillna(0)\n",
        "    logging.info(\"Test matrix tracks: %d, Predicted tracks: %d\", len(test_matrix.columns), len(predicted_ratings.columns))\n",
        "    print(f\"Test matrix tracks: {len(test_matrix.columns)}, Predicted tracks: {len(predicted_ratings.columns)}\")\n",
        "\n",
        "    # Ensure shared tracks\n",
        "    shared_tracks = set(test_matrix.columns).intersection(set(predicted_ratings.columns))\n",
        "    logging.info(\"Shared tracks between test and predicted: %d\", len(shared_tracks))\n",
        "    print(f\"Shared tracks between test and predicted: {len(shared_tracks)}\")\n",
        "\n",
        "    if not shared_tracks:\n",
        "        logging.warning(\"No shared tracks between test and predicted. Returning zero metrics.\")\n",
        "        print(\"No shared tracks between test and predicted. Returning zero metrics.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    # Evaluate on test data\n",
        "    precisions = []\n",
        "    for user_id in test_matrix.index:\n",
        "        if user_id in predicted_ratings.index:\n",
        "            true_ratings = test_matrix.loc[user_id]\n",
        "            pred_ratings = predicted_ratings.loc[user_id]\n",
        "            valid_top_k = pred_ratings[pred_ratings.index.isin(shared_tracks)].sort_values(ascending=False).index[:k]\n",
        "            if len(valid_top_k) == 0:\n",
        "                logging.warning(\"No valid tracks for user %s in test set.\", user_id)\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "            relevant = (true_ratings[valid_top_k] > 3.5).astype(int)\n",
        "            precision = relevant.mean()\n",
        "            precisions.append(precision)\n",
        "            logging.info(\"User %s: %d valid tracks, precision=%f\", user_id, len(valid_top_k), precision)\n",
        "\n",
        "    metrics = {'precision@k': np.mean(precisions) if precisions else 0.0}\n",
        "    logging.info(\"CF Metrics: %s\", metrics)\n",
        "    return metrics\n",
        "\n",
        "class GenreClassifier(nn.Module):\n",
        "    \"\"\"Task A1.1: Custom genre classifier combining audio and text features.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, num_classes, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "        return self.fc(combined)\n",
        "\n",
        "def train_classifier(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Task A1.1: Train the genre classifier.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, labels in train_loader:\n",
        "        audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_classifier(model, test_loader, genres):\n",
        "    \"\"\"Task C5: Evaluate classifier with precision, recall, F1, and confusion matrix.\"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    errors = [(i, y_true[i], y_pred[i]) for i in range(len(y_true)) if y_true[i] != y_pred[i]]\n",
        "    print(f\"Error Analysis: {len(errors)} misclassifications\")\n",
        "    for idx, true, pred in errors[:5]:\n",
        "        print(f\"Sample {idx}: True={genres[true]}, Predicted={genres[pred]}\")\n",
        "    logging.info(\"Test set class distribution: %s\", Counter(y_true))\n",
        "    print(f\"Test set class distribution: {Counter(y_true)}\")\n",
        "    return {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for audio files and genre labels.\"\"\"\n",
        "    def __init__(self, audio_files, labels, genre_to_label, feature_extractor, max_length=160000):\n",
        "        self.audio_files = audio_files\n",
        "        self.labels = labels\n",
        "        self.genre_to_label = genre_to_label\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            audio_file = self.audio_files[idx]\n",
        "            y, sr = librosa.load(audio_file, sr=16000, duration=10.0)  # Limit to 10 seconds\n",
        "            inputs = self.feature_extractor(\n",
        "                y,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_length,\n",
        "                return_attention_mask=True\n",
        "            )\n",
        "            # Remove batch dimension\n",
        "            input_values = inputs.input_values.squeeze(0)\n",
        "            attention_mask = inputs.attention_mask.squeeze(0) if inputs.attention_mask is not None else None\n",
        "            label = self.genre_to_label[self.labels[idx]]\n",
        "            return {\n",
        "                \"input_values\": input_values,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(\"Failed to process audio file %s: %s\", audio_file, str(e))\n",
        "            # Return a dummy item to avoid breaking the DataLoader\n",
        "            return {\n",
        "                \"input_values\": torch.zeros(self.max_length),\n",
        "                \"attention_mask\": torch.zeros(self.max_length),\n",
        "                \"labels\": torch.tensor(0, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "def ast_classifier(audio_files, labels, genres, df_metadata):\n",
        "    \"\"\"Task A2: Audio classification with fine-tuned Wav2Vec2 model.\"\"\"\n",
        "    print(\"\\nSetting up Audio Classifier (Wav2Vec2)...\")\n",
        "\n",
        "    # Initialize feature extractor\n",
        "    try:\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to load feature extractor: %s\", str(e))\n",
        "        print(f\"Failed to load feature extractor: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    # Dynamic genre mapping\n",
        "    genre_to_label = {genre: idx for idx, genre in enumerate(sorted(genres))}\n",
        "    label_to_genre = {idx: genre for genre, idx in genre_to_label.items()}\n",
        "\n",
        "    # Filter valid audio files and labels\n",
        "    valid_audio_files = []\n",
        "    valid_labels = []\n",
        "    for audio_file, label in zip(audio_files, labels):\n",
        "        track_id = Path(audio_file).stem\n",
        "        if track_id in df_metadata['track_id'].values and os.path.exists(audio_file) and os.path.getsize(audio_file) > 0:\n",
        "            genre = df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]\n",
        "            if genre in genre_to_label:\n",
        "                valid_audio_files.append(audio_file)\n",
        "                valid_labels.append(genre)\n",
        "            else:\n",
        "                logging.warning(\"Genre %s not in genre_to_label for track %s\", genre, track_id)\n",
        "        else:\n",
        "            logging.warning(\"Invalid or missing audio file: %s\", audio_file)\n",
        "\n",
        "    if not valid_audio_files:\n",
        "        print(\"No valid audio files for processing.\")\n",
        "        logging.error(\"No valid audio files for processing.\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    # Split into train and eval sets\n",
        "    train_files, eval_files, train_labels, eval_labels = train_test_split(\n",
        "        valid_audio_files, valid_labels, test_size=0.2, random_state=42, stratify=valid_labels\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = AudioDataset(train_files, train_labels, genre_to_label, feature_extractor)\n",
        "    eval_dataset = AudioDataset(eval_files, eval_labels, genre_to_label, feature_extractor)\n",
        "\n",
        "    # Initialize model with correct number of labels\n",
        "    try:\n",
        "        model = AutoModelForAudioClassification.from_pretrained(\n",
        "            \"facebook/wav2vec2-base-960h\",\n",
        "            num_labels=len(genre_to_label),\n",
        "            label2id=genre_to_label,\n",
        "            id2label=label_to_genre\n",
        "        ).to(DEVICE)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to load Wav2Vec2 model: %s\", str(e))\n",
        "        print(f\"Failed to load Wav2Vec2 model: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(OUTPUT_DIR, \"wav2vec2-finetuned\"),\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"steps\",  # Updated from evaluation_strategy\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        logging_steps=100,\n",
        "        learning_rate=3e-5,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "    )\n",
        "\n",
        "    # Define compute_metrics function for evaluation\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = pred.predictions.argmax(-1)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
        "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    print(\"Fine-tuning Wav2Vec2 model...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model(os.path.join(OUTPUT_DIR, \"wav2vec2-finetuned\"))\n",
        "        print(\"Model fine-tuning completed and saved.\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Fine-tuning failed: %s\", str(e))\n",
        "        print(f\"Fine-tuning failed: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    # Evaluate on eval set\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    for audio_file, genre in zip(eval_files, eval_labels):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_file, sr=16000, duration=10.0)\n",
        "            inputs = feature_extractor(\n",
        "                y,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=160000,\n",
        "                return_attention_mask=True\n",
        "            ).to(DEVICE)\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs).logits\n",
        "            predicted = torch.argmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            y_true.append(genre_to_label[genre])\n",
        "            y_pred.append(min(predicted, len(genre_to_label) - 1))\n",
        "        except Exception as e:\n",
        "            logging.error(\"Evaluation failed for %s: %s\", audio_file, str(e))\n",
        "            continue\n",
        "\n",
        "    # Compute and save metrics\n",
        "    if y_true:\n",
        "        metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        print(f\"Wav2Vec2 Metrics: Precision: {metrics[0]:.4f}, Recall: {metrics[1]:.4f}, F1: {metrics[2]:.4f}\")\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=list(genre_to_label.keys()), yticklabels=list(genre_to_label.keys()))\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Wav2Vec2 Confusion Matrix')\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'wav2vec2_confusion_matrix.png'))\n",
        "        plt.close()\n",
        "        return {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n",
        "    else:\n",
        "        print(\"No valid audio files processed for evaluation.\")\n",
        "        logging.error(\"No valid audio files processed for evaluation.\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "def music_search(query, df, text_embeddings, model):\n",
        "    \"\"\"Task A3: Content-based music discovery using LLM embeddings and cosine similarity.\"\"\"\n",
        "    logging.info(\"Performing music search for query: %s\", query)\n",
        "    print(f\"Performing music search for query: {query}\")\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    similarities = {}\n",
        "    for track_id, embedding in text_embeddings.items():\n",
        "        genre_weight = 1.5 if df[df['track_id'] == track_id]['genre'].iloc[0] == 'Rock' else 1.0\n",
        "        similarities[track_id] = (1 - cosine(query_embedding, embedding)) * genre_weight\n",
        "    top_tracks = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    result_ids = [track_id for track_id, _ in top_tracks]\n",
        "    results = df[df['track_id'].isin(result_ids)][['track_id', 'artist_name', 'title', 'genre']]\n",
        "    logging.info(\"Search returned %d results.\", len(results))\n",
        "    return results\n",
        "\n",
        "def zero_shot_classification(df_metadata, lyrics_dict):\n",
        "    \"\"\"Task A1.2: Zero-shot genre classification using BART.\"\"\"\n",
        "    print(\"\\nZero-Shot Genre Classification:\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if torch.cuda.is_available() else -1)\n",
        "    genres = df_metadata['genre'].unique().tolist()\n",
        "    results = []\n",
        "    for _, row in df_metadata.head(5).iterrows():\n",
        "        text = row['title'] + \" \" + lyrics_dict.get(row['track_id'], \"\")\n",
        "        scores = classifier(text, candidate_labels=genres, multi_label=False)\n",
        "        results.append((row['track_id'], scores['labels'][0], scores['scores'][0]))\n",
        "        print(f\"Track {row['track_id']}: Predicted={scores['labels'][0]} ({scores['scores'][0]:.4f}), True={row['genre']}\")\n",
        "    return results\n",
        "\n",
        "def analyze_feature_contribution(model, test_loader, feature_columns):\n",
        "    \"\"\"Task A1.1: Analyze contribution of audio features to genre classification.\"\"\"\n",
        "    print(\"\\nFeature Contribution Analysis:\")\n",
        "    model.eval()\n",
        "    contributions = {col: [] for col in feature_columns}\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, _ in test_loader:\n",
        "            audio_features, text_features = audio_features.to(DEVICE), text_features.to(DEVICE)\n",
        "            baseline_output = model(audio_features, text_features)\n",
        "            for i, col in enumerate(feature_columns):\n",
        "                modified_features = audio_features.clone()\n",
        "                modified_features[:, i] = 0\n",
        "                modified_output = model(modified_features, text_features)\n",
        "                diff = torch.mean(torch.abs(baseline_output - modified_output)).item()\n",
        "                contributions[col].append(diff)\n",
        "    for col in feature_columns:\n",
        "        print(f\"{col}: Mean contribution = {np.mean(contributions[col]):.4f}\")\n",
        "    return contributions\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function orchestrating all tasks.\"\"\"\n",
        "    logging.info(\"Starting main execution...\")\n",
        "    print(\"Starting main execution...\")\n",
        "\n",
        "    # Task A1, A3, A5: Setup dataset\n",
        "    setup_fma_dataset()\n",
        "    df_metadata, df_features, lyrics_dict = load_fma_data(AUDIO_PATH, METADATA_PATH, ARTISTS_PATH, GENRES_PATH, LYRICS_PATH, TAGS_PATH)\n",
        "\n",
        "    if df_metadata.empty or df_features.empty or not lyrics_dict:\n",
        "        logging.error(\"Data loading failed. Exiting.\")\n",
        "        print(\"Data loading failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Task A1.2: Generate text embeddings\n",
        "    text_embeddings = generate_text_embeddings(lyrics_dict)\n",
        "\n",
        "    # Task A1.2: Linguistic analysis\n",
        "    analyze_linguistic_patterns(df_metadata, lyrics_dict)\n",
        "\n",
        "    # Task A1.2: Zero-shot classification\n",
        "    zero_shot_classification(df_metadata, lyrics_dict)\n",
        "\n",
        "    # Prepare features, aligning with valid audio tracks\n",
        "    valid_track_ids = df_features['track_id'].tolist()\n",
        "    if len(valid_track_ids) < 50:\n",
        "        logging.warning(\"Only %d valid audio tracks found. Consider increasing MAX_TRACKS or checking audio files.\", len(valid_track_ids))\n",
        "        print(f\"Warning: Only {len(valid_track_ids)} valid audio tracks found. Using available tracks.\")\n",
        "\n",
        "    # Filter metadata and lyrics to valid tracks\n",
        "    df_metadata = df_metadata[df_metadata['track_id'].isin(valid_track_ids)]\n",
        "    text_features = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df_metadata['track_id']])\n",
        "\n",
        "    # Normalize audio features\n",
        "    scaler = StandardScaler()\n",
        "    audio_features = scaler.fit_transform(df_features[[col for col in df_features.columns if col != 'track_id']].values)\n",
        "    audio_features = np.nan_to_num(audio_features)\n",
        "    logging.info(\"Audio feature stats: mean=%s, std=%s\", audio_features.mean(), audio_features.std())\n",
        "    print(f\"Audio feature stats: mean={audio_features.mean():.4f}, std={audio_features.std():.4f}\")\n",
        "\n",
        "    if audio_features.shape[0] == 0 or text_features.shape[0] == 0:\n",
        "        logging.error(\"No valid features after filtering. Exiting.\")\n",
        "        print(\"No valid features after filtering. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Task A5: Load user ratings\n",
        "    try:\n",
        "        user_data = pd.read_csv(USER_DATA_PATH)\n",
        "        user_data['track_id'] = user_data['track_id'].astype(str).str.zfill(6)\n",
        "        user_data = user_data[user_data['track_id'].isin(df_metadata['track_id'])]\n",
        "        ratings = np.clip(user_data['rating'].values / 5.0, 0.0, 1.0)\n",
        "        logging.info(\"Ratings stats: min=%s, max=%s, mean=%s\", ratings.min(), ratings.max(), ratings.mean())\n",
        "        print(f\"Ratings stats: min={ratings.min():.4f}, max={ratings.max():.4f}, mean={ratings.mean():.4f}\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading ratings: %s. Using synthetic ratings.\", str(e))\n",
        "        print(f\"Error loading ratings: {str(e)}. Using synthetic ratings.\")\n",
        "        ratings = np.random.uniform(0.2, 1.0, len(df_metadata))\n",
        "        ratings = np.clip(ratings, 0.0, 1.0)\n",
        "        logging.info(\"Synthetic ratings stats: min=%s, max=%s, mean=%s\", ratings.min(), ratings.max(), ratings.mean())\n",
        "        print(f\"Synthetic ratings stats: min={ratings.min():.4f}, max={ratings.max():.4f}, mean={ratings.mean():.4f}\")\n",
        "\n",
        "    if len(ratings) != len(df_metadata):\n",
        "        logging.warning(\"Ratings length mismatch. Truncating to match metadata.\")\n",
        "        print(\"Ratings length mismatch. Truncating to match metadata.\")\n",
        "        ratings = ratings[:len(df_metadata)]\n",
        "\n",
        "    # Task A5: Train-test split for recommender\n",
        "    indices = np.arange(len(df_metadata))\n",
        "    X_train_rec, X_test_rec, y_train_rec, y_test_rec, train_idx, test_idx = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), ratings, indices, test_size=0.2, random_state=42\n",
        "    )\n",
        "    logging.info(\"Train-test split for recommender: train=%d, test=%d\", len(X_train_rec), len(X_test_rec))\n",
        "    print(f\"Train-test split for recommender: train={len(X_train_rec)}, test={len(X_test_rec)}\")\n",
        "\n",
        "    train_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_rec, dtype=torch.float32)\n",
        "    )\n",
        "    train_loader_rec = torch.utils.data.DataLoader(train_dataset_rec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    test_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_rec, dtype=torch.float32)\n",
        "    )\n",
        "    test_loader_rec = torch.utils.data.DataLoader(test_dataset_rec, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Task A5: Train content-based recommender\n",
        "    recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=384).to(DEVICE)\n",
        "    criterion_rec = nn.BCELoss()\n",
        "    optimizer_rec = torch.optim.Adam(recommender.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"\\nTraining Recommender Model...\")\n",
        "    for epoch in range(NUM_EPOCHS_REC):\n",
        "        loss = train_recommender(recommender, train_loader_rec, criterion_rec, optimizer_rec)\n",
        "        logging.info(\"Recommendation Epoch %d, Loss: %.4f\", epoch+1, loss)\n",
        "        print(f\"Recommendation Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Task C5: Evaluate recommender\n",
        "    rec_metrics = evaluate_recommender(recommender, test_loader_rec, df_metadata, test_idx, df_metadata['genre'].unique())\n",
        "    logging.info(\"Recommendation Metrics: %s\", rec_metrics)\n",
        "    print(f\"Recommendation Metrics: {rec_metrics}\")\n",
        "\n",
        "    # Task A5: Collaborative filtering\n",
        "    cf_metrics = collaborative_filtering(user_data, df_metadata)\n",
        "    print(f\"Collaborative Filtering Metrics: {cf_metrics}\")\n",
        "    print(f\"Comparison: Content-Based Precision@K={rec_metrics['precision@k']:.4f}, CF Precision@K={cf_metrics['precision@k']:.4f}\")\n",
        "\n",
        "    # Task A1.1: Prepare classifier data\n",
        "    genres = df_metadata['genre'].unique()\n",
        "    genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
        "    labels = df_metadata['genre'].map(genre_to_idx).values\n",
        "\n",
        "    X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    logging.info(\"Train-test split for classifier: train=%d, test=%d\", len(X_train_cls), len(X_test_cls))\n",
        "    print(f\"Train-test split for classifier: train={len(X_train_cls)}, test={len(X_test_cls)}\")\n",
        "\n",
        "    train_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_cls, dtype=torch.long)\n",
        "    )\n",
        "    train_loader_cls = torch.utils.data.DataLoader(train_dataset_cls, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    test_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_cls, dtype=torch.long)\n",
        "    )\n",
        "    test_loader_cls = torch.utils.data.DataLoader(test_dataset_cls, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Task A1.1: Train custom classifier\n",
        "    classifier = GenreClassifier(audio_dim=audio_features.shape[1], text_dim=384, num_classes=len(genres)).to(DEVICE)\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    print(\"\\nTraining Genre Classifier Model...\")\n",
        "    for epoch in range(NUM_EPOCHS_CLS):\n",
        "        loss = train_classifier(classifier, train_loader_cls, criterion_cls, optimizer_cls)\n",
        "        logging.info(\"Classification Epoch %d, Loss: %.4f\", epoch+1, loss)\n",
        "        print(f\"Classification Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Task C5: Evaluate classifier\n",
        "    cls_metrics = evaluate_classifier(classifier, test_loader_cls, genres)\n",
        "    logging.info(\"Classification Metrics: %s\", cls_metrics)\n",
        "    print(f\"Classification Metrics: Precision: {cls_metrics['precision']:.4f}, Recall: {cls_metrics['recall']:.4f}, F1: {cls_metrics['f1']:.4f}\")\n",
        "\n",
        "    # Task A1.1: Analyze feature contributions\n",
        "    analyze_feature_contribution(classifier, test_loader_cls, df_features.columns[1:])\n",
        "\n",
        "    # Task A2: Audio classification\n",
        "    audio_files = [os.path.join(AUDIO_PATH, f\"{tid}.mp3\") for tid in df_metadata['track_id'] if os.path.exists(os.path.join(AUDIO_PATH, f\"{tid}.mp3\"))]\n",
        "    ast_metrics = ast_classifier(audio_files, labels[:len(audio_files)], genres, df_metadata)\n",
        "    print(f\"Wav2Vec2 vs Custom Classifier: Wav2Vec2 F1={ast_metrics['f1']:.4f}, Custom F1={cls_metrics['f1']:.4f}\")\n",
        "\n",
        "    # Task A3: Perform music search\n",
        "    search_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    query = \"upbeat rock songs\"\n",
        "    results = music_search(query, df_metadata, text_embeddings, search_model)\n",
        "    print(\"\\nSearch Results:\")\n",
        "    print(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ssl9XfPLe2w2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3757ae-ee09-4beb-e649-c02e19526be4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Starting main execution...\n",
            "=== Dataset Setup ===\n",
            "fma_small.zip already exists, skipping download.\n",
            "fma_metadata.zip already exists, skipping download.\n",
            "Verifying checksums...\n",
            "Checking and moving MP3 files...\n",
            "Moved 0 MP3 files.\n",
            "Processing metadata...\n",
            "Dataset setup completed successfully.\n",
            "Tracks shape: (400, 5), Genres shape: (163, 2), Ratings shape: (400, 3)\n",
            "Loading FMA data...\n",
            "Metadata shape: (400, 4)\n",
            "Genre distribution:\n",
            "genre\n",
            "Hip-Hop          50\n",
            "Pop              50\n",
            "Folk             50\n",
            "Experimental     50\n",
            "Rock             50\n",
            "International    50\n",
            "Electronic       50\n",
            "Instrumental     50\n",
            "Name: count, dtype: int64\n",
            "Found 2584 audio files.\n",
            "Valid track IDs (first 10): ['001276', '001075', '007527', '013571', '001102', '003778', '003763', '001069', '013197', '000853']\n",
            "Audio file IDs (first 10): ['030043', '004022', '020364', '048463', '024426', '048042', '011947', '018037', '021891', '052120']\n",
            "Matching IDs: 400\n",
            "Processed 400 valid audio files.\n",
            "Valid audio files (first 5): ['/content/fma/data/audio_files/004022.mp3', '/content/fma/data/audio_files/020364.mp3', '/content/fma/data/audio_files/021891.mp3', '/content/fma/data/audio_files/003533.mp3', '/content/fma/data/audio_files/001073.mp3']\n",
            "Features shape: (400, 28)\n",
            "Lyrics dict size: 400\n",
            "Generating text embeddings...\n",
            "Text embeddings generated for 400 tracks.\n",
            "\n",
            "Linguistic Analysis: Top 5 words per genre\n",
            "Hip-Hop top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "Pop top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "Folk top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "Experimental top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "Rock top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "International top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "Electronic top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "Instrumental top words: [('Synthetic', 50), ('lyrics', 50), ('for', 50), ('track', 50), ('in', 50)]\n",
            "\n",
            "Zero-Shot Genre Classification:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Track 000002: Predicted=Hip-Hop (0.9402), True=Hip-Hop\n",
            "Track 000005: Predicted=Hip-Hop (0.5383), True=Hip-Hop\n",
            "Track 000010: Predicted=Pop (0.9424), True=Pop\n",
            "Track 000140: Predicted=Folk (0.9474), True=Folk\n",
            "Track 000141: Predicted=Folk (0.7373), True=Folk\n",
            "Audio feature stats: mean=-0.0000, std=1.0000\n",
            "Ratings stats: min=0.2003, max=0.9976, mean=0.6068\n",
            "Train-test split for recommender: train=320, test=80\n",
            "\n",
            "Training Recommender Model...\n",
            "Recommendation Epoch 1, Loss: 0.6716\n",
            "Recommendation Epoch 2, Loss: 0.6648\n",
            "Recommendation Epoch 3, Loss: 0.6589\n",
            "Recommendation Epoch 4, Loss: 0.6571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No shared tracks between test and predicted. Returning zero metrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendation Epoch 5, Loss: 0.6542\n",
            "Recommendation Metrics: {'precision@k': np.float64(0.6666666766007742), 'map': np.float64(0.6812424954752659), 'ndcg@k': np.float64(0.6622623010305043), 'diversity': np.float64(0.75)}\n",
            "\n",
            "Training Collaborative Filtering Model...\n",
            "CF data split: train=320, test=80\n",
            "Test matrix tracks: 80, Predicted tracks: 320\n",
            "Shared tracks between test and predicted: 0\n",
            "No shared tracks between test and predicted. Returning zero metrics.\n",
            "Collaborative Filtering Metrics: {'precision@k': 0.0}\n",
            "Comparison: Content-Based Precision@K=0.6667, CF Precision@K=0.0000\n",
            "Train-test split for classifier: train=320, test=80\n",
            "\n",
            "Training Genre Classifier Model...\n",
            "Classification Epoch 1, Loss: 2.1093\n",
            "Classification Epoch 2, Loss: 2.0307\n",
            "Classification Epoch 3, Loss: 1.9911\n",
            "Classification Epoch 4, Loss: 1.9341\n",
            "Classification Epoch 5, Loss: 1.8681\n",
            "Classification Epoch 6, Loss: 1.7903\n",
            "Classification Epoch 7, Loss: 1.7210\n",
            "Classification Epoch 8, Loss: 1.6257\n",
            "Classification Epoch 9, Loss: 1.5313\n",
            "Classification Epoch 10, Loss: 1.4225\n",
            "Error Analysis: 22 misclassifications\n",
            "Sample 1: True=Hip-Hop, Predicted=Folk\n",
            "Sample 6: True=Instrumental, Predicted=International\n",
            "Sample 10: True=Folk, Predicted=Hip-Hop\n",
            "Sample 13: True=Folk, Predicted=Pop\n",
            "Sample 19: True=Instrumental, Predicted=Pop\n",
            "Test set class distribution: Counter({np.int64(2): 14, np.int64(3): 13, np.int64(7): 13, np.int64(0): 10, np.int64(5): 10, np.int64(6): 8, np.int64(1): 6, np.int64(4): 6})\n",
            "Classification Metrics: Precision: 0.7576, Recall: 0.7250, F1: 0.7298\n",
            "\n",
            "Feature Contribution Analysis:\n",
            "mfcc_1: Mean contribution = 0.0283\n",
            "mfcc_2: Mean contribution = 0.0254\n",
            "mfcc_3: Mean contribution = 0.0376\n",
            "mfcc_4: Mean contribution = 0.0355\n",
            "mfcc_5: Mean contribution = 0.0230\n",
            "mfcc_6: Mean contribution = 0.0259\n",
            "mfcc_7: Mean contribution = 0.0217\n",
            "mfcc_8: Mean contribution = 0.0252\n",
            "mfcc_9: Mean contribution = 0.0379\n",
            "mfcc_10: Mean contribution = 0.0217\n",
            "mfcc_11: Mean contribution = 0.0179\n",
            "mfcc_12: Mean contribution = 0.0227\n",
            "mfcc_13: Mean contribution = 0.0202\n",
            "chroma_1: Mean contribution = 0.0293\n",
            "chroma_2: Mean contribution = 0.0257\n",
            "chroma_3: Mean contribution = 0.0248\n",
            "chroma_4: Mean contribution = 0.0305\n",
            "chroma_5: Mean contribution = 0.0233\n",
            "chroma_6: Mean contribution = 0.0277\n",
            "chroma_7: Mean contribution = 0.0310\n",
            "chroma_8: Mean contribution = 0.0289\n",
            "chroma_9: Mean contribution = 0.0331\n",
            "chroma_10: Mean contribution = 0.0271\n",
            "chroma_11: Mean contribution = 0.0308\n",
            "chroma_12: Mean contribution = 0.0300\n",
            "spectral_centroid: Mean contribution = 0.0337\n",
            "tempo: Mean contribution = 0.0256\n",
            "\n",
            "Setting up Audio Classifier (Wav2Vec2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "ERROR:root:Fine-tuning failed: 'AudioDataset' object has no attribute '_data'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning Wav2Vec2 model...\n",
            "Fine-tuning failed: 'AudioDataset' object has no attribute '_data'\n",
            "Wav2Vec2 vs Custom Classifier: Wav2Vec2 F1=0.0000, Custom F1=0.7298\n",
            "Performing music search for query: upbeat rock songs\n",
            "\n",
            "Search Results:\n",
            "    track_id                     artist_name                      title genre\n",
            "21    000368                  Blah Blah Blah                   Vampires  Rock\n",
            "27    000574                    Clockcleaner             Caliente Queen  Rock\n",
            "53    000825  Here Comes A Big Black Cloud!!                Death March  Rock\n",
            "58    000993      Jad Fair and Jason Willett       Or So I've Been Told  Rock\n",
            "70    001087                        Mahjongg  Tell The Police The Truth  Rock\n",
            "109   001706            Strapping Fieldhands              In the Pineys  Rock\n",
            "110   001720                        Sun Araw            Harken Sunshine  Rock\n",
            "116   001891                    Thee Oh Sees               Kids In Cars  Rock\n",
            "145   003720                  Indian Jewelry       Walking on the Water  Rock\n",
            "200   004779                   Half Japanese             Secret Admirer  Rock\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}