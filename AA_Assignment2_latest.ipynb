{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viditk07-Bits/AudioAnalytics_S2-24_AIMLCZG527/blob/main/AA_Assignment2_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BOSdHzXqxEV"
      },
      "source": [
        "# Music Information Retrieval System\n",
        "\n",
        "## Assignment Objective\n",
        "This assignment implements a comprehensive Music Information Retrieval (MIR) system using Large Language Models (LLMs) and deep learning techniques. It includes music recommendation, genre classification, and semantic search applications, combining audio analysis with natural language processing.\n",
        "\n",
        "## Dataset Setup\n",
        "Using the Free Music Archive (FMA) dataset with audio files, metadata, and synthetic user data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhEBS6nUqxEW",
        "outputId": "cae62d16-bc85-4d36-de9a-15de6db06ce0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import hashlib\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "from librosa.feature.rhythm import tempo as librosa_tempo\n",
        "\n",
        "# Mount Google Drive (optional, for persistent storage)\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/fma/data\"\n",
        "AUDIO_PATH = os.path.join(DATA_PATH, \"audio_files/\")\n",
        "METADATA_PATH = os.path.join(DATA_PATH, \"metadata/tracks.csv\")\n",
        "ARTISTS_PATH = os.path.join(DATA_PATH, \"metadata/artists.csv\")\n",
        "GENRES_PATH = os.path.join(DATA_PATH, \"metadata/genres.csv\")\n",
        "LYRICS_PATH = os.path.join(DATA_PATH, \"lyrics/\")\n",
        "USER_DATA_PATH = os.path.join(DATA_PATH, \"user_data/ratings.csv\")\n",
        "TAGS_PATH = os.path.join(DATA_PATH, \"descriptions/tags.csv\")\n",
        "OUTPUT_DIR = \"outputs/\"\n",
        "TEMP_DIR = \"/content/fma\"\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "MAX_TRACKS = 1000  # Limit for faster testing\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def setup_fma_dataset():\n",
        "    \"\"\"Download and set up FMA dataset, moving files to expected structure.\"\"\"\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "    os.chdir(TEMP_DIR)\n",
        "\n",
        "    # Step 1: Clone FMA GitHub repository\n",
        "    if not os.path.exists(os.path.join(TEMP_DIR, \"fma\")):\n",
        "        print(\"Cloning FMA repository...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/mdeff/fma.git\"], check=True)\n",
        "    else:\n",
        "        print(\"FMA repository already exists.\")\n",
        "    os.chdir(os.path.join(TEMP_DIR, \"fma\"))\n",
        "\n",
        "    # Step 2: Download fma_small.zip and fma_metadata.zip\n",
        "    fma_small_zip = \"fma_small.zip\"\n",
        "    fma_metadata_zip = \"fma_metadata.zip\"\n",
        "\n",
        "    if os.path.exists(fma_small_zip):\n",
        "        os.remove(fma_small_zip)\n",
        "        print(f\"Removed existing {fma_small_zip}\")\n",
        "    if os.path.exists(fma_metadata_zip):\n",
        "        os.remove(fma_metadata_zip)\n",
        "        print(f\"Removed existing {fma_metadata_zip}\")\n",
        "\n",
        "    print(\"Downloading fma_small.zip...\")\n",
        "    subprocess.run([\"wget\", \"-O\", fma_small_zip, \"https://os.unil.cloud.switch.ch/fma/fma_small.zip\"], check=True)\n",
        "    print(\"Downloading fma_metadata.zip...\")\n",
        "    subprocess.run([\"wget\", \"-O\", fma_metadata_zip, \"https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\"], check=True)\n",
        "\n",
        "    # Step 3: Verify SHA1 checksums\n",
        "    def sha1_checksum(file_path):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(file_path, 'rb') as f:\n",
        "            while chunk := f.read(8192):\n",
        "                sha1.update(chunk)\n",
        "        return sha1.hexdigest()\n",
        "\n",
        "    assert sha1_checksum(\"fma_small.zip\") == \"ade154f733639d52e35e32f5593efe5be76c6d70\", \"fma_small.zip checksum failed!\"\n",
        "    assert sha1_checksum(\"fma_metadata.zip\") == \"f0df49ffe5f2a6008d7dc83c6915b31835dfe733\", \"fma_metadata.zip checksum failed!\"\n",
        "    print(\"âœ… SHA1 checksums verified.\")\n",
        "\n",
        "    # Step 4: Unzip files\n",
        "    os.makedirs(DATA_PATH, exist_ok=True)\n",
        "    if not os.path.exists(os.path.join(DATA_PATH, \"fma_small\")):\n",
        "        print(\"Unzipping fma_small.zip...\")\n",
        "        subprocess.run([\"unzip\", \"-q\", \"fma_small.zip\", \"-d\", DATA_PATH], check=True)\n",
        "    else:\n",
        "        print(\"fma_small already unzipped.\")\n",
        "\n",
        "    if not os.path.exists(os.path.join(DATA_PATH, \"fma_metadata\")):\n",
        "        print(\"Unzipping fma_metadata.zip...\")\n",
        "        subprocess.run([\"unzip\", \"-q\", \"fma_metadata.zip\", \"-d\", DATA_PATH], check=True)\n",
        "    else:\n",
        "        print(\"fma_metadata already unzipped.\")\n",
        "\n",
        "    # Step 5: Move .mp3 files to audio_files/\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    print(\"Moving MP3 files...\")\n",
        "    for mp3_file in Path(DATA_PATH).rglob(\"*.mp3\"):\n",
        "        target = os.path.join(AUDIO_PATH, mp3_file.name)\n",
        "        if not os.path.exists(target):\n",
        "            os.rename(mp3_file, target)\n",
        "    print(\"MP3 files moved.\")\n",
        "\n",
        "    # Step 6: Process metadata\n",
        "    print(\"Processing metadata...\")\n",
        "    tracks = pd.read_csv(os.path.join(DATA_PATH, \"fma_metadata\", \"tracks.csv\"), index_col=0, header=[0, 1])\n",
        "    genres = pd.read_csv(os.path.join(DATA_PATH, \"fma_metadata\", \"genres.csv\"))\n",
        "\n",
        "    # Create artists.csv\n",
        "    df_artists = tracks['artist'][['name']].reset_index().rename(columns={'track_id': 'artist_id', 'name': 'artist_name'})\n",
        "    df_artists['artist_id'] = df_artists['artist_id'].astype(str).str.zfill(6)\n",
        "    os.makedirs(os.path.dirname(ARTISTS_PATH), exist_ok=True)\n",
        "    df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "\n",
        "    # Create genres.csv\n",
        "    df_genres = genres[['genre_id', 'title']].rename(columns={'title': 'genre_name'})\n",
        "    os.makedirs(os.path.dirname(GENRES_PATH), exist_ok=True)\n",
        "    df_genres.to_csv(GENRES_PATH, index=False)\n",
        "\n",
        "    # Adapt tracks.csv\n",
        "    df_tracks = tracks['track'][['title', 'genre_top']].reset_index()\n",
        "    df_tracks['track_id'] = df_tracks['track_id'].astype(str).str.zfill(6)\n",
        "    df_tracks['artist_id'] = df_tracks['track_id']  # FMA doesn't provide artist_id, use track_id as proxy\n",
        "    df_tracks['genre_id'] = df_tracks['genre_top'].map(df_genres.set_index('genre_name')['genre_id'])\n",
        "    df_tracks = df_tracks[['track_id', 'title', 'artist_id', 'genre_id']].dropna()\n",
        "    df_tracks = df_tracks.head(MAX_TRACKS)  # Limit tracks for faster processing\n",
        "    os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n",
        "    df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "\n",
        "    # Create synthetic ratings.csv\n",
        "    os.makedirs(os.path.dirname(USER_DATA_PATH), exist_ok=True)\n",
        "    ratings = pd.DataFrame({\n",
        "        'user_id': ['user_001'] * len(df_tracks),\n",
        "        'track_id': df_tracks['track_id'],\n",
        "        'rating': np.random.uniform(0.1, 1.0, len(df_tracks))\n",
        "    })\n",
        "    ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "\n",
        "    # Create empty lyrics/ and descriptions/\n",
        "    os.makedirs(LYRICS_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(TAGS_PATH), exist_ok=True)\n",
        "    pd.DataFrame({'track_id': df_tracks['track_id'], 'tag': ['music'] * len(df_tracks)}).to_csv(TAGS_PATH, index=False)\n",
        "\n",
        "    print(\"ðŸŽµ Metadata and audio files are ready.\")\n",
        "    print(f\"Tracks shape: {df_tracks.shape}\")\n",
        "    print(f\"Genres shape: {df_genres.shape}\")\n",
        "    print(f\"Ratings shape: {ratings.shape}\")\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    \"\"\"Extract audio features (MFCCs, chroma, spectral features, tempo) from an MP3 file using Librosa.\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "        tempo = librosa_tempo(y=y, sr=sr)[0]\n",
        "        tempo = tempo[0] if isinstance(tempo, np.ndarray) else tempo\n",
        "\n",
        "        return np.concatenate([\n",
        "            np.mean(mfccs, axis=1),\n",
        "            np.mean(chroma, axis=1),\n",
        "            np.mean(spectral_centroid, axis=1),\n",
        "            np.mean(spectral_contrast, axis=1),\n",
        "            [tempo]  # Ensure this is a 1D list\n",
        "        ])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return np.zeros(40)  # 20 MFCCs + 12 chroma + 1 centroid + 7 contrast + 1 tempo\n",
        "\n",
        "def load_fma_data(audio_path, metadata_path, artists_path, genres_path, lyrics_path=None, tags_path=None):\n",
        "    \"\"\"Load and preprocess FMA dataset from MP3 files, metadata, and lyrics.\"\"\"\n",
        "    if not os.path.exists(audio_path):\n",
        "        print(f\"Error: Audio directory {audio_path} does not exist.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), {}\n",
        "\n",
        "    # Load metadata\n",
        "    try:\n",
        "        df_tracks = pd.read_csv(metadata_path)\n",
        "        df_metadata = df_tracks[['track_id', 'title', 'artist_id', 'genre_id']].dropna()\n",
        "        if os.path.exists(artists_path):\n",
        "            df_artists = pd.read_csv(artists_path)[['artist_id', 'artist_name']]\n",
        "            df_metadata = pd.merge(df_metadata, df_artists, on='artist_id', how='left')\n",
        "        else:\n",
        "            df_metadata['artist_name'] = 'Unknown Artist'\n",
        "\n",
        "        if os.path.exists(genres_path):\n",
        "            df_genres = pd.read_csv(genres_path)[['genre_id', 'genre_name']]\n",
        "            df_metadata = pd.merge(df_metadata, df_genres, on='genre_id', how='left')\n",
        "        else:\n",
        "            df_metadata['genre_name'] = 'unknown'\n",
        "\n",
        "        df_metadata = df_metadata[['track_id', 'artist_name', 'title', 'genre_name']].dropna()\n",
        "        df_metadata.columns = ['track_id', 'artist_name', 'title', 'genre']\n",
        "        df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "        print(f\"Initial metadata shape: {df_metadata.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading metadata: {e}. Creating synthetic metadata.\")\n",
        "        df_metadata = pd.DataFrame(columns=['track_id', 'artist_name', 'title', 'genre'])\n",
        "\n",
        "    # Extract audio features\n",
        "    features = []\n",
        "    audio_files = list(Path(audio_path).glob(\"*.mp3\"))\n",
        "    print(f\"Found {len(audio_files)} audio files.\")\n",
        "    valid_track_ids = df_metadata['track_id'].tolist()\n",
        "    audio_files_to_process = []\n",
        "    processed_count = 0\n",
        "    for audio_file in audio_files:\n",
        "        track_id = audio_file.stem\n",
        "        if track_id in valid_track_ids:\n",
        "            audio_files_to_process.append(audio_file)\n",
        "            processed_count += 1\n",
        "            if processed_count >= MAX_TRACKS:\n",
        "                break\n",
        "\n",
        "    print(f\"Processing features for {len(audio_files_to_process)} relevant audio files.\")\n",
        "    for audio_file in audio_files_to_process:\n",
        "        track_id = audio_file.stem\n",
        "        audio_features = extract_audio_features(audio_file)\n",
        "        if audio_features is not None and audio_features.shape[0] > 0:\n",
        "            features.append([track_id] + audio_features.tolist())\n",
        "        else:\n",
        "            print(f\"Skipping {track_id} due to feature extraction error.\")\n",
        "\n",
        "    feature_columns = ['track_id'] + [f'mfcc_{i+1}' for i in range(20)] + [f'chroma_{i+1}' for i in range(12)] + \\\n",
        "                     ['spectral_centroid'] + [f'spectral_contrast_{i+1}' for i in range(7)] + ['tempo']\n",
        "    df_features = pd.DataFrame(features, columns=feature_columns).dropna()\n",
        "    print(f\"Extracted features for {len(df_features)} tracks\")\n",
        "\n",
        "    # Filter metadata to match available audio features\n",
        "    df_metadata = pd.merge(df_metadata, df_features[['track_id']], on='track_id', how='inner')\n",
        "    print(f\"Filtered metadata shape: {df_metadata.shape}\")\n",
        "\n",
        "    # Load lyrics and tags\n",
        "    lyrics_dict = {}\n",
        "    if lyrics_path and os.path.exists(lyrics_path):\n",
        "        for lyric_file in Path(lyrics_path).glob(\"*.txt\"):\n",
        "            track_id = lyric_file.stem\n",
        "            if track_id in df_metadata['track_id'].values:\n",
        "                with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                    lyrics_dict[track_id] = f.read().strip()\n",
        "\n",
        "    if tags_path and os.path.exists(tags_path):\n",
        "        try:\n",
        "            df_tags = pd.read_csv(tags_path)\n",
        "            for _, row in df_tags.iterrows():\n",
        "                track_id = str(row['track_id']).zfill(6)\n",
        "                if track_id in df_metadata['track_id'].values:\n",
        "                    tag = str(row['tag'])\n",
        "                    if track_id in lyrics_dict:\n",
        "                        lyrics_dict[track_id] += \" \" + tag\n",
        "                    else:\n",
        "                        lyrics_dict[track_id] = tag\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tags: {e}\")\n",
        "\n",
        "    return df_metadata, df_features, lyrics_dict\n",
        "\n",
        "def generate_text_embeddings(lyrics_dict):\n",
        "    \"\"\"Generate embeddings for lyrics and tags using Sentence-Transformers on GPU.\"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    embeddings = {}\n",
        "    for track_id, text in lyrics_dict.items():\n",
        "        embeddings[track_id] = model.encode(text, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Original GenreClassifier and train/evaluate functions\n",
        "class GenreClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_classes, hidden_dim=128):\n",
        "        super(GenreClassifier, self).__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        return self.fc(combined)\n",
        "\n",
        "def train_classifier(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, labels in train_loader:\n",
        "        audio_features, text_features, labels = (\n",
        "            audio_features.to(DEVICE),\n",
        "            text_features.to(DEVICE),\n",
        "            labels.to(DEVICE)\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_classifier(model, test_loader, genres):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = (\n",
        "                audio_features.to(DEVICE),\n",
        "                text_features.to(DEVICE),\n",
        "                labels.to(DEVICE)\n",
        "            )\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    return precision, recall, f1\n",
        "\n",
        "def train_recommender(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, ratings in train_loader:\n",
        "        audio_features, text_features, ratings = (\n",
        "            audio_features.to(DEVICE),\n",
        "            text_features.to(DEVICE),\n",
        "            ratings.to(DEVICE)\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_recommender(model, test_loader):\n",
        "    model.eval()\n",
        "    precisions = []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, ratings in test_loader:\n",
        "            audio_features, text_features, ratings = (\n",
        "                audio_features.to(DEVICE),\n",
        "                text_features.to(DEVICE),\n",
        "                ratings.to(DEVICE)\n",
        "            )\n",
        "            outputs = model(audio_features, text_features)\n",
        "            k = min(10, outputs.size(1))\n",
        "            if k == 0:\n",
        "                print(\"Warning: No tracks available for top-k selection\")\n",
        "                continue\n",
        "            top_k = torch.topk(outputs, k=k, dim=1).indices\n",
        "            relevant = (ratings.gather(1, top_k) > 0.5).float()\n",
        "            precision = relevant.mean().item()\n",
        "            precisions.append(precision)\n",
        "    return np.mean(precisions) if precisions else 0.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W63UiB5PqxEX"
      },
      "source": [
        "# 2.1.1 Audio Feature Integration with LLMs\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sample_rate=22050):\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def extract_features(self, audio_path):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            return np.concatenate([\n",
        "                np.mean(mfcc, axis=1),\n",
        "                np.mean(spectral_centroid, axis=1),\n",
        "                np.mean(spectral_contrast, axis=1),\n",
        "                np.mean(chroma, axis=1),\n",
        "                [tempo]\n",
        "            ])\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting features from {audio_path}: {e}\")\n",
        "            return np.zeros(40)  # 20 MFCCs + 1 centroid + 7 contrast + 12 chroma + 1 tempo\n",
        "\n",
        "class AudioEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(AudioEmbedding, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, attn_dim=128):\n",
        "        super(CrossModalAttention, self).__init__()\n",
        "        self.audio_proj = nn.Linear(audio_dim, attn_dim)\n",
        "        self.text_proj = nn.Linear(text_dim, attn_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=attn_dim, num_heads=4, batch_first=True)\n",
        "\n",
        "    def forward(self, audio_emb, text_emb):\n",
        "        audio_emb = self.audio_proj(audio_emb).unsqueeze(1)  # (B, 1, attn_dim)\n",
        "        text_emb = self.text_proj(text_emb).unsqueeze(1)     # (B, 1, attn_dim)\n",
        "        attn_output, _ = self.attention(audio_emb, text_emb, text_emb)\n",
        "        return attn_output.squeeze(1)\n",
        "\n",
        "def analyze_feature_contribution(model, features, labels):\n",
        "    contributions = {}\n",
        "    feature_groups = {\n",
        "        'mfcc': slice(0, 20),\n",
        "        'spectral_centroid': slice(20, 21),\n",
        "        'spectral_contrast': slice(21, 28),\n",
        "        'chroma': slice(28, 40),\n",
        "        'tempo': slice(40, 41)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        base_pred = model(features).cpu().numpy()\n",
        "        base_pred_labels = np.argmax(base_pred, axis=1)\n",
        "        base_error = np.mean((base_pred_labels - labels.cpu().numpy()) ** 2)\n",
        "\n",
        "        for feature_name, feature_slice in feature_groups.items():\n",
        "            temp_features = features.clone()\n",
        "            temp_features[:, feature_slice] = 0\n",
        "            pred = model(temp_features).cpu().numpy()\n",
        "            pred_labels = np.argmax(pred, axis=1)\n",
        "            error = np.mean((pred_labels - labels.cpu().numpy()) ** 2)\n",
        "            contributions[feature_name] = error - base_error\n",
        "\n",
        "    return contributions"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsBatpowqxEX"
      },
      "source": [
        "# 2.1.2 Text-Based Genre Classification\n",
        "class TextGenreClassifier(nn.Module):\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', num_classes=10):\n",
        "        super(TextGenreClassifier, self).__init__()\n",
        "        self.model = SentenceTransformer(model_name, device=DEVICE)\n",
        "        self.classifier = nn.Linear(384, num_classes)  # MiniLM-L6-v2 has 384-dim embeddings\n",
        "\n",
        "    def forward(self, lyrics):\n",
        "        embeddings = self.model.encode(lyrics, convert_to_tensor=True, device=DEVICE)\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "    def fine_tune(self, lyrics_data, labels, epochs=3):\n",
        "        self.train()\n",
        "        optimizer = torch.optim.Adam(self.classifier.parameters(), lr=2e-5)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self(lyrics_data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Fine-tuning Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    def zero_shot_classify(self, lyrics, genre_list):\n",
        "        lyrics_emb = self.model.encode([lyrics], convert_to_tensor=True, device=DEVICE)[0]\n",
        "        genre_embs = self.model.encode(genre_list, convert_to_tensor=True, device=DEVICE)\n",
        "        similarities = [1 - cosine(lyrics_emb.cpu().numpy(), genre_emb.cpu().numpy()) for genre_emb in genre_embs]\n",
        "        return genre_list[np.argmax(similarities)]\n",
        "\n",
        "    def analyze_linguistic_patterns(self, lyrics_data, genres):\n",
        "        embeddings = self.model.encode(lyrics_data, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "        patterns = {}\n",
        "        for genre in set(genres):\n",
        "            genre_indices = [i for i, g in enumerate(genres) if g == genre]\n",
        "            genre_embs = embeddings[genre_indices]\n",
        "            patterns[genre] = np.mean(genre_embs, axis=0)\n",
        "        return patterns"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmOk0GplqxEY"
      },
      "source": [
        "# 2.1.3 Hybrid Multi-Modal Classification\n",
        "class HybridGenreClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_genres):\n",
        "        super(HybridGenreClassifier, self).__init__()\n",
        "        self.audio_emb = AudioEmbedding(audio_dim, 128)\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.attention = CrossModalAttention(audio_dim=128, text_dim=text_dim, attn_dim=128)\n",
        "        self.classifier = nn.Linear(128 + text_dim + text_dim, num_genres)  # 128 from attention, 384 text, 384 metadata\n",
        "\n",
        "    def forward(self, audio_features, lyrics, metadata):\n",
        "        audio_emb = self.audio_emb(audio_features)\n",
        "        text_emb = self.text_model.encode(lyrics, convert_to_tensor=True, device=DEVICE)\n",
        "        metadata_emb = self.text_model.encode(\n",
        "            [f\"{m[0]} {m[1]}\" for m in metadata],\n",
        "            convert_to_tensor=True,\n",
        "            device=DEVICE\n",
        "        )\n",
        "        fused = self.attention(audio_emb, text_emb)\n",
        "        combined = torch.cat([fused, text_emb, metadata_emb], dim=-1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "    def get_confidence_scores(self, outputs):\n",
        "        return torch.softmax(outputs, dim=-1)\n",
        "\n",
        "def compare_with_audio_only(hybrid_model, audio_only_model, test_data, lyrics, metadata, labels):\n",
        "    hybrid_preds = hybrid_model(test_data, lyrics, metadata)\n",
        "    audio_preds = audio_only_model(test_data)\n",
        "    hybrid_metrics = precision_recall_fscore_support(labels.cpu().numpy(), torch.argmax(hybrid_preds, dim=1).cpu().numpy(), average='weighted')\n",
        "    audio_metrics = precision_recall_fscore_support(labels.cpu().numpy(), torch.argmax(audio_preds, dim=1).cpu().numpy(), average='weighted')\n",
        "    return hybrid_metrics, audio_metrics"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWGLb4TtqxEY"
      },
      "source": [
        "# 2.2 Transformer-Based Audio Classification\n",
        "class AudioSpectrogramTransformer(nn.Module):\n",
        "    def __init__(self, patch_size=16, in_channels=1, embed_dim=768, num_heads=12, num_layers=12, num_classes=10):\n",
        "        super(AudioSpectrogramTransformer, self).__init__()\n",
        "        self.patch_embedding = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads), num_layers=num_layers\n",
        "        )\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, spectrogram):\n",
        "        patches = self.patch_embedding(spectrogram)\n",
        "        patches = patches.flatten(2).transpose(1, 2)\n",
        "        transformer_output = self.transformer(patches)\n",
        "        return self.classifier(transformer_output[:, 0])\n",
        "\n",
        "def visualize_attention_patterns(model, spectrogram):\n",
        "    with torch.no_grad():\n",
        "        patches = model.patch_embedding(spectrogram).flatten(2).transpose(1, 2)\n",
        "        attention = model.transformer.layers[-1].self_attn(patches, patches, patches)[1]\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(attention[0].cpu().numpy(), cmap='viridis')\n",
        "        plt.title('Attention Patterns')\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'attention_patterns.png'))\n",
        "        plt.close()\n",
        "\n",
        "class CNNBaseline(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNNBaseline, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * 30 * 30, num_classes)  # Adjusted for 128x128 input\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.flatten(1)\n",
        "        return self.fc(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3cyWohoqxEY"
      },
      "source": [
        "# 3.1 Semantic Music Search\n",
        "class MusicSearchSystem:\n",
        "    def __init__(self):\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.audio_extractor = AudioFeatureExtractor()\n",
        "        self.intent_classifier = nn.Linear(384, 5)  # 5 intent classes\n",
        "\n",
        "    def process_query(self, query):\n",
        "        query_emb = self.text_model.encode([query], convert_to_tensor=True, device=DEVICE)[0]\n",
        "        intent = torch.softmax(self.intent_classifier(query_emb), dim=-1)\n",
        "        expanded_query = self.expand_query(query)\n",
        "        return intent, expanded_query\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        synonyms = self.text_model.encode([f'similar to {query}', f'like {query}'], convert_to_tensor=True, device=DEVICE)\n",
        "        return synonyms.mean(dim=0)\n",
        "\n",
        "    def multi_modal_search(self, query, audio_features, lyrics, metadata, df_metadata):\n",
        "        query_emb = self.text_model.encode([query], convert_to_tensor=True, device=DEVICE)[0].cpu().numpy()\n",
        "        audio_embs = torch.tensor(audio_features, dtype=torch.float32, device=DEVICE)\n",
        "        lyrics_embs = self.text_model.encode(lyrics, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "        scores = []\n",
        "        for i in range(len(lyrics)):\n",
        "            text_score = 1 - cosine(query_emb, lyrics_embs[i])\n",
        "            audio_score = 1 - cosine(query_emb, audio_embs[i].cpu().numpy())\n",
        "            metadata_score = self.metadata_similarity(query_emb, metadata[i])\n",
        "            scores.append(0.4 * text_score + 0.4 * audio_score + 0.2 * metadata_score)\n",
        "        top_indices = np.argsort(scores)[::-1][:10]\n",
        "        return df_metadata.iloc[top_indices][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    def metadata_similarity(self, query_emb, metadata):\n",
        "        metadata_emb = self.text_model.encode([f\"{metadata[0]} {metadata[1]}\"], convert_to_tensor=True, device=DEVICE)[0].cpu().numpy()\n",
        "        return 1 - cosine(query_emb, metadata_emb)\n",
        "\n",
        "# 3.1.3 Content-Based Music Discovery\n",
        "class MusicDiscovery:\n",
        "    def __init__(self):\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.audio_model = AudioSpectrogramTransformer()\n",
        "        self.audio_extractor = AudioFeatureExtractor()\n",
        "        self.mood_classifier = nn.Linear(384, 5)  # 5 mood classes\n",
        "        self.energy_predictor = nn.Linear(40, 1)\n",
        "        self.danceability_predictor = nn.Linear(40, 1)\n",
        "\n",
        "    def generate_tags(self, audio_path, lyrics):\n",
        "        audio_features = self.audio_extractor.extract_features(audio_path)\n",
        "        audio_features = torch.tensor(audio_features, dtype=torch.float32, device=DEVICE)\n",
        "        lyrics_emb = self.text_model.encode([lyrics], convert_to_tensor=True, device=DEVICE)[0]\n",
        "        mood_scores = torch.softmax(self.mood_classifier(lyrics_emb), dim=-1)\n",
        "        energy = torch.sigmoid(self.energy_predictor(audio_features))\n",
        "        danceability = torch.sigmoid(self.danceability_predictor(audio_features))\n",
        "        return {'mood': mood_scores.cpu().numpy(), 'energy': energy.cpu().numpy(), 'danceability': danceability.cpu().numpy()}\n",
        "\n",
        "    def generate_playlist(self, seed_song, music_collection, df_metadata):\n",
        "        seed_emb = self.get_song_embedding(seed_song)\n",
        "        similarities = [1 - cosine(seed_emb, self.get_song_embedding(song)) for song in music_collection]\n",
        "        top_indices = np.argsort(similarities)[::-1][:10]\n",
        "        return df_metadata.iloc[top_indices][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    def get_song_embedding(self, song):\n",
        "        audio_features = self.audio_extractor.extract_features(song['audio_path'])\n",
        "        lyrics_emb = self.text_model.encode([song['lyrics']], convert_to_tensor=True, device=DEVICE)[0].cpu().numpy()\n",
        "        return np.concatenate([audio_features, lyrics_emb])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZVhtHITqxEY"
      },
      "source": [
        "# 4.1 Personalized Recommendation Engines\n",
        "class MusicRecommender(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_tracks):\n",
        "        super(MusicRecommender, self).__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, 128)\n",
        "        self.text_layer = nn.Linear(text_dim, 128)\n",
        "        self.user_model = nn.Linear(128, 128)\n",
        "        self.item_model = nn.Linear(128, 128)\n",
        "        self.fc = nn.Linear(128 * 2, num_tracks)\n",
        "\n",
        "    def build_user_profile(self, listening_history, ratings):\n",
        "        history_emb = self.text_model.encode(listening_history, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "        weighted_emb = np.average(history_emb, weights=ratings, axis=0)\n",
        "        return self.user_model(torch.tensor(weighted_emb, dtype=torch.float32, device=DEVICE))\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        return torch.sigmoid(self.fc(combined))\n",
        "\n",
        "    def generate_explanation(self, user_id, item_id, df_metadata):\n",
        "        user_profile = self.build_user_profile(df_metadata['title'].tolist(), df_metadata['rating'].tolist())\n",
        "        item_idx = df_metadata[df_metadata['track_id'] == item_id].index[0]\n",
        "        item_genre = df_metadata.loc[item_idx, 'genre']\n",
        "        return f\"Recommended for user {user_id} because of interest in {item_genre} and similar audio characteristics.\"\n",
        "\n",
        "    def get_item_features(self, item_id):\n",
        "        # Placeholder, to be replaced with actual feature extraction\n",
        "        return torch.zeros(128, device=DEVICE)\n",
        "\n",
        "class HybridRecommender(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_tracks):\n",
        "        super(HybridRecommender, self).__init__()\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.audio_layer = nn.Linear(audio_dim, 128)\n",
        "        self.text_layer = nn.Linear(text_dim, 128)\n",
        "        self.fc = nn.Linear(128 * 2, num_tracks)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        return torch.sigmoid(self.fc(combined))\n",
        "\n",
        "    def recommend(self, user_id, context, audio_features, text_features, df_metadata):\n",
        "        context_emb = self.text_model.encode([context], convert_to_tensor=True, device=DEVICE)\n",
        "        scores = self.forward(audio_features, text_features + context_emb)\n",
        "        top_indices = torch.argsort(scores, dim=1, descending=True)[:, :10].cpu().numpy().flatten()\n",
        "        return df_metadata.iloc[top_indices][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    def optimize_diversity(self, recommendations, df_metadata):\n",
        "        genre_counts = recommendations['genre'].value_counts()\n",
        "        diversity_score = len(genre_counts) / len(recommendations)\n",
        "        if diversity_score < 0.5:\n",
        "            diverse_indices = []\n",
        "            for genre in df_metadata['genre'].unique():\n",
        "                genre_recs = recommendations[recommendations['genre'] == genre]\n",
        "                if not genre_recs.empty:\n",
        "                    diverse_indices.append(genre_recs.index[0])\n",
        "            return df_metadata.iloc[diverse_indices]\n",
        "        return recommendations"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wb4kKECqxEY"
      },
      "source": [
        "# 5.1 Comprehensive Evaluation Framework\n",
        "class EvaluationFramework:\n",
        "    def __init__(self):\n",
        "        self.metrics = {}\n",
        "\n",
        "    def evaluate_classification(self, y_true, y_pred, genres):\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'classification_confusion_matrix.png'))\n",
        "        plt.close()\n",
        "        return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "    def evaluate_retrieval(self, relevant_items, retrieved_items, k_values=[5, 10, 20]):\n",
        "        metrics = {}\n",
        "        for k in k_values:\n",
        "            k = min(k, len(retrieved_items))\n",
        "            precision = len(set(retrieved_items[:k]) & set(relevant_items)) / k if k > 0 else 0\n",
        "            recall = len(set(retrieved_items[:k]) & set(relevant_items)) / len(relevant_items) if relevant_items else 0\n",
        "            metrics[f'P@{k}'] = precision\n",
        "            metrics[f'R@{k}'] = recall\n",
        "        ap = self.calculate_map(relevant_items, retrieved_items)\n",
        "        ndcg = self.calculate_ndcg(relevant_items, retrieved_items)\n",
        "        metrics['MAP'] = ap\n",
        "        metrics['NDCG'] = ndcg\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_recommendation(self, recommendations, user_interactions):\n",
        "        ctr = sum(1 for rec in recommendations['track_id'] if rec in user_interactions) / len(recommendations) if len(recommendations) > 0 else 0\n",
        "        diversity = len(recommendations['genre'].unique()) / len(recommendations) if len(recommendations) > 0 else 0\n",
        "        novelty = 1 - sum(1 for rec in recommendations['track_id'] if rec in user_interactions) / len(recommendations) if len(recommendations) > 0 else 0\n",
        "        return {'CTR': ctr, 'Diversity': diversity, 'Novelty': novelty}\n",
        "\n",
        "    def calculate_map(self, relevant, retrieved):\n",
        "        ap = 0\n",
        "        relevant_set = set(relevant)\n",
        "        for i, item in enumerate(retrieved):\n",
        "            if item in relevant_set:\n",
        "                ap += len(set(retrieved[:i+1]) & relevant_set) / (i + 1)\n",
        "        return ap / len(relevant) if relevant else 0\n",
        "\n",
        "    def calculate_ndcg(self, relevant, retrieved):\n",
        "        dcg = 0\n",
        "        idcg = sum(1 / np.log2(i + 2) for i in range(len(relevant)))\n",
        "        for i, item in enumerate(retrieved):\n",
        "            if item in relevant:\n",
        "                dcg += 1 / np.log2(i + 2)\n",
        "        return dcg / idcg if idcg > 0 else 0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xqPZ6NKqxEY",
        "outputId": "0e41ad51-c6ff-4409-bdf8-5b00e76d3141"
      },
      "source": [
        "# Main Execution\n",
        "def main():\n",
        "    # Setup dataset\n",
        "    if not os.path.exists(DATA_PATH) or not os.path.exists(METADATA_PATH):\n",
        "        print(\"Dataset not found. Setting up FMA dataset...\")\n",
        "        setup_fma_dataset()\n",
        "\n",
        "    # Load data\n",
        "    df_metadata, df_features, lyrics_dict = load_fma_data(AUDIO_PATH, METADATA_PATH, ARTISTS_PATH, GENRES_PATH, LYRICS_PATH, TAGS_PATH)\n",
        "    if df_metadata.empty or df_features.empty:\n",
        "        print(\"Error: No valid data loaded.\")\n",
        "        return\n",
        "\n",
        "    # Generate text embeddings\n",
        "    text_embeddings = generate_text_embeddings(lyrics_dict) if lyrics_dict else {tid: np.zeros(384) for tid in df_metadata['track_id']}\n",
        "    audio_features = df_features[[col for col in df_features.columns if col != 'track_id']].values\n",
        "    text_features = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df_metadata['track_id']])\n",
        "    genres = df_metadata['genre'].unique()\n",
        "    genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
        "    labels = df_metadata['genre'].map(genre_to_idx).values\n",
        "\n",
        "    # Load ratings\n",
        "    num_tracks = len(df_metadata)\n",
        "    track_id_to_idx = {tid: idx for idx, tid in enumerate(df_metadata['track_id'])}\n",
        "    ratings_matrix = np.zeros((num_tracks, num_tracks))\n",
        "    if os.path.exists(USER_DATA_PATH):\n",
        "        user_data = pd.read_csv(USER_DATA_PATH)\n",
        "        user_data['track_id'] = user_data['track_id'].astype(str).str.zfill(6)\n",
        "        user_data = user_data[user_data['track_id'].isin(df_metadata['track_id'])]\n",
        "        for _, row in user_data.iterrows():\n",
        "            tid = row['track_id']\n",
        "            if tid in track_id_to_idx:\n",
        "                ratings_matrix[track_id_to_idx[tid], track_id_to_idx[tid]] = row['rating']\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    X_train_rec, X_test_rec, y_train_rec, y_test_rec = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), ratings_matrix, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Classification data loaders\n",
        "    train_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.long)\n",
        "    )\n",
        "    train_loader_cls = torch.utils.data.DataLoader(train_dataset_cls, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test, dtype=torch.long)\n",
        "    )\n",
        "    test_loader_cls = torch.utils.data.DataLoader(test_dataset_cls, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Recommendation data loaders\n",
        "    train_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_rec, dtype=torch.float32)\n",
        "    )\n",
        "    train_loader_rec = torch.utils.data.DataLoader(train_dataset_rec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_rec, dtype=torch.float32)\n",
        "    )\n",
        "    test_loader_rec = torch.utils.data.DataLoader(test_dataset_rec, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # 2.1.1: Audio Feature Contribution Analysis\n",
        "    audio_model = AudioEmbedding(input_dim=audio_features.shape[1]).to(DEVICE)\n",
        "    features_tensor = torch.tensor(audio_features, dtype=torch.float32, device=DEVICE)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long, device=DEVICE)\n",
        "    contributions = analyze_feature_contribution(audio_model, features_tensor, labels_tensor)\n",
        "    print(\"\\nFeature Contributions:\")\n",
        "    print(contributions)\n",
        "\n",
        "    # 2.1.2: Text-Based Genre Classification\n",
        "    text_classifier = TextGenreClassifier(num_classes=len(genres)).to(DEVICE)\n",
        "    lyrics_list = list(lyrics_dict.values()) if lyrics_dict else ['music'] * len(df_metadata)\n",
        "    text_classifier.fine_tune(lyrics_list[:len(X_train)], torch.tensor(y_train, dtype=torch.long, device=DEVICE))\n",
        "    zero_shot_result = text_classifier.zero_shot_classify(\"upbeat rock song\", genres.tolist())\n",
        "    print(\"\\nZero-Shot Classification Result:\", zero_shot_result)\n",
        "    patterns = text_classifier.analyze_linguistic_patterns(lyrics_list, df_metadata['genre'].tolist())\n",
        "    print(\"\\nLinguistic Patterns:\", {k: np.mean(v) for k, v in patterns.items()})\n",
        "\n",
        "    # 2.1.3: Hybrid Genre Classification\n",
        "    classifier = HybridGenreClassifier(audio_dim=audio_features.shape[1], text_dim=384, num_genres=len(genres)).to(DEVICE)\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for audio_features, _, labels in train_loader_cls:  # Ignore text_features from loader\n",
        "            audio_features, labels = audio_features.to(DEVICE), labels.to(DEVICE)\n",
        "            lyrics_batch = lyrics_list[:len(audio_features)]  # Simplified, assumes aligned data\n",
        "            metadata_batch = df_metadata[['artist_name', 'title']].iloc[:len(audio_features)].values\n",
        "            optimizer_cls.zero_grad()\n",
        "            outputs = classifier(audio_features, lyrics_batch, metadata_batch)\n",
        "            loss = criterion_cls(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_cls.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Hybrid Classification Epoch {epoch+1}, Loss: {total_loss/len(train_loader_cls):.4f}\")\n",
        "\n",
        "    classifier.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, _, labels in test_loader_cls:\n",
        "            audio_features, labels = audio_features.to(DEVICE), labels.to(DEVICE)\n",
        "            lyrics_batch = lyrics_list[:len(audio_features)]\n",
        "            metadata_batch = df_metadata[['artist_name', 'title']].iloc[:len(audio_features)].values\n",
        "            outputs = classifier(audio_features, lyrics_batch, metadata_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    precision_cls, recall_cls, f1_cls, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    print(f\"Hybrid Classification Precision: {precision_cls:.4f}, Recall: {recall_cls:.4f}, F1: {f1_cls:.4f}\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.title('Hybrid Classification Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'hybrid_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Instantiate the GenreClassifier\n",
        "    audio_only_model = GenreClassifier(\n",
        "        audio_dim=audio_features.shape[1],\n",
        "        text_dim=384,  # Still required, but will be filled with zeros\n",
        "        num_classes=len(genres)\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Prepare dummy lyrics (list of strings)\n",
        "    dummy_lyrics = [\"music\"] * len(X_test)\n",
        "\n",
        "    # Compare hybrid vs audio-only\n",
        "    hybrid_metrics, audio_metrics = compare_with_audio_only(\n",
        "        classifier,\n",
        "        audio_only_model,\n",
        "        torch.tensor(X_test[:, :audio_features.shape[1]], dtype=torch.float32, device=DEVICE),\n",
        "        dummy_lyrics,  # âœ… Correct\n",
        "        df_metadata[['artist_name', 'title']].iloc[:len(X_test)].values,\n",
        "        torch.tensor(y_test, dtype=torch.long, device=DEVICE)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nHybrid vs Audio-Only Metrics: Hybrid={hybrid_metrics}, Audio-Only={audio_metrics}\")\n",
        "\n",
        "    # 2.2: Transformer-Based Audio Classification\n",
        "    def generate_spectrograms(audio_path, track_ids):\n",
        "        spectrograms = []\n",
        "        for track_id in track_ids:\n",
        "            audio_file = os.path.join(audio_path, f\"{track_id}.mp3\")\n",
        "            if os.path.exists(audio_file):\n",
        "                y, sr = librosa.load(audio_file, sr=22050)\n",
        "                spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "                spec_db = librosa.power_to_db(spec, ref=np.max)\n",
        "                if spec_db.shape[1] > 128:\n",
        "                    spec_db = spec_db[:, :128]\n",
        "                else:\n",
        "                    spec_db = np.pad(spec_db, ((0, 0), (0, 128 - spec_db.shape[1])), mode='constant')\n",
        "                spectrograms.append(spec_db)\n",
        "            else:\n",
        "                spectrograms.append(np.zeros((128, 128)))\n",
        "        return np.array(spectrograms)\n",
        "\n",
        "    spectrograms = generate_spectrograms(AUDIO_PATH, df_metadata['track_id'].tolist())\n",
        "    spectrograms = spectrograms[:, np.newaxis, :, :]  # Add channel dimension\n",
        "    X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(\n",
        "        spectrograms, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    train_dataset_spec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_spec, dtype=torch.float32), torch.tensor(y_train_spec, dtype=torch.long)\n",
        "    )\n",
        "    train_loader_spec = torch.utils.data.DataLoader(train_dataset_spec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_spec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_spec, dtype=torch.float32), torch.tensor(y_test_spec, dtype=torch.long)\n",
        "    )\n",
        "    test_loader_spec = torch.utils.data.DataLoader(test_dataset_spec, batch_size=BATCH_SIZE)\n",
        "\n",
        "    ast = AudioSpectrogramTransformer(num_classes=len(genres)).to(DEVICE)\n",
        "    criterion_ast = nn.CrossEntropyLoss()\n",
        "    optimizer_ast = torch.optim.Adam(ast.parameters(), lr=0.001)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        ast.train()\n",
        "        total_loss = 0\n",
        "        for specs, labels in train_loader_spec:\n",
        "            specs, labels = specs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer_ast.zero_grad()\n",
        "            outputs = ast(specs)\n",
        "            loss = criterion_ast(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_ast.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"AST Epoch {epoch+1}, Loss: {total_loss/len(train_loader_spec):.4f}\")\n",
        "\n",
        "    ast.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for specs, labels in test_loader_spec:\n",
        "            specs, labels = specs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = ast(specs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    ast_metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    print(f\"AST Precision: {ast_metrics[0]:.4f}, Recall: {ast_metrics[1]:.4f}, F1: {ast_metrics[2]:.4f}\")\n",
        "    visualize_attention_patterns(ast, torch.tensor(X_test_spec[:1], dtype=torch.float32, device=DEVICE))\n",
        "\n",
        "    # 3.1: Semantic Music Search\n",
        "    search_system = MusicSearchSystem()\n",
        "    query = \"upbeat rock songs\"\n",
        "    results = search_system.multi_modal_search(query, audio_features, list(lyrics_dict.values()), df_metadata[['artist_name', 'title']].values, df_metadata)\n",
        "    print(\"\\nSearch Results:\")\n",
        "    print(results)\n",
        "\n",
        "    # 3.1.3: Content-Based Discovery\n",
        "    discovery = MusicDiscovery()\n",
        "    sample_audio = os.path.join(AUDIO_PATH, df_metadata['track_id'].iloc[0] + '.mp3')\n",
        "    sample_lyrics = lyrics_dict.get(df_metadata['track_id'].iloc[0], 'music')\n",
        "    tags = discovery.generate_tags(sample_audio, sample_lyrics)\n",
        "    print(\"\\nGenerated Tags:\")\n",
        "    print(tags)\n",
        "    playlist = discovery.generate_playlist({'audio_path': sample_audio, 'lyrics': sample_lyrics}, df_metadata.to_dict('records'), df_metadata)\n",
        "    print(\"\\nGenerated Playlist:\")\n",
        "    print(playlist)\n",
        "\n",
        "    # 4.1: Recommendation\n",
        "    recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=384, num_tracks=num_tracks).to(DEVICE)\n",
        "    criterion_rec = nn.BCELoss()\n",
        "    optimizer_rec = torch.optim.Adam(recommender.parameters(), lr=0.001)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss_rec = train_recommender(recommender, train_loader_rec, criterion_rec, optimizer_rec)\n",
        "        print(f\"Recommendation Epoch {epoch+1}, Loss: {loss_rec:.4f}\")\n",
        "\n",
        "    precision_rec = evaluate_recommender(recommender, test_loader_rec)\n",
        "    print(f\"Recommendation Precision@10: {precision_rec:.4f}\")\n",
        "    recommendations = recommender.recommend('user_001', 'upbeat', torch.tensor(audio_features, dtype=torch.float32, device=DEVICE),\n",
        "                                          torch.tensor(text_features, dtype=torch.float32, device=DEVICE), df_metadata)\n",
        "    recommendations = recommender.optimize_diversity(recommendations, df_metadata)\n",
        "    print(\"\\nRecommendations:\")\n",
        "    print(recommendations)\n",
        "\n",
        "    # Generate explanation for a sample recommendation\n",
        "    music_recommender = MusicRecommender(audio_dim=audio_features.shape[1], text_dim=384, num_tracks=num_tracks).to(DEVICE)\n",
        "    sample_item_id = recommendations['track_id'].iloc[0]\n",
        "    explanation = music_recommender.generate_explanation('user_001', sample_item_id, pd.concat([df_metadata, user_data[['rating']]], axis=1))\n",
        "    print(\"\\nRecommendation Explanation:\")\n",
        "    print(explanation)\n",
        "\n",
        "    # 5.1: Evaluation\n",
        "    evaluator = EvaluationFramework()\n",
        "    retrieval_metrics = evaluator.evaluate_retrieval(df_metadata[df_metadata['genre'] == 'Rock']['track_id'].tolist()[:10], results['track_id'].tolist())\n",
        "    print(\"\\nRetrieval Metrics:\")\n",
        "    print(retrieval_metrics)\n",
        "    rec_metrics = evaluator.evaluate_recommendation(recommendations, user_data[user_data['rating'] > 0.5]['track_id'].tolist())\n",
        "    print(\"\\nRecommendation Metrics:\")\n",
        "    print(rec_metrics)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset not found. Setting up FMA dataset...\n",
            "Cloning FMA repository...\n",
            "Downloading fma_small.zip...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}