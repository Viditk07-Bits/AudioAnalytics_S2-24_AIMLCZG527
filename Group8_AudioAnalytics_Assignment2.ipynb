{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viditk07-Bits/AudioAnalytics_S2-24_AIMLCZG527/blob/main/Group8_AudioAnalytics_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EN9uWs6-RQ"
      },
      "source": [
        "# Music Information Retrieval System\n",
        "\n",
        "## Assignment Objective\n",
        "This assignment implements a comprehensive Music Information Retrieval (MIR) system using Large Language Models (LLMs) and deep learning techniques. It includes music recommendation, genre classification, and semantic search applications, combining audio analysis with natural language processing.\n",
        "\n",
        "## Dataset Setup\n",
        "Using the Free Music Archive (FMA) dataset with audio files, metadata, and synthetic user data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssl9XfPLe2w2",
        "outputId": "8b029450-bd43-4e94-d3f0-e4021001fa2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed faiss-cpu\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install required packages at the start to avoid ModuleNotFoundError\n",
        "import subprocess\n",
        "required_packages = ['faiss-cpu', 'transformers', 'sentence-transformers', 'tqdm', 'imblearn', 'librosa', 'nltk', 'matplotlib', 'seaborn', 'datasets']\n",
        "for pkg in required_packages:\n",
        "    try:\n",
        "        __import__(pkg.replace('-', '_'))\n",
        "    except ImportError:\n",
        "        subprocess.run(['pip', 'install', pkg], check=True)\n",
        "        print(f\"Installed {pkg}\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor, Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, average_precision_score, ndcg_score\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import subprocess\n",
        "import logging\n",
        "import random\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from datasets import Dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "import faiss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm  # Use notebook-friendly progress bar\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable wandb logging\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Setup logging (only show INFO and above, suppress WARNING)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable wandb logging\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Constants (updated to use Google Drive)\n",
        "DATA_PATH = \"/content/drive/MyDrive/fma/data\"\n",
        "AUDIO_PATH = os.path.join(DATA_PATH, \"audio_files\")\n",
        "METADATA_PATH = os.path.join(DATA_PATH, \"metadata/tracks.csv\")\n",
        "ARTISTS_PATH = os.path.join(DATA_PATH, \"metadata/artists.csv\")\n",
        "GENRES_PATH = os.path.join(DATA_PATH, \"metadata/genres.csv\")\n",
        "LYRICS_PATH = os.path.join(DATA_PATH, \"lyrics\")\n",
        "USER_DATA_PATH = os.path.join(DATA_PATH, \"user_data/ratings.csv\")\n",
        "TAGS_PATH = os.path.join(DATA_PATH, \"descriptions/tags.csv\")\n",
        "\n",
        "# Output and temp directories can remain local\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "TEMP_DIR = \"/content/fma\"\n",
        "\n",
        "NUM_EPOCHS_REC = 5\n",
        "NUM_EPOCHS_CLS = 10\n",
        "BATCH_SIZE = 16\n",
        "MAX_TRACKS = 100\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TIMEOUT_SECONDS = 3600\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Enhanced genre-specific keyword pools for richer lyrics\n",
        "GENRE_KEYWORDS = {\n",
        "    'Rock': ['electric guitar wails', 'rebellious spirit soars', 'grunge heart pounds', 'classic riffs ignite', 'indie soul rebels', 'punk fire explodes', 'rock anthem roars'],\n",
        "    'Pop': ['infectious hooks dance', 'neon lights pulse', 'melodic dreams soar', 'upbeat rhythm shines', 'love story sparkles', 'dancefloor beats throb', 'pop fever rises'],\n",
        "    'Jazz': ['saxophone weaves magic', 'improvised notes flow', 'bluesy soul swings', 'smooth grooves linger', 'jazz night whispers', 'rhythmic scat hums', 'cool vibes drift'],\n",
        "    'Classical': ['orchestral swells rise', 'violin sings softly', 'piano echoes grace', 'symphonic waves crash', 'baroque harmony soars', 'elegant strings weave', 'timeless beauty unfolds'],\n",
        "    'Hip-Hop': ['heavy beats drop hard', 'sharp rhymes cut deep', 'street stories unfold', 'flow rides the rhythm', 'urban pulse vibrates', 'mic drops with swagger', 'hip-hop reigns supreme'],\n",
        "    'Electronic': ['synth pulses glow', 'techno beats surge', 'ambient waves drift', 'EDM sparks the night', 'futuristic sounds hum', 'electro vibes ignite', 'digital dreams pulse'],\n",
        "    'Folk': ['acoustic chords strum', 'heartfelt tales weave', 'rustic paths wander', 'folk roots run deep', 'gentle melodies soothe', 'campfire stories sing', 'tradition lives on'],\n",
        "    'Blues': ['guitar wails with soul', 'heartache spills over', 'raw blues cry out', 'delta notes resonate', 'mournful chords linger', 'blues spirit endures', 'emotional strings weep'],\n",
        "    'Country': ['banjo twangs with pride', 'heartland stories sing', 'cowboy boots stomp', 'rural roads ramble', 'love songs ride free', 'country heart beats strong', 'honky-tonk nights shine'],\n",
        "    'Reggae': ['rasta riddims sway', 'island vibes chill', 'roots reggae grooves', 'one love unites all', 'skank beat lifts high', 'irie spirit flows', 'dreadlocks dance free'],\n",
        "    'International': ['world rhythms blend', 'exotic melodies soar', 'cultural beats pulse', 'global sounds unite', 'traditional chants echo', 'fusion vibes transcend', 'earthâ€™s heartbeat sings'],\n",
        "    'Instrumental': ['ambient chords float', 'strings weave dreams', 'piano paints silence', 'orchestral tides rise', 'melody speaks alone', 'instrumental soul soars', 'soundscapes breathe life'],\n",
        "    'Experimental': ['avant-garde sounds twist', 'abstract beats morph', 'sonic boundaries break', 'unorthodox rhythms pulse', 'experimental vibes soar', 'sound art redefines', 'future notes unfold']\n",
        "}\n",
        "\n",
        "def download_fma_dataset():\n",
        "    \"\"\"Download and extract FMA small dataset with error handling and retries.\"\"\"\n",
        "    logging.info(\"Attempting to download FMA small dataset...\")\n",
        "    max_retries = 3\n",
        "    retry_delay = 5  # seconds\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            # Check if audio files and metadata exist and are valid\n",
        "            if os.path.exists(METADATA_PATH) and os.path.exists(AUDIO_PATH):\n",
        "                mp3_files = [f for f in os.listdir(AUDIO_PATH) if f.endswith('.mp3')]\n",
        "                if len(mp3_files) > 0 and os.path.exists(METADATA_PATH):\n",
        "                    logging.info(\"FMA dataset already exists at %s with %d audio files\", AUDIO_PATH, len(mp3_files))\n",
        "                    return True\n",
        "                else:\n",
        "                    logging.warning(\"Audio directory %s exists but contains no .mp3 files or metadata is missing. Triggering redownload.\", AUDIO_PATH)\n",
        "\n",
        "            # Clean up existing zip files to avoid corrupted downloads\n",
        "            for zip_file in [\"/content/fma_small.zip\", \"/content/fma_metadata.zip\"]:\n",
        "                if os.path.exists(zip_file):\n",
        "                    logging.info(\"Removing existing %s for redownload\", zip_file)\n",
        "                    os.remove(zip_file)\n",
        "\n",
        "            # Create directories\n",
        "            os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n",
        "            os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "\n",
        "            # Download and extract audio files\n",
        "            logging.info(\"Downloading fma_small.zip (attempt %d/%d)...\", attempt, max_retries)\n",
        "            subprocess.run([\"wget\", \"-q\", \"https://os.unil.cloud.switch.ch/fma/fma_small.zip\", \"-O\", \"/content/fma_small.zip\"], check=True, timeout=TIMEOUT_SECONDS)\n",
        "            logging.info(\"Extracting fma_small.zip...\")\n",
        "            subprocess.run([\"unzip\", \"-q\", \"/content/fma_small.zip\", \"-d\", DATA_PATH], check=True, timeout=TIMEOUT_SECONDS)\n",
        "\n",
        "            # Download and extract metadata\n",
        "            logging.info(\"Downloading fma_metadata.zip (attempt %d/%d)...\", attempt, max_retries)\n",
        "            subprocess.run([\"wget\", \"-q\", \"https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\", \"-O\", \"/content/fma_metadata.zip\"], check=True, timeout=TIMEOUT_SECONDS)\n",
        "            logging.info(\"Extracting fma_metadata.zip to %s...\", os.path.dirname(METADATA_PATH))\n",
        "            subprocess.run([\"unzip\", \"-q\", \"/content/fma_metadata.zip\", \"-d\", os.path.dirname(METADATA_PATH)], check=True, timeout=TIMEOUT_SECONDS)\n",
        "\n",
        "            # Verify metadata and audio files\n",
        "            if not os.path.exists(METADATA_PATH):\n",
        "                logging.error(\"FMA metadata file (%s) not found after extraction.\", METADATA_PATH)\n",
        "                if attempt == max_retries:\n",
        "                    return False\n",
        "                logging.info(\"Retrying download after %d seconds...\", retry_delay)\n",
        "                import time\n",
        "                time.sleep(retry_delay)\n",
        "                continue\n",
        "\n",
        "            mp3_files = [f for f in os.listdir(AUDIO_PATH) if f.endswith('.mp3')] if os.path.exists(AUDIO_PATH) else []\n",
        "            metadata_files = [f for f in os.listdir(os.path.dirname(METADATA_PATH)) if f.endswith('.csv')]\n",
        "            logging.info(\"FMA dataset downloaded successfully. Found %d audio files and %d metadata files.\", len(mp3_files), len(metadata_files))\n",
        "            return True\n",
        "        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, Exception) as e:\n",
        "            logging.error(\"Failed to download FMA dataset on attempt %d: %s\", attempt, str(e))\n",
        "            if attempt == max_retries:\n",
        "                logging.error(\"Max retries reached. Falling back to synthetic dataset.\")\n",
        "                return False\n",
        "            logging.info(\"Retrying download after %d seconds...\", retry_delay)\n",
        "            import time\n",
        "            time.sleep(retry_delay)\n",
        "\n",
        "def generate_lyrics(args):\n",
        "    \"\"\"Helper function to generate richer synthetic lyrics.\"\"\"\n",
        "    track_id, genre, lyrics_path = args\n",
        "    keywords = random.sample(GENRE_KEYWORDS[genre], min(3, len(GENRE_KEYWORDS[genre])))\n",
        "    verse_lines = [\n",
        "        f\"Feel the {keywords[0].split()[0]} in your soul, let it take control.\",\n",
        "        f\"{keywords[1].split()[0]} carries you away, into the night and day.\",\n",
        "        f\"With every {keywords[2].split()[0]} the heart beats strong, singing {genre}â€™s song.\"\n",
        "    ]\n",
        "    chorus_lines = [\n",
        "        f\"{keywords[1].split()[0]} vibes, weâ€™re alive, dancing through the night.\",\n",
        "        f\"{genre} spirit, feel it rise, reaching for the skies.\",\n",
        "        f\"Let the {keywords[0].split()[0]} flow, take us where we go.\"\n",
        "    ]\n",
        "    lyrics = f\"{genre} song: {', '.join(keywords)}.\\n\" + \\\n",
        "             f\"Verse 1:\\n{verse_lines[0]}\\n{verse_lines[1]}\\n{verse_lines[2]}\\n\" + \\\n",
        "             f\"Chorus:\\n{chorus_lines[0]}\\n{chorus_lines[1]}\\n{chorus_lines[2]}\"\n",
        "    with open(os.path.join(lyrics_path, f\"{track_id}.txt\"), 'w', encoding='utf-8') as f:\n",
        "        f.write(lyrics)\n",
        "    return track_id\n",
        "\n",
        "def create_synthetic_dataset():\n",
        "    \"\"\"Task A1, A3, A5: Create synthetic dataset with FMA fallback and denser user data.\"\"\"\n",
        "    logging.info(\"Creating synthetic dataset...\")\n",
        "    print(\"Creating synthetic dataset...\")\n",
        "\n",
        "    # Ensure all output directories exist\n",
        "    os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(ARTISTS_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(GENRES_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(USER_DATA_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(TAGS_PATH), exist_ok=True)\n",
        "    logging.info(\"Created output directories: %s, %s, %s\", os.path.dirname(METADATA_PATH), os.path.dirname(USER_DATA_PATH), os.path.dirname(TAGS_PATH))\n",
        "\n",
        "    fma_available = download_fma_dataset()\n",
        "\n",
        "    if fma_available:\n",
        "        try:\n",
        "            fma_metadata = pd.read_csv(METADATA_PATH, header=[0, 1], low_memory=False)\n",
        "            fma_metadata.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in fma_metadata.columns]\n",
        "            track_id_col = None\n",
        "            title_col = None\n",
        "            artist_id_col = None\n",
        "            genre_top_col = None\n",
        "            for col in fma_metadata.columns:\n",
        "                col_lower = col.lower()\n",
        "                if 'track_id' in col_lower or 'track.id' in col_lower:\n",
        "                    track_id_col = col\n",
        "                elif 'title' in col_lower:\n",
        "                    title_col = col\n",
        "                elif 'artist_id' in col_lower or 'artist.id' in col_lower:\n",
        "                    artist_id_col = col\n",
        "                elif 'genre_top' in col_lower or 'genre' in col_lower:\n",
        "                    genre_top_col = col\n",
        "            required_cols = [track_id_col, title_col, artist_id_col, genre_top_col]\n",
        "            if None in required_cols:\n",
        "                logging.error(\"Required columns missing in FMA metadata: %s\", required_cols)\n",
        "                fma_available = False\n",
        "            else:\n",
        "                fma_metadata = fma_metadata[[track_id_col, title_col, artist_id_col, genre_top_col]].dropna()\n",
        "                fma_metadata = fma_metadata.rename(columns={\n",
        "                    track_id_col: 'track_id',\n",
        "                    title_col: 'title',\n",
        "                    artist_id_col: 'artist_id',\n",
        "                    genre_top_col: 'genre_top'\n",
        "                })\n",
        "                fma_metadata['track_id'] = fma_metadata['track_id'].astype(str).str.zfill(6)\n",
        "                # Filter to tracks with existing audio files\n",
        "                mp3_files = {f.replace('.mp3', '') for f in os.listdir(AUDIO_PATH) if f.endswith('.mp3')} if os.path.exists(AUDIO_PATH) else set()\n",
        "                fma_metadata = fma_metadata[fma_metadata['track_id'].isin(mp3_files)]\n",
        "                # Ensure exactly MAX_TRACKS by sampling or padding\n",
        "                if len(fma_metadata) > MAX_TRACKS:\n",
        "                    fma_metadata = fma_metadata.sample(n=MAX_TRACKS, random_state=42)\n",
        "                elif len(fma_metadata) < MAX_TRACKS:\n",
        "                    # Pad with synthetic data if FMA has fewer tracks\n",
        "                    num_missing = MAX_TRACKS - len(fma_metadata)\n",
        "                    genres = list(GENRE_KEYWORDS.keys())\n",
        "                    synthetic_tracks = pd.DataFrame({\n",
        "                        'track_id': [str(i).zfill(6) for i in range(len(fma_metadata) + 1, len(fma_metadata) + num_missing + 1)],\n",
        "                        'title': [f\"Track_{i}\" for i in range(len(fma_metadata) + 1, len(fma_metadata) + num_missing + 1)],\n",
        "                        'artist_id': [str(i).zfill(6) for i in range(len(fma_metadata) + 1, len(fma_metadata) + num_missing + 1)],\n",
        "                        'genre_top': [random.choice(genres) for _ in range(num_missing)]\n",
        "                    })\n",
        "                    fma_metadata = pd.concat([fma_metadata, synthetic_tracks], ignore_index=True)\n",
        "                fma_metadata['genre_id'] = fma_metadata['genre_top'].map({g: i+1 for i, g in enumerate(GENRE_KEYWORDS.keys())}).fillna(random.randint(1, 13))\n",
        "        except (FileNotFoundError, Exception) as e:\n",
        "            logging.error(\"Error loading FMA metadata: %s\", str(e))\n",
        "            fma_available = False\n",
        "\n",
        "    if not fma_available:\n",
        "        logging.warning(\"Falling back to synthetic dataset due to missing or invalid FMA data...\")\n",
        "        genres = list(GENRE_KEYWORDS.keys())\n",
        "        df_tracks = pd.DataFrame({\n",
        "            'track_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "            'title': [f\"Track_{i}\" for i in range(1, MAX_TRACKS + 1)],\n",
        "            'artist_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "            'genre_id': [random.randint(1, 13) for _ in range(MAX_TRACKS)],\n",
        "            'genre_top': [random.choice(genres) for _ in range(MAX_TRACKS)]\n",
        "        })\n",
        "    else:\n",
        "        df_tracks = fma_metadata[['track_id', 'title', 'artist_id', 'genre_id', 'genre_top']]\n",
        "\n",
        "    df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "    logging.info(\"Saved tracks metadata to %s\", METADATA_PATH)\n",
        "\n",
        "    df_artists = pd.DataFrame({\n",
        "        'artist_id': df_tracks['artist_id'],\n",
        "        'artist_name': [f\"Artist_{i}\" for i in range(1, len(df_tracks) + 1)]\n",
        "    })\n",
        "    df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "    logging.info(\"Saved artists metadata to %s\", ARTISTS_PATH)\n",
        "\n",
        "    df_genres = pd.DataFrame({\n",
        "        'genre_id': range(1, 14),\n",
        "        'genre_name': list(GENRE_KEYWORDS.keys())\n",
        "    })\n",
        "    df_genres.to_csv(GENRES_PATH, index=False)\n",
        "    logging.info(\"Saved genres metadata to %s\", GENRES_PATH)\n",
        "\n",
        "    genre_rating_bias = {\n",
        "        'Pop': 0.2, 'Rock': 0.1, 'Jazz': -0.1, 'Classical': -0.2, 'Hip-Hop': 0.15,\n",
        "        'Electronic': 0.1, 'Folk': -0.05, 'Blues': -0.1, 'Country': -0.05, 'Reggae': 0.05,\n",
        "        'International': 0.05, 'Instrumental': -0.2, 'Experimental': -0.1\n",
        "    }\n",
        "    ratings = []\n",
        "    for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top']):\n",
        "        for user_id in [f\"user_{i+1}\" for i in range(20)]:\n",
        "            if np.random.random() < 0.9:\n",
        "                rating = min(max(np.random.normal(3.0 + genre_rating_bias.get(genre, 0), 0.5), 1), 5)\n",
        "                ratings.append({'user_id': user_id, 'track_id': track_id, 'rating': rating})\n",
        "    ratings = pd.DataFrame(ratings)\n",
        "    ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "    logging.info(\"Saved user ratings to %s\", USER_DATA_PATH)\n",
        "\n",
        "    tags = []\n",
        "    for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top']):\n",
        "        num_tags = random.randint(3, 6)\n",
        "        track_tags = random.sample(GENRE_KEYWORDS.get(genre, ['generic']), min(num_tags, len(GENRE_KEYWORDS.get(genre, ['generic']))))\n",
        "        tags.extend([{'track_id': track_id, 'tag': tag} for tag in track_tags])\n",
        "    pd.DataFrame(tags).to_csv(TAGS_PATH, index=False)\n",
        "    logging.info(\"Saved tags to %s\", TAGS_PATH)\n",
        "\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    os.makedirs(LYRICS_PATH, exist_ok=True)\n",
        "    lyrics_args = [(track_id, genre, LYRICS_PATH) for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top'])]\n",
        "    with Pool(processes=4) as pool:\n",
        "        list(tqdm(pool.imap(generate_lyrics, lyrics_args), total=len(lyrics_args), desc=\"Generating synthetic lyrics\"))\n",
        "\n",
        "    print(f\"Synthetic dataset created: Tracks shape: {df_tracks.shape}, Ratings shape: {ratings.shape}, Tags: {len(tags)}\")\n",
        "    logging.info(\"Synthetic dataset created: Tracks shape: %s, Ratings: %s, Tags: %d\", df_tracks.shape, ratings.shape, len(tags))\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    \"\"\"Task A1.1, A2: Extract audio features using Librosa.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(audio_path):\n",
        "            return np.zeros(26)  # Silently return zeros for missing files\n",
        "        if os.path.getsize(audio_path) < 100:\n",
        "            return np.zeros(26)  # Silently return zeros for small files\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        if len(y) == 0:\n",
        "            return np.zeros(26)  # Silently return zeros for empty audio\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        tempo = librosa.feature.tempo(y=y, sr=sr)\n",
        "        tempo = float(tempo[0]) if isinstance(tempo, np.ndarray) else float(tempo)\n",
        "        features = np.concatenate([\n",
        "            np.mean(mfccs, axis=1),\n",
        "            np.mean(chroma, axis=1),\n",
        "            np.mean(spectral_centroid, axis=1),\n",
        "            [tempo]\n",
        "        ])\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Error processing %s: %s\", audio_path, str(e))\n",
        "        return np.zeros(26)\n",
        "\n",
        "def load_fma_data(audio_path, metadata_path, artists_path, genres_path, lyrics_path, tags_path):\n",
        "    \"\"\"Task A1, A3, A5: Load synthetic dataset.\"\"\"\n",
        "    logging.info(\"Loading synthetic data...\")\n",
        "    print(\"Loading synthetic data...\")\n",
        "    create_synthetic_dataset()\n",
        "\n",
        "    df_metadata = pd.read_csv(metadata_path)\n",
        "    df_metadata = pd.merge(df_metadata, pd.read_csv(artists_path), on='artist_id', how='left')\n",
        "    df_metadata = pd.merge(df_metadata, pd.read_csv(genres_path), on='genre_id', how='left')\n",
        "    df_metadata = df_metadata[['track_id', 'artist_name', 'title', 'genre_name']].rename(columns={'genre_name': 'genre'})\n",
        "    df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "    logging.info(\"Metadata shape: %s\", df_metadata.shape)\n",
        "    print(f\"Metadata shape: {df_metadata.shape}\")\n",
        "\n",
        "    # Ensure all track_ids are valid for feature extraction\n",
        "    valid_track_ids = df_metadata['track_id'].tolist()\n",
        "    features = []\n",
        "    for track_id in valid_track_ids:\n",
        "        audio_file = os.path.join(audio_path, f\"{track_id}.mp3\")\n",
        "        audio_features = extract_audio_features(audio_file)\n",
        "        features.append([track_id] + audio_features.tolist())\n",
        "    feature_columns = ['track_id'] + [f'mfcc_{i+1}' for i in range(12)] + [f'chroma_{i+1}' for i in range(12)] + ['spectral_centroid', 'tempo']\n",
        "    if len(feature_columns) != 27:\n",
        "        logging.error(\"Feature columns mismatch: expected 27, got %d\", len(feature_columns))\n",
        "        raise ValueError(f\"Feature columns mismatch: expected 27, got {len(feature_columns)}\")\n",
        "    df_features = pd.DataFrame(features, columns=feature_columns)\n",
        "    logging.info(\"Features shape: %s\", df_features.shape)\n",
        "    print(f\"Features shape: {df_features.shape}\")\n",
        "\n",
        "    lyrics_dict = {}\n",
        "    for lyric_file in Path(lyrics_path).glob(\"*.txt\"):\n",
        "        track_id = lyric_file.stem\n",
        "        if track_id in valid_track_ids:\n",
        "            with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                lyrics_dict[track_id] = f.read().strip() or 'music'\n",
        "\n",
        "    if tags_path and os.path.exists(tags_path):\n",
        "        df_tags = pd.read_csv(tags_path)\n",
        "        for _, row in df_tags.iterrows():\n",
        "            track_id = str(row['track_id']).zfill(6)\n",
        "            if track_id in valid_track_ids:\n",
        "                lyrics_dict[track_id] = lyrics_dict.get(track_id, '') + \" \" + str(row['tag'])\n",
        "\n",
        "    logging.info(\"Lyrics dict size: %d\", len(lyrics_dict))\n",
        "    print(f\"Lyrics dict size: {len(lyrics_dict)}\")\n",
        "    return df_metadata, df_features, lyrics_dict\n",
        "\n",
        "def generate_text_embeddings(lyrics_dict):\n",
        "    \"\"\"Task A1.2, A3: Generate semantic embeddings for lyrics.\"\"\"\n",
        "    logging.info(\"Generating text embeddings...\")\n",
        "    print(\"Generating text embeddings...\")\n",
        "\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    embeddings = {}\n",
        "\n",
        "    for track_id, text in tqdm(lyrics_dict.items(), desc=\"Generating text embeddings\"):\n",
        "        embeddings[track_id] = model.encode(\n",
        "            text,\n",
        "            convert_to_tensor=True,\n",
        "            device=DEVICE,\n",
        "            show_progress_bar=False  # ðŸ‘ˆ disables internal batching progress\n",
        "        ).cpu().numpy()\n",
        "\n",
        "    logging.info(\"Text embeddings generated for %d tracks.\", len(embeddings))\n",
        "    print(f\"Text embeddings generated for {len(embeddings)} tracks.\")\n",
        "    return embeddings\n",
        "\n",
        "def analyze_linguistic_patterns(df_metadata, lyrics_dict):\n",
        "    \"\"\"Task A1.2: Analyze linguistic patterns and topics.\"\"\"\n",
        "    print(\"\\nLinguistic Analysis:\")\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    genre_words = {g: [] for g in df_metadata['genre'].unique()}\n",
        "    for track_id, text in lyrics_dict.items():\n",
        "        genre = df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "        genre_words[genre].extend(tokens)\n",
        "    for genre, words in genre_words.items():\n",
        "        top_words = Counter(words).most_common(5)\n",
        "        print(f\"{genre} top words: {top_words}\")\n",
        "        logging.info(\"%s top words: %s\", genre, top_words)\n",
        "\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(lyrics_dict.values())\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(X)\n",
        "    for i, topic in enumerate(lda.components_):\n",
        "        print(f\"Topic {i}: {[vectorizer.get_feature_names_out()[j] for j in topic.argsort()[-5:]]}\")\n",
        "\n",
        "class AudioTextBERTClassifier(nn.Module):\n",
        "    \"\"\"BERT with audio feature integration.\"\"\"\n",
        "    def __init__(self, bert_model, audio_dim, num_classes, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model  # Now using BertModel\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.combined_layer = nn.Linear(self.bert.config.hidden_size + hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features):\n",
        "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_out = bert_outputs.pooler_output  # Safe to use with BertModel\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        combined = torch.cat([text_out, audio_out], dim=1)\n",
        "        combined = torch.relu(self.combined_layer(combined))\n",
        "        combined = self.dropout(combined)\n",
        "        return self.fc(combined)\n",
        "\n",
        "class HybridRecommender(nn.Module):\n",
        "    \"\"\"Task A5: Content-based recommender.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "        return torch.sigmoid(self.fc(combined))\n",
        "\n",
        "def train_recommender(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Task A5: Train the recommender.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, ratings in train_loader:\n",
        "        audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, ratings.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_recommender(model, test_loader, df_metadata, test_idx_resampled, original_indices_resampled, genres, k=10):\n",
        "    \"\"\"Task C5: Evaluate recommender.\"\"\"\n",
        "    model.eval()\n",
        "    precisions, recalls, maps, ndcgs, diversities = [], [], [], [], []\n",
        "    global train_indices\n",
        "    if 'train_indices' not in globals():\n",
        "        train_indices = []\n",
        "    train_track_ids = set(df_metadata.iloc[train_indices]['track_id'])\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (audio_features, text_features, ratings) in enumerate(test_loader):\n",
        "            audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            binary_ratings = (ratings > 0.6).float()\n",
        "            top_k = torch.topk(outputs, k=min(k, outputs.size(0)), dim=0).indices.flatten()\n",
        "            batch_start = batch_idx * test_loader.batch_size\n",
        "            batch_end = batch_start + len(audio_features)\n",
        "            batch_test_idx_resampled = test_idx_resampled[batch_start:batch_end]\n",
        "            batch_original_indices = original_indices_resampled[batch_test_idx_resampled]\n",
        "            top_k_original_indices = batch_original_indices[top_k.cpu()]\n",
        "            top_k_ids = df_metadata.iloc[top_k_original_indices]['track_id']\n",
        "            top_k_genres = df_metadata.iloc[top_k_original_indices]['genre']\n",
        "            relevant = binary_ratings[top_k]\n",
        "            precision = relevant.mean().item()\n",
        "            recall = relevant.sum().item() / binary_ratings.sum().item() if binary_ratings.sum() > 0 else 0.0\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            if binary_ratings.sum() > 0:\n",
        "                map_score = average_precision_score(binary_ratings.cpu().numpy(), outputs.cpu().numpy())\n",
        "                ndcg = ndcg_score(binary_ratings.cpu().numpy().reshape(1, -1), outputs.cpu().numpy().reshape(1, -1), k=k)\n",
        "            else:\n",
        "                map_score, ndcg = 0.0, 0.0\n",
        "            maps.append(map_score)\n",
        "            ndcgs.append(ndcg)\n",
        "            diversity = len(set(top_k_genres)) / len(genres) if len(genres) > 0 else 1.0\n",
        "            diversities.append(diversity)\n",
        "            novelty = 1 - len(set(top_k_ids).intersection(train_track_ids)) / len(top_k_ids) if len(top_k_ids) > 0 else 1.0\n",
        "    return {\n",
        "        'precision@k': np.mean(precisions) if precisions else 0.0,\n",
        "        'recall@k': np.mean(recalls) if recalls else 0.0,\n",
        "        'map': np.mean(maps) if maps else 0.0,\n",
        "        'ndcg@k': np.mean(ndcgs) if ndcgs else 0.0,\n",
        "        'diversity': np.mean(diversities) if diversities else 1.0,\n",
        "        'novelty': novelty\n",
        "    }\n",
        "\n",
        "def collaborative_filtering(user_data, df_metadata, k=10):\n",
        "    \"\"\"Task A5: Collaborative filtering using SVD.\"\"\"\n",
        "    print(\"\\nTraining Collaborative Filtering Model (SVD)...\")\n",
        "    valid_track_ids = set(df_metadata['track_id'].astype(str).str.zfill(6))\n",
        "    user_data = user_data[user_data['track_id'].isin(valid_track_ids)]\n",
        "\n",
        "    if user_data.empty:\n",
        "        logging.error(\"No valid user data.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    train_data, test_data = train_test_split(user_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Deduplicate both train and test sets\n",
        "    train_data = train_data.groupby(['user_id', 'track_id'], as_index=False)['rating'].mean()\n",
        "    test_data = test_data.groupby(['user_id', 'track_id'], as_index=False)['rating'].mean()\n",
        "\n",
        "    user_item_matrix = train_data.pivot(index='user_id', columns='track_id', values='rating').fillna(0)\n",
        "    if user_item_matrix.empty:\n",
        "        logging.error(\"Empty user-item matrix.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
        "    user_features = svd.fit_transform(user_item_matrix)\n",
        "    item_features = svd.components_\n",
        "    predictions = np.dot(user_features, item_features)\n",
        "    predicted_ratings = pd.DataFrame(predictions, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
        "\n",
        "    test_matrix = test_data.pivot(index='user_id', columns='track_id', values='rating').fillna(0)\n",
        "    shared_tracks = set(test_matrix.columns).intersection(set(predicted_ratings.columns))\n",
        "    precisions = []\n",
        "    for user_id in test_matrix.index:\n",
        "        if user_id in predicted_ratings.index:\n",
        "            true_ratings = test_matrix.loc[user_id]\n",
        "            pred_ratings = predicted_ratings.loc[user_id]\n",
        "            valid_top_k = pred_ratings[pred_ratings.index.isin(shared_tracks)].sort_values(ascending=False).index[:k]\n",
        "            if len(valid_top_k) == 0:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "            relevant = (true_ratings[valid_top_k] > 3.0).astype(int)\n",
        "            precision = relevant.mean()\n",
        "            precisions.append(precision)\n",
        "    return {'precision@k': np.mean(precisions) if precisions else 0.0}\n",
        "\n",
        "class GenreClassifier(nn.Module):\n",
        "    \"\"\"Task A1.1: Custom genre classifier.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, num_classes, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "        return self.fc(combined)\n",
        "\n",
        "def train_classifier(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Task A1.1: Train the classifier.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, labels in train_loader:\n",
        "        audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_classifier(model, test_loader, genres):\n",
        "    \"\"\"Task C5: Evaluate classifier with error analysis.\"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    errors = [(i, genres[true], genres[pred]) for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
        "    error_counts = Counter((true, pred) for _, true, pred in errors)\n",
        "    print(\"\\nMisclassification Patterns:\")\n",
        "    for (true, pred), count in error_counts.most_common():\n",
        "        print(f\"True: {true}, Predicted: {pred}, Count: {count}\")\n",
        "\n",
        "    return {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n",
        "\n",
        "def ast_classifier(audio_files, labels, genres, df_metadata):\n",
        "    \"\"\"Task A2: AST-based audio classification.\"\"\"\n",
        "    print(\"\\nSetting up AST Classifier...\")\n",
        "    try:\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.2\")\n",
        "        model = AutoModelForAudioClassification.from_pretrained(\n",
        "            \"MIT/ast-finetuned-audioset-10-10-0.2\",\n",
        "            num_labels=len(genres),\n",
        "            label2id={genre: idx for idx, genre in enumerate(genres)},\n",
        "            id2label={idx: genre for idx, genre in enumerate(genres)}\n",
        "        ).to(DEVICE)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to load AST model: %s\", str(e))\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    valid_audio_files = [f for f in audio_files if os.path.exists(f) and os.path.getsize(f) > 100]\n",
        "    valid_labels = [labels[i] for i, f in enumerate(audio_files) if f in valid_audio_files]\n",
        "\n",
        "    def preprocess_audio(audio_file, label):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_file, sr=16000, duration=5.0)\n",
        "            inputs = feature_extractor(y, sampling_rate=16000, return_tensors=\"np\", padding=\"max_length\", max_length=80000)\n",
        "            return {\n",
        "                \"input_values\": inputs.input_values[0],\n",
        "                \"labels\": label\n",
        "            }\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    data = [preprocess_audio(f, l) for f, l in zip(valid_audio_files, valid_labels)]\n",
        "    data = [d for d in data if d is not None]\n",
        "    if not data:\n",
        "        logging.error(\"No valid audio data.\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"input_values\": [d[\"input_values\"] for d in data],\n",
        "        \"labels\": [d[\"labels\"] for d in data]\n",
        "    })\n",
        "    dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(OUTPUT_DIR, \"ast-finetuned\"),\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        logging_steps=100,\n",
        "        learning_rate=3e-5,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        compute_metrics=lambda pred: {'precision': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[0],\n",
        "                                     'recall': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[1],\n",
        "                                     'f1': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[2]}\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    return {'precision': eval_results['eval_precision'], 'recall': eval_results['eval_recall'], 'f1': eval_results['eval_f1']}\n",
        "\n",
        "def music_search(query, df, text_embeddings, audio_features_df, model, scaler, k=10):\n",
        "    \"\"\"Task A3: Content-based music discovery with retrieval metrics.\"\"\"\n",
        "    logging.info(\"Performing music search for query: %s\", query)\n",
        "    print(f\"Performing music search for query: {query}\")\n",
        "\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
        "\n",
        "    text_embedding_matrix = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df['track_id']])\n",
        "    text_embedding_matrix = text_embedding_matrix / np.linalg.norm(text_embedding_matrix, axis=1, keepdims=True)\n",
        "    text_index = faiss.IndexFlatIP(text_embedding_matrix.shape[1])\n",
        "    text_index.add(text_embedding_matrix.astype(np.float32))\n",
        "\n",
        "    text_scores, text_indices = text_index.search(query_embedding.reshape(1, -1).astype(np.float32), k=20)\n",
        "\n",
        "    audio_features = audio_features_df[[col for col in audio_features_df.columns if col != 'track_id']].values\n",
        "    audio_features = scaler.transform(np.nan_to_num(audio_features))\n",
        "    audio_features = audio_features / np.linalg.norm(audio_features, axis=1, keepdims=True)\n",
        "\n",
        "    combined_scores = {}\n",
        "    for idx, score in zip(text_indices[0], text_scores[0]):\n",
        "        track_id = df['track_id'].iloc[idx]\n",
        "        genre = df[df['track_id'] == track_id]['genre'].iloc[0]\n",
        "        genre_weight = 1.5 if genre == 'Rock' else 1.0\n",
        "        combined_scores[track_id] = 0.7 * score * genre_weight\n",
        "        audio_idx = audio_features_df.index[audio_features_df['track_id'] == track_id].tolist()\n",
        "        if audio_idx:\n",
        "            audio_idx = audio_idx[0]\n",
        "            genre_tracks = df[df['genre'] == genre]['track_id']\n",
        "            genre_audio_features = audio_features[audio_features_df['track_id'].isin(genre_tracks)]\n",
        "            if len(genre_audio_features) > 0:\n",
        "                avg_genre_audio = np.mean(genre_audio_features, axis=0)\n",
        "                audio_similarity = 1 - cosine(audio_features[audio_idx], avg_genre_audio)\n",
        "                combined_scores[track_id] += 0.3 * audio_similarity * genre_weight\n",
        "\n",
        "    top_tracks = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "    result_ids = [track_id for track_id, _ in top_tracks]\n",
        "    results = df[df['track_id'].isin(result_ids)][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    # Calculate retrieval metrics\n",
        "    relevant_tracks = df[df['genre'] == 'Rock']['track_id'].values  # Assuming query targets Rock\n",
        "    y_true = [1 if track_id in relevant_tracks else 0 for track_id in result_ids]\n",
        "    y_scores = [combined_scores[track_id] for track_id in result_ids]\n",
        "    precision = sum(y_true) / len(y_true) if len(y_true) > 0 else 0.0\n",
        "    recall = sum(y_true) / len(relevant_tracks) if len(relevant_tracks) > 0 else 0.0\n",
        "    map_score = average_precision_score(y_true, y_scores) if sum(y_true) > 0 else 0.0\n",
        "    ndcg = ndcg_score(np.array(y_true).reshape(1, -1), np.array(y_scores).reshape(1, -1), k=k) if sum(y_true) > 0 else 0.0\n",
        "    diversity = len(set(results['genre'])) / len(df['genre'].unique()) if len(df['genre'].unique()) > 0 else 1.0\n",
        "    novelty = len(set(result_ids) - set(df['track_id'].iloc[train_indices])) / len(result_ids) if len(result_ids) > 0 else 1.0\n",
        "\n",
        "    print(f\"\\nSearch Results for '{query}':\")\n",
        "    print(results.to_string(index=False))\n",
        "    print(f\"Retrieval Metrics: Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, MAP: {map_score:.4f}, NDCG@{k}: {ndcg:.4f}, Diversity: {diversity:.4f}, Novelty: {novelty:.4f}\")\n",
        "    return results, {'precision@k': precision, 'recall@k': recall, 'map': map_score, 'ndcg@k': ndcg, 'diversity': diversity, 'novelty': novelty}\n",
        "\n",
        "def zero_shot_classification(df_metadata, lyrics_dict):\n",
        "    \"\"\"Task A1.2: Zero-shot genre classification.\"\"\"\n",
        "    print(\"\\nZero-Shot Genre Classification:\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if torch.cuda.is_available() else -1)\n",
        "    genres = df_metadata['genre'].unique().tolist()\n",
        "    results = []\n",
        "    for _, row in df_metadata.head(5).iterrows():\n",
        "        text = row['title'] + \" \" + lyrics_dict.get(row['track_id'], \"\")\n",
        "        scores = classifier(text, candidate_labels=genres, multi_label=False)\n",
        "        results.append((row['track_id'], scores['labels'][0], scores['scores'][0], row['genre']))\n",
        "        print(f\"Track {row['track_id']}: Predicted={scores['labels'][0]} ({scores['scores'][0]:.4f}), True={row['genre']}\")\n",
        "    accuracy = sum(1 for _, pred, _, true in results if pred == true) / len(results)\n",
        "    print(f\"Zero-Shot Accuracy: {accuracy:.4f}\")\n",
        "    return results\n",
        "\n",
        "def few_shot_classification(df_metadata, lyrics_dict):\n",
        "    \"\"\"Task A1.2: Few-shot genre classification.\"\"\"\n",
        "    print(\"\\nFew-Shot Genre Classification:\")\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df_metadata['genre'].unique())).to(DEVICE)\n",
        "\n",
        "    genres = df_metadata['genre'].unique().tolist()\n",
        "    few_shot_data = []\n",
        "    for genre in genres:\n",
        "        genre_tracks = df_metadata[df_metadata['genre'] == genre].head(5)\n",
        "        for _, row in genre_tracks.iterrows():\n",
        "            text = row['title'] + \" \" + lyrics_dict.get(row['track_id'], \"\")\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n",
        "            few_shot_data.append((inputs, genres.index(genre)))\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    for epoch in range(5):\n",
        "        for inputs, label in few_shot_data:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs, labels=torch.tensor([label]).to(DEVICE))\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    results = []\n",
        "    for _, row in df_metadata.head(5).iterrows():\n",
        "        text = row['title'] + \" \" + lyrics_dict.get(row['track_id'], \"\")\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            predicted = torch.argmax(outputs.logits, dim=1).item()\n",
        "        results.append((row['track_id'], genres[predicted], row['genre']))\n",
        "        print(f\"Track {row['track_id']}: Predicted={genres[predicted]}, True={row['genre']}\")\n",
        "    accuracy = sum(1 for _, pred, true in results if pred == true) / len(results)\n",
        "    print(f\"Few-Shot Accuracy: {accuracy:.4f}\")\n",
        "    return results\n",
        "\n",
        "def analyze_feature_contribution(model, test_loader, feature_columns):\n",
        "    \"\"\"Task A1.1: Analyze audio feature contributions.\"\"\"\n",
        "    print(\"\\nFeature Contribution Analysis:\")\n",
        "    model.eval()\n",
        "    contributions = {col: [] for col in feature_columns}\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, _ in test_loader:\n",
        "            audio_features, text_features = audio_features.to(DEVICE), text_features.to(DEVICE)\n",
        "            baseline_output = model(audio_features, text_features)\n",
        "            for i, col in enumerate(feature_columns):\n",
        "                modified_features = audio_features.clone()\n",
        "                modified_features[:, i] = 0\n",
        "                modified_output = model(modified_features, text_features)\n",
        "                diff = torch.mean(torch.abs(baseline_output - modified_output)).item()\n",
        "                contributions[col].append(diff)\n",
        "    for col in feature_columns:\n",
        "        print(f\"{col}: Mean contribution = {np.mean(contributions[col]):.4f}\")\n",
        "    return contributions\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Main function orchestrating all tasks.\"\"\"\n",
        "#     logging.info(\"Starting main execution...\")\n",
        "#     print(\"Starting main execution...\")\n",
        "\n",
        "#     df_metadata, df_features, lyrics_dict = load_fma_data(AUDIO_PATH, METADATA_PATH, ARTISTS_PATH, GENRES_PATH, LYRICS_PATH, TAGS_PATH)\n",
        "\n",
        "#     if df_metadata.empty or df_features.empty or not lyrics_dict:\n",
        "#         logging.error(\"Data loading failed.\")\n",
        "#         return\n",
        "\n",
        "#     text_embeddings = generate_text_embeddings(lyrics_dict)\n",
        "#     analyze_linguistic_patterns(df_metadata, lyrics_dict)\n",
        "#     zero_shot_classification(df_metadata, lyrics_dict)\n",
        "#     few_shot_classification(df_metadata, lyrics_dict)\n",
        "\n",
        "#     valid_track_ids = df_features['track_id'].tolist()\n",
        "#     df_metadata = df_metadata[df_metadata['track_id'].isin(valid_track_ids)]\n",
        "#     text_features = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df_metadata['track_id']])\n",
        "#     scaler = StandardScaler()\n",
        "#     audio_features = scaler.fit_transform(df_features[[col for col in df_features.columns if col != 'track_id']].values)\n",
        "#     audio_features = np.nan_to_num(audio_features)\n",
        "\n",
        "#     user_data = pd.read_csv(USER_DATA_PATH)\n",
        "#     user_data['track_id'] = user_data['track_id'].astype(str).str.zfill(6)\n",
        "#     user_data = user_data[user_data['track_id'].isin(df_metadata['track_id'])]\n",
        "#     ratings_agg = user_data.groupby('track_id')['rating'].mean().reindex(df_metadata['track_id']).fillna(3.0).values\n",
        "#     ratings = np.clip(ratings_agg / 5.0, 0.0, 1.0)\n",
        "\n",
        "#     X = np.hstack([audio_features, text_features])\n",
        "#     if X.shape[0] != len(ratings):\n",
        "#         logging.error(\"Mismatch in samples: X has %d samples, ratings has %d\", X.shape[0], len(ratings))\n",
        "#         raise ValueError(f\"Mismatch in samples: X has {X.shape[0]} samples, ratings has {len(ratings)}\")\n",
        "\n",
        "#     original_indices = np.arange(len(X))\n",
        "#     binary_ratings = (ratings > 0.5).astype(int)\n",
        "#     class_counts = Counter(binary_ratings)\n",
        "#     min_class_size = min(class_counts.values())\n",
        "#     logging.info(\"Class distribution: %s\", class_counts)\n",
        "#     print(f\"Class distribution: {class_counts}\")\n",
        "\n",
        "#     X_resampled, ratings_resampled, original_indices_resampled = X, binary_ratings, original_indices\n",
        "#     if min_class_size >= 2:\n",
        "#         k_neighbors = min(5, min_class_size - 1)\n",
        "#         smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "#         try:\n",
        "#             X_resampled, ratings_resampled = smote.fit_resample(X, binary_ratings)\n",
        "#             original_indices_resampled = np.concatenate([\n",
        "#                 original_indices[ratings == 0],\n",
        "#                 original_indices[ratings == 1],\n",
        "#                 np.random.choice(original_indices[ratings == 0], size=len(X_resampled) - len(X), replace=True)\n",
        "#             ])\n",
        "#             shuffle_indices = np.random.permutation(len(X_resampled))\n",
        "#             X_resampled = X_resampled[shuffle_indices]\n",
        "#             ratings_resampled = ratings_resampled[shuffle_indices]\n",
        "#             original_indices_resampled = original_indices_resampled[shuffle_indices]\n",
        "#         except ValueError as e:\n",
        "#             logging.error(\"SMOTE failed: %s\", str(e))\n",
        "#             print(f\"SMOTE failed: {str(e)}. Using original data.\")\n",
        "\n",
        "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "#     rec_metrics_all = []\n",
        "#     global train_indices\n",
        "\n",
        "#     for fold, (train_idx_resampled, test_idx_resampled) in enumerate(kf.split(X_resampled)):\n",
        "#         print(f\"\\nFold {fold+1}\")\n",
        "#         train_indices = original_indices_resampled[train_idx_resampled]\n",
        "#         X_train_rec, X_test_rec = X_resampled[train_idx_resampled], X_resampled[test_idx_resampled]\n",
        "#         y_train_rec, y_test_rec = ratings_resampled[train_idx_resampled], ratings_resampled[test_idx_resampled]\n",
        "\n",
        "#         train_dataset_rec = torch.utils.data.TensorDataset(\n",
        "#             torch.tensor(X_train_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "#             torch.tensor(X_train_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "#             torch.tensor(y_train_rec, dtype=torch.float32)\n",
        "#         )\n",
        "#         train_loader_rec = torch.utils.data.DataLoader(train_dataset_rec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#         test_dataset_rec = torch.utils.data.TensorDataset(\n",
        "#             torch.tensor(X_test_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "#             torch.tensor(X_test_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "#             torch.tensor(y_test_rec, dtype=torch.float32)\n",
        "#         )\n",
        "#         test_loader_rec = torch.utils.data.DataLoader(test_dataset_rec, batch_size=BATCH_SIZE)\n",
        "\n",
        "#         recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=384).to(DEVICE)\n",
        "#         criterion_rec = nn.BCELoss()\n",
        "#         optimizer_rec = torch.optim.Adam(recommender.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "#         print(\"\\nTraining Recommender Model...\")\n",
        "#         for epoch in range(NUM_EPOCHS_REC):\n",
        "#             loss = train_recommender(recommender, train_loader_rec, criterion_rec, optimizer_rec)\n",
        "#             print(f\"Recommendation Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "#         rec_metrics = evaluate_recommender(recommender, test_loader_rec, df_metadata, test_idx_resampled, original_indices_resampled, df_metadata['genre'].unique())\n",
        "#         rec_metrics_all.append(rec_metrics)\n",
        "#         print(f\"Recommendation Metrics: {rec_metrics}\")\n",
        "\n",
        "#     avg_rec_metrics = {k: np.mean([m[k] for m in rec_metrics_all]) for k in rec_metrics_all[0]}\n",
        "#     print(f\"Average Recommendation Metrics: {avg_rec_metrics}\")\n",
        "\n",
        "#     cf_metrics = collaborative_filtering(user_data, df_metadata)\n",
        "#     print(f\"Collaborative Filtering Metrics: {cf_metrics}\")\n",
        "\n",
        "#     genres = df_metadata['genre'].unique()\n",
        "#     genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
        "#     labels = df_metadata['genre'].map(genre_to_idx).values\n",
        "\n",
        "#     cls_metrics_all = []\n",
        "#     for fold, (train_idx_resampled, test_idx_resampled) in enumerate(kf.split(X_resampled)):\n",
        "#         print(f\"\\nFold {fold+1}\")\n",
        "#         X_train_cls, X_test_cls = X_resampled[train_idx_resampled], X_resampled[test_idx_resampled]\n",
        "#         y_train_cls = labels[original_indices_resampled[train_idx_resampled]]\n",
        "#         y_test_cls = labels[original_indices_resampled[test_idx_resampled]]\n",
        "\n",
        "#         train_dataset_cls = torch.utils.data.TensorDataset(\n",
        "#             torch.tensor(X_train_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "#             torch.tensor(X_train_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "#             torch.tensor(y_train_cls, dtype=torch.long)\n",
        "#         )\n",
        "#         train_loader_cls = torch.utils.data.DataLoader(train_dataset_cls, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#         test_dataset_cls = torch.utils.data.TensorDataset(\n",
        "#             torch.tensor(X_test_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "#             torch.tensor(X_test_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "#             torch.tensor(y_test_cls, dtype=torch.long)\n",
        "#         )\n",
        "#         test_loader_cls = torch.utils.data.DataLoader(test_dataset_cls, batch_size=BATCH_SIZE)\n",
        "\n",
        "#         classifier = GenreClassifier(audio_dim=audio_features.shape[1], text_dim=384, num_classes=len(genres)).to(DEVICE)\n",
        "#         criterion_cls = nn.CrossEntropyLoss()\n",
        "#         optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "#         print(\"\\nTraining Genre Classifier Model...\")\n",
        "#         for epoch in range(NUM_EPOCHS_CLS):\n",
        "#             loss = train_classifier(classifier, train_loader_cls, criterion_cls, optimizer_cls)\n",
        "#             print(f\"Classification Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "#         cls_metrics = evaluate_classifier(classifier, test_loader_cls, genres)\n",
        "#         cls_metrics_all.append(cls_metrics)\n",
        "#         print(f\"Classification Metrics: {cls_metrics}\")\n",
        "\n",
        "#     avg_cls_metrics = {k: np.mean([m[k] for m in cls_metrics_all]) for k in cls_metrics_all[0]}\n",
        "#     print(f\"Average Classification Metrics: {avg_cls_metrics}\")\n",
        "\n",
        "#     bert_metrics = bert_classifier(df_metadata, text_embeddings, audio_features, genres)\n",
        "#     print(f\"BERT Classifier Metrics: {bert_metrics}\")\n",
        "\n",
        "#     roberta_metrics = roberta_classifier(df_metadata, text_embeddings, genres)\n",
        "#     print(f\"RoBERTa Classifier Metrics: {roberta_metrics}\")\n",
        "\n",
        "#     audio_files = [os.path.join(AUDIO_PATH, f\"{tid}.mp3\") for tid in df_metadata['track_id']]\n",
        "#     ast_metrics = ast_classifier(audio_files, labels, genres, df_metadata)\n",
        "#     print(f\"AST vs Custom Classifier: AST F1={ast_metrics['f1']:.4f}, Custom F1={avg_cls_metrics['f1']:.4f}\")\n",
        "\n",
        "#     search_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "#     query = \"upbeat rock songs\"\n",
        "#     _, search_metrics = music_search(query, df_metadata, text_embeddings, df_features, search_model, scaler)\n",
        "#     print(f\"Search Metrics: {search_metrics}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.info(\"Starting main execution...\")\n",
        "print(\"Starting main execution...\")\n",
        "\n",
        "df_metadata, df_features, lyrics_dict = load_fma_data(AUDIO_PATH, METADATA_PATH, ARTISTS_PATH, GENRES_PATH, LYRICS_PATH, TAGS_PATH)\n",
        "\n",
        "if df_metadata.empty or df_features.empty or not lyrics_dict:\n",
        "    logging.error(\"Data loading failed.\")\n",
        "    # return\n",
        "\n",
        "text_embeddings = generate_text_embeddings(lyrics_dict)\n",
        "analyze_linguistic_patterns(df_metadata, lyrics_dict)\n",
        "zero_shot_classification(df_metadata, lyrics_dict)\n",
        "few_shot_classification(df_metadata, lyrics_dict)\n",
        "\n",
        "valid_track_ids = df_features['track_id'].tolist()\n",
        "df_metadata = df_metadata[df_metadata['track_id'].isin(valid_track_ids)]\n",
        "text_features = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df_metadata['track_id']])\n",
        "scaler = StandardScaler()\n",
        "audio_features = scaler.fit_transform(df_features[[col for col in df_features.columns if col != 'track_id']].values)\n",
        "audio_features = np.nan_to_num(audio_features)\n",
        "\n",
        "user_data = pd.read_csv(USER_DATA_PATH)\n",
        "user_data['track_id'] = user_data['track_id'].astype(str).str.zfill(6)\n",
        "user_data = user_data[user_data['track_id'].isin(df_metadata['track_id'])]\n",
        "ratings_agg = user_data.groupby('track_id')['rating'].mean().reindex(df_metadata['track_id']).fillna(3.0).values\n",
        "ratings = np.clip(ratings_agg / 5.0, 0.0, 1.0)\n",
        "\n",
        "X = np.hstack([audio_features, text_features])\n",
        "if X.shape[0] != len(ratings):\n",
        "    logging.error(\"Mismatch in samples: X has %d samples, ratings has %d\", X.shape[0], len(ratings))\n",
        "    raise ValueError(f\"Mismatch in samples: X has {X.shape[0]} samples, ratings has {len(ratings)}\")\n",
        "\n",
        "original_indices = np.arange(len(X))\n",
        "binary_ratings = (ratings > 0.5).astype(int)\n",
        "class_counts = Counter(binary_ratings)\n",
        "min_class_size = min(class_counts.values())\n",
        "logging.info(\"Class distribution: %s\", class_counts)\n",
        "print(f\"Class distribution: {class_counts}\")\n",
        "\n",
        "X_resampled, ratings_resampled, original_indices_resampled = X, binary_ratings, original_indices\n",
        "if min_class_size >= 2:\n",
        "    k_neighbors = min(5, min_class_size - 1)\n",
        "    smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "    try:\n",
        "        X_resampled, ratings_resampled = smote.fit_resample(X, binary_ratings)\n",
        "        original_indices_resampled = np.concatenate([\n",
        "            original_indices[ratings == 0],\n",
        "            original_indices[ratings == 1],\n",
        "            np.random.choice(original_indices[ratings == 0], size=len(X_resampled) - len(X), replace=True)\n",
        "        ])\n",
        "        shuffle_indices = np.random.permutation(len(X_resampled))\n",
        "        X_resampled = X_resampled[shuffle_indices]\n",
        "        ratings_resampled = ratings_resampled[shuffle_indices]\n",
        "        original_indices_resampled = original_indices_resampled[shuffle_indices]\n",
        "    except ValueError as e:\n",
        "        logging.error(\"SMOTE failed: %s\", str(e))\n",
        "        print(f\"SMOTE failed: {str(e)}. Using original data.\")\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rec_metrics_all = []\n",
        "global train_indices\n",
        "\n",
        "for fold, (train_idx_resampled, test_idx_resampled) in enumerate(kf.split(X_resampled)):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    train_indices = original_indices_resampled[train_idx_resampled]\n",
        "    X_train_rec, X_test_rec = X_resampled[train_idx_resampled], X_resampled[test_idx_resampled]\n",
        "    y_train_rec, y_test_rec = ratings_resampled[train_idx_resampled], ratings_resampled[test_idx_resampled]\n",
        "\n",
        "    train_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_rec, dtype=torch.float32)\n",
        "    )\n",
        "    train_loader_rec = torch.utils.data.DataLoader(train_dataset_rec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_rec, dtype=torch.float32)\n",
        "    )\n",
        "    test_loader_rec = torch.utils.data.DataLoader(test_dataset_rec, batch_size=BATCH_SIZE)\n",
        "\n",
        "    recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=384).to(DEVICE)\n",
        "    criterion_rec = nn.BCELoss()\n",
        "    optimizer_rec = torch.optim.Adam(recommender.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    print(\"\\nTraining Recommender Model...\")\n",
        "    for epoch in range(NUM_EPOCHS_REC):\n",
        "        loss = train_recommender(recommender, train_loader_rec, criterion_rec, optimizer_rec)\n",
        "        print(f\"Recommendation Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    rec_metrics = evaluate_recommender(recommender, test_loader_rec, df_metadata, test_idx_resampled, original_indices_resampled, df_metadata['genre'].unique())\n",
        "    rec_metrics_all.append(rec_metrics)\n",
        "    print(f\"Recommendation Metrics: {rec_metrics}\")\n",
        "\n",
        "avg_rec_metrics = {k: np.mean([m[k] for m in rec_metrics_all]) for k in rec_metrics_all[0]}\n",
        "print(f\"Average Recommendation Metrics: {avg_rec_metrics}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f5f8eaad251f47d5996ef79ad0241d5b",
            "3b0db85da33a42d984aea44064b9795b",
            "918ba39e100c442e842aea386a0d9a12",
            "0e4e213339a74a7d9c3476b440bac0ad",
            "fef50e8a82fa4b5cb658032d6427d005",
            "023dfc05c4334e79a72d0c244338f9eb",
            "6c3d958c36ba434f9980caaaea70efb4",
            "89906d90c4d245f3adbdbecb615293a0",
            "72a646bd3eef455284591e3ce991e077",
            "818cbc38fc0647e7b15f37354589f645",
            "30c43883efb44a52995825a3f0004572",
            "34b7e0eeb211436491e83963ad105162",
            "17b16fd7bec14294a37c24cfd9530df9",
            "f145e219832343d6b610ff9fd35cc7dc",
            "9294b2a57ced4f088de825c46fc6be63",
            "2fbb8ba4e8af4e1f95b9abb1ffb3ee45",
            "9b4f9780531c4e27b7d361a0f50232c9",
            "1d5c17604f6b4ac1a3c722f976d231d6",
            "3afdf2c8a7b84e1490e4b6657a34880f",
            "37994e60ad21464e9d74d939f80c9f0d",
            "651ff62e25714c3e94eb0734d6fd1805",
            "d110a7d83fc4496a9a0b8d270a668095"
          ]
        },
        "id": "wOZBWvY4q_S0",
        "outputId": "ffc7f239-f70c-49d2-fbf9-77036ea81b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Starting main execution...\n",
            "INFO:root:Loading synthetic data...\n",
            "INFO:root:Creating synthetic dataset...\n",
            "INFO:root:Created output directories: /content/drive/MyDrive/fma/data/metadata, /content/drive/MyDrive/fma/data/user_data, /content/drive/MyDrive/fma/data/descriptions\n",
            "INFO:root:Attempting to download FMA small dataset...\n",
            "INFO:root:FMA dataset already exists at /content/drive/MyDrive/fma/data/audio_files with 10 audio files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting main execution...\n",
            "Loading synthetic data...\n",
            "Creating synthetic dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Saved tracks metadata to /content/drive/MyDrive/fma/data/metadata/tracks.csv\n",
            "INFO:root:Saved artists metadata to /content/drive/MyDrive/fma/data/metadata/artists.csv\n",
            "INFO:root:Saved genres metadata to /content/drive/MyDrive/fma/data/metadata/genres.csv\n",
            "INFO:root:Saved user ratings to /content/drive/MyDrive/fma/data/user_data/ratings.csv\n",
            "INFO:root:Saved tags to /content/drive/MyDrive/fma/data/descriptions/tags.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating synthetic lyrics:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5f8eaad251f47d5996ef79ad0241d5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Synthetic dataset created: Tracks shape: (100, 5), Ratings: (1826, 3), Tags: 447\n",
            "INFO:root:Metadata shape: (130, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic dataset created: Tracks shape: (100, 5), Ratings shape: (1826, 3), Tags: 447\n",
            "Metadata shape: (130, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Features shape: (130, 27)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (130, 27)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Lyrics dict size: 95\n",
            "INFO:root:Generating text embeddings...\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lyrics dict size: 95\n",
            "Generating text embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating text embeddings:   0%|          | 0/95 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34b7e0eeb211436491e83963ad105162"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Text embeddings generated for 95 tracks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text embeddings generated for 95 tracks.\n",
            "\n",
            "Linguistic Analysis:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Jazz top words: [('jazz', 24), ('night', 15), ('flow', 13), ('saxophone', 12), ('improvised', 12)]\n",
            "INFO:root:Blues top words: [('blues', 29), ('delta', 12), ('emotional', 11), ('guitar', 11), ('song', 10)]\n",
            "INFO:root:Country top words: [('country', 39), ('banjo', 17), ('heart', 16), ('beats', 16), ('strong', 16)]\n",
            "INFO:root:Electronic top words: [('night', 30), ('beats', 24), ('techno', 24), ('electronic', 24), ('edm', 22)]\n",
            "INFO:root:Folk top words: [('folk', 26), ('acoustic', 17), ('tradition', 14), ('song', 12), ('heartfelt', 12)]\n",
            "INFO:root:Experimental top words: [('experimental', 33), ('vibes', 15), ('abstract', 15), ('beats', 15), ('song', 12)]\n",
            "INFO:root:International top words: [('international', 18), ('fusion', 15), ('vibes', 15), ('song', 12), ('feel', 12)]\n",
            "INFO:root:Pop top words: [('pop', 54), ('neon', 28), ('love', 27), ('song', 24), ('beats', 24)]\n",
            "INFO:root:Rock top words: [('rock', 33), ('indie', 16), ('soul', 15), ('heart', 15), ('song', 14)]\n",
            "INFO:root:Instrumental top words: [('instrumental', 45), ('soul', 19), ('orchestral', 19), ('rise', 18), ('song', 16)]\n",
            "INFO:root:Hip-Hop top words: [('flow', 23), ('beats', 15), ('song', 14), ('feel', 14), ('let', 14)]\n",
            "INFO:root:Classical top words: [('classical', 33), ('rise', 23), ('symphonic', 23), ('baroque', 23), ('timeless', 23)]\n",
            "INFO:root:Reggae top words: [('reggae', 27), ('island', 16), ('roots', 15), ('vibes', 15), ('song', 12)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jazz top words: [('jazz', 24), ('night', 15), ('flow', 13), ('saxophone', 12), ('improvised', 12)]\n",
            "Blues top words: [('blues', 29), ('delta', 12), ('emotional', 11), ('guitar', 11), ('song', 10)]\n",
            "Country top words: [('country', 39), ('banjo', 17), ('heart', 16), ('beats', 16), ('strong', 16)]\n",
            "Electronic top words: [('night', 30), ('beats', 24), ('techno', 24), ('electronic', 24), ('edm', 22)]\n",
            "Folk top words: [('folk', 26), ('acoustic', 17), ('tradition', 14), ('song', 12), ('heartfelt', 12)]\n",
            "Experimental top words: [('experimental', 33), ('vibes', 15), ('abstract', 15), ('beats', 15), ('song', 12)]\n",
            "International top words: [('international', 18), ('fusion', 15), ('vibes', 15), ('song', 12), ('feel', 12)]\n",
            "Pop top words: [('pop', 54), ('neon', 28), ('love', 27), ('song', 24), ('beats', 24)]\n",
            "Rock top words: [('rock', 33), ('indie', 16), ('soul', 15), ('heart', 15), ('song', 14)]\n",
            "Instrumental top words: [('instrumental', 45), ('soul', 19), ('orchestral', 19), ('rise', 18), ('song', 16)]\n",
            "Hip-Hop top words: [('flow', 23), ('beats', 15), ('song', 14), ('feel', 14), ('let', 14)]\n",
            "Classical top words: [('classical', 33), ('rise', 23), ('symphonic', 23), ('baroque', 23), ('timeless', 23)]\n",
            "Reggae top words: [('reggae', 27), ('island', 16), ('roots', 15), ('vibes', 15), ('song', 12)]\n",
            "Topic 0: ['let', 'soul', 'heart', 'indie', 'rock']\n",
            "Topic 1: ['baroque', 'symphonic', 'timeless', 'rise', 'classical']\n",
            "Topic 2: ['song', 'feel', 'let', 'vibes', 'night']\n",
            "Topic 3: ['let', 'song', 'feel', 'night', 'country']\n",
            "Topic 4: ['pop', 'feel', 'song', 'let', 'night']\n",
            "\n",
            "Zero-Shot Genre Classification:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cf_metrics = collaborative_filtering(user_data, df_metadata)\n",
        "print(f\"Collaborative Filtering Metrics: {cf_metrics}\")\n",
        "\n",
        "genres = df_metadata['genre'].unique()\n",
        "genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
        "labels = df_metadata['genre'].map(genre_to_idx).values\n",
        "\n",
        "cls_metrics_all = []\n",
        "for fold, (train_idx_resampled, test_idx_resampled) in enumerate(kf.split(X_resampled)):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    X_train_cls, X_test_cls = X_resampled[train_idx_resampled], X_resampled[test_idx_resampled]\n",
        "    y_train_cls = labels[original_indices_resampled[train_idx_resampled]]\n",
        "    y_test_cls = labels[original_indices_resampled[test_idx_resampled]]\n",
        "\n",
        "    train_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_cls, dtype=torch.long)\n",
        "    )\n",
        "    train_loader_cls = torch.utils.data.DataLoader(train_dataset_cls, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_cls, dtype=torch.long)\n",
        "    )\n",
        "    test_loader_cls = torch.utils.data.DataLoader(test_dataset_cls, batch_size=BATCH_SIZE)\n",
        "\n",
        "    classifier = GenreClassifier(audio_dim=audio_features.shape[1], text_dim=384, num_classes=len(genres)).to(DEVICE)\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    print(\"\\nTraining Genre Classifier Model...\")\n",
        "    for epoch in range(NUM_EPOCHS_CLS):\n",
        "        loss = train_classifier(classifier, train_loader_cls, criterion_cls, optimizer_cls)\n",
        "        print(f\"Classification Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    cls_metrics = evaluate_classifier(classifier, test_loader_cls, genres)\n",
        "    cls_metrics_all.append(cls_metrics)\n",
        "    print(f\"Classification Metrics: {cls_metrics}\")\n",
        "\n",
        "avg_cls_metrics = {k: np.mean([m[k] for m in cls_metrics_all]) for k in cls_metrics_all[0]}\n",
        "print(f\"Average Classification Metrics: {avg_cls_metrics}\")"
      ],
      "metadata": {
        "id": "4oP-mtcv81MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "def bert_classifier(df_metadata, text_embeddings, audio_features, genres):\n",
        "    print(\"\\nTraining Audio-Text BERT Classifier...\")\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased')  # Use BertModel here\n",
        "    model = AudioTextBERTClassifier(bert_model, audio_dim=audio_features.shape[1], num_classes=len(genres)).to(DEVICE)\n",
        "\n",
        "    texts = [f\"{row['title']} {row['genre']}\" for _, row in df_metadata.iterrows()]\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "    genres_list = genres.tolist()\n",
        "    labels = [genres_list.index(row['genre']) for _, row in df_metadata.iterrows()]\n",
        "\n",
        "    if len(inputs['input_ids']) != len(audio_features) or len(inputs['input_ids']) != len(labels):\n",
        "        logging.error(\"Data mismatch: input_ids=%d, audio_features=%d, labels=%d\",\n",
        "                      len(inputs['input_ids']), len(audio_features), len(labels))\n",
        "        raise ValueError(\"Data mismatch in bert_classifier inputs\")\n",
        "\n",
        "    dataset_dict = {\n",
        "        'input_ids': inputs['input_ids'].numpy(),\n",
        "        'attention_mask': inputs['attention_mask'].numpy(),\n",
        "        'audio_features': audio_features,\n",
        "        'labels': np.array(labels, dtype=np.int64)\n",
        "    }\n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    dataset = DatasetDict({\n",
        "        \"train\": split_dataset[\"train\"],\n",
        "        \"test\": split_dataset[\"test\"]\n",
        "    })\n",
        "\n",
        "    expected_keys = {'input_ids', 'attention_mask', 'audio_features', 'labels'}\n",
        "    actual_keys = set(dataset['train'].features.keys())\n",
        "    if not expected_keys.issubset(actual_keys):\n",
        "        logging.error(\"Dataset missing required keys: %s\", actual_keys)\n",
        "        raise ValueError(\"Dataset missing required keys\")\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        return {\n",
        "            'input_ids': torch.tensor([item['input_ids'] for item in batch], dtype=torch.long).to(DEVICE),\n",
        "            'attention_mask': torch.tensor([item['attention_mask'] for item in batch], dtype=torch.long).to(DEVICE),\n",
        "            'audio_features': torch.tensor([item['audio_features'] for item in batch], dtype=torch.float32).to(DEVICE),\n",
        "            'labels': torch.tensor([item['labels'] for item in batch], dtype=torch.long).to(DEVICE)\n",
        "        }\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(OUTPUT_DIR, \"bert-finetuned\"),\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        data_collator=collate_fn,\n",
        "        compute_metrics=lambda pred: {\n",
        "            'precision': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[0],\n",
        "            'recall': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[1],\n",
        "            'f1': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[2]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    return {\n",
        "        'precision': eval_results.get('eval_precision', 0.0),\n",
        "        'recall': eval_results.get('eval_recall', 0.0),\n",
        "        'f1': eval_results.get('eval_f1', 0.0)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def roberta_classifier(df_metadata, text_embeddings, genres):\n",
        "    \"\"\"Task A1.2: RoBERTa-based genre classification.\"\"\"\n",
        "    print(\"\\nTraining RoBERTa Classifier...\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(genres)).to(DEVICE)\n",
        "\n",
        "    texts = [f\"{row['title']} {row['genre']}\" for _, row in df_metadata.iterrows()]\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(DEVICE)\n",
        "    genres_list = genres.tolist()\n",
        "    labels = torch.tensor([genres_list.index(row['genre']) for _, row in df_metadata.iterrows()]).to(DEVICE)\n",
        "\n",
        "    dataset = Dataset.from_dict({\n",
        "        'input_ids': inputs['input_ids'].cpu().numpy(),\n",
        "        'attention_mask': inputs['attention_mask'].cpu().numpy(),\n",
        "        'labels': labels.cpu().numpy()\n",
        "    })\n",
        "    dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(OUTPUT_DIR, \"roberta-finetuned\"),\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        compute_metrics=lambda pred: {'precision': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[0],\n",
        "                                     'recall': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[1],\n",
        "                                     'f1': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[2]}\n",
        "    )\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    return {'precision': eval_results['eval_precision'], 'recall': eval_results['eval_recall'], 'f1': eval_results['eval_f1']}\n"
      ],
      "metadata": {
        "id": "20QJG1KjRECp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_metrics = bert_classifier(df_metadata, text_embeddings, audio_features, genres)\n",
        "print(f\"BERT Classifier Metrics: {bert_metrics}\")\n",
        "\n",
        "roberta_metrics = roberta_classifier(df_metadata, text_embeddings, genres)\n",
        "print(f\"RoBERTa Classifier Metrics: {roberta_metrics}\")\n",
        "\n",
        "audio_files = [os.path.join(AUDIO_PATH, f\"{tid}.mp3\") for tid in df_metadata['track_id']]\n",
        "ast_metrics = ast_classifier(audio_files, labels, genres, df_metadata)\n",
        "print(f\"AST vs Custom Classifier: AST F1={ast_metrics['f1']:.4f}, Custom F1={avg_cls_metrics['f1']:.4f}\")\n",
        "\n",
        "search_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "query = \"upbeat rock songs\"\n",
        "_, search_metrics = music_search(query, df_metadata, text_embeddings, df_features, search_model, scaler)\n",
        "print(f\"Search Metrics: {search_metrics}\")"
      ],
      "metadata": {
        "id": "QEzCBF2CrH4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5f8eaad251f47d5996ef79ad0241d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b0db85da33a42d984aea44064b9795b",
              "IPY_MODEL_918ba39e100c442e842aea386a0d9a12",
              "IPY_MODEL_0e4e213339a74a7d9c3476b440bac0ad"
            ],
            "layout": "IPY_MODEL_fef50e8a82fa4b5cb658032d6427d005"
          }
        },
        "3b0db85da33a42d984aea44064b9795b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_023dfc05c4334e79a72d0c244338f9eb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6c3d958c36ba434f9980caaaea70efb4",
            "value": "Generatingâ€‡syntheticâ€‡lyrics:â€‡100%"
          }
        },
        "918ba39e100c442e842aea386a0d9a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89906d90c4d245f3adbdbecb615293a0",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72a646bd3eef455284591e3ce991e077",
            "value": 100
          }
        },
        "0e4e213339a74a7d9c3476b440bac0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818cbc38fc0647e7b15f37354589f645",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30c43883efb44a52995825a3f0004572",
            "value": "â€‡100/100â€‡[00:03&lt;00:00,â€‡53.88it/s]"
          }
        },
        "fef50e8a82fa4b5cb658032d6427d005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "023dfc05c4334e79a72d0c244338f9eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c3d958c36ba434f9980caaaea70efb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89906d90c4d245f3adbdbecb615293a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a646bd3eef455284591e3ce991e077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "818cbc38fc0647e7b15f37354589f645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c43883efb44a52995825a3f0004572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34b7e0eeb211436491e83963ad105162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17b16fd7bec14294a37c24cfd9530df9",
              "IPY_MODEL_f145e219832343d6b610ff9fd35cc7dc",
              "IPY_MODEL_9294b2a57ced4f088de825c46fc6be63"
            ],
            "layout": "IPY_MODEL_2fbb8ba4e8af4e1f95b9abb1ffb3ee45"
          }
        },
        "17b16fd7bec14294a37c24cfd9530df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b4f9780531c4e27b7d361a0f50232c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1d5c17604f6b4ac1a3c722f976d231d6",
            "value": "Generatingâ€‡textâ€‡embeddings:â€‡100%"
          }
        },
        "f145e219832343d6b610ff9fd35cc7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3afdf2c8a7b84e1490e4b6657a34880f",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37994e60ad21464e9d74d939f80c9f0d",
            "value": 95
          }
        },
        "9294b2a57ced4f088de825c46fc6be63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_651ff62e25714c3e94eb0734d6fd1805",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d110a7d83fc4496a9a0b8d270a668095",
            "value": "â€‡95/95â€‡[00:31&lt;00:00,â€‡â€‡4.43it/s]"
          }
        },
        "2fbb8ba4e8af4e1f95b9abb1ffb3ee45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4f9780531c4e27b7d361a0f50232c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5c17604f6b4ac1a3c722f976d231d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3afdf2c8a7b84e1490e4b6657a34880f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37994e60ad21464e9d74d939f80c9f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "651ff62e25714c3e94eb0734d6fd1805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d110a7d83fc4496a9a0b8d270a668095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}