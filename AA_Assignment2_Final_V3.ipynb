{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viditk07-Bits/AudioAnalytics_S2-24_AIMLCZG527/blob/main/AA_Assignment2_Final_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EN9uWs6-RQ"
      },
      "source": [
        "# Music Information Retrieval System\n",
        "\n",
        "## Assignment Objective\n",
        "This assignment implements a comprehensive Music Information Retrieval (MIR) system using Large Language Models (LLMs) and deep learning techniques. It includes music recommendation, genre classification, and semantic search applications, combining audio analysis with natural language processing.\n",
        "\n",
        "## Dataset Setup\n",
        "Using the Free Music Archive (FMA) dataset with audio files, metadata, and synthetic user data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor, Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, average_precision_score, ndcg_score\n",
        "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import logging\n",
        "import random\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from datasets import Dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "import faiss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import time\n",
        "import streamlit as st\n",
        "import unittest\n",
        "import configparser\n",
        "import warnings\n",
        "import logging.handlers\n",
        "import timeout_decorator\n",
        "\n",
        "# Install required packages\n",
        "required_packages = ['faiss-cpu', 'transformers', 'sentence-transformers', 'tqdm', 'imblearn', 'librosa', 'nltk', 'matplotlib', 'seaborn', 'datasets', 'streamlit', 'scikit-learn', 'numpy', 'torch', 'timeout-decorator']\n",
        "for pkg in required_packages:\n",
        "    try:\n",
        "        __import__(pkg.replace('-', '_'))\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        try:\n",
        "            subprocess.run(['pip', 'install', pkg, '--no-cache-dir'], check=True, capture_output=True, text=True)\n",
        "            print(f\"Successfully installed {pkg}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Failed to install {pkg}: {e.stderr}\")\n",
        "            raise\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Setup logging with rotation\n",
        "log_dir = \"/content/logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.handlers.RotatingFileHandler(os.path.join(log_dir, 'music_recommender.log'), maxBytes=1000000, backupCount=5),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/fma_metadata\"\n",
        "METADATA_PATH = os.path.join(DATA_PATH, \"tracks.csv\")\n",
        "ARTISTS_PATH = os.path.join(DATA_PATH, \"artists.csv\")\n",
        "GENRES_PATH = os.path.join(DATA_PATH, \"genres.csv\")\n",
        "LYRICS_PATH = \"/content/fma/lyrics\"\n",
        "USER_DATA_PATH = \"/content/fma/user_data/ratings.csv\"\n",
        "TAGS_PATH = \"/content/fma/descriptions/tags.csv\"\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "TEMP_DIR = \"/content/fma\"\n",
        "NUM_EPOCHS_REC = 5\n",
        "NUM_EPOCHS_CLS = 10\n",
        "BATCH_SIZE = 16\n",
        "MAX_TRACKS = 100\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TIMEOUT_SECONDS = 3600\n",
        "TEXT_EMBEDDING_DIM = 384\n",
        "FMA_BASE_DIR = \"/content/drive/MyDrive/fma_small/\"\n",
        "FMA_AUDIO_DIRS = [str(p) for p in Path(FMA_BASE_DIR).glob(\"*\") if p.is_dir()]\n",
        "if not FMA_AUDIO_DIRS:\n",
        "    logging.warning(\"No subdirectories found in %s\", FMA_BASE_DIR)\n",
        "    print(f\"Warning: No subdirectories found in {FMA_BASE_DIR}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Genre-specific keywords\n",
        "GENRE_KEYWORDS = {\n",
        "    'Rock': ['electric guitar wails', 'rebellious spirit soars', 'grunge heart pounds', 'classic riffs ignite', 'indie soul rebels', 'punk fire explodes', 'rock anthem roars'],\n",
        "    'Pop': ['infectious hooks dance', 'neon lights pulse', 'melodic dreams soar', 'upbeat rhythm shines', 'love story sparkles', 'dancefloor beats throb', 'pop fever rises'],\n",
        "    'Jazz': ['saxophone weaves magic', 'improvised notes flow', 'bluesy soul swings', 'smooth grooves linger', 'jazz night whispers', 'rhythmic scat hums', 'cool vibes drift'],\n",
        "    'Classical': ['orchestral swells rise', 'violin sings softly', 'piano echoes grace', 'symphonic waves crash', 'baroque harmony soars', 'elegant strings weave', 'timeless beauty unfolds'],\n",
        "    'Hip-Hop': ['heavy beats drop hard', 'sharp rhymes cut deep', 'street stories unfold', 'flow rides the rhythm', 'urban pulse vibrates', 'mic drops with swagger', 'hip-hop reigns supreme'],\n",
        "    'Electronic': ['synth pulses glow', 'techno beats surge', 'ambient waves drift', 'EDM sparks the night', 'futuristic sounds hum', 'electro vibes ignite', 'digital dreams pulse'],\n",
        "    'Folk': ['acoustic chords strum', 'heartfelt tales weave', 'rustic paths wander', 'folk roots run deep', 'gentle melodies soothe', 'campfire stories sing', 'tradition lives on'],\n",
        "    'Blues': ['guitar wails with soul', 'heartache spills over', 'raw blues cry out', 'delta notes resonate', 'mournful chords linger', 'blues spirit endures', 'emotional strings weep'],\n",
        "    'Country': ['banjo twangs with pride', 'heartland stories sing', 'cowboy boots stomp', 'rural roads ramble', 'love songs ride free', 'country heart beats strong', 'honky-tonk nights shine'],\n",
        "    'Reggae': ['rasta riddims sway', 'island vibes chill', 'roots reggae grooves', 'one love unites all', 'skank beat lifts high', 'irie spirit flows', 'dreadlocks dance free'],\n",
        "    'International': ['world rhythms blend', 'exotic melodies soar', 'cultural beats pulse', 'global sounds unite', 'traditional chants echo', 'fusion vibes transcend', 'earthâ€™s heartbeat sings'],\n",
        "    'Instrumental': ['ambient chords float', 'strings weave dreams', 'piano paints silence', 'orchestral tides rise', 'melody speaks alone', 'instrumental soul soars', 'soundscapes breathe life'],\n",
        "    'Experimental': ['avant-garde sounds twist', 'abstract beats morph', 'sonic boundaries break', 'unorthodox rhythms pulse', 'experimental vibes soar', 'sound art redefines', 'future notes unfold']\n",
        "}\n",
        "\n",
        "@timeout_decorator.timeout(30, timeout_exception=TimeoutError)\n",
        "def generate_lyrics(args):\n",
        "    \"\"\"Generate creative lyrics using GPT-2 for a given track and genre.\"\"\"\n",
        "    track_id, genre, lyrics_path, tokenizer, model = args\n",
        "    try:\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        keywords = random.sample(GENRE_KEYWORDS.get(genre, ['music vibes']), min(3, len(GENRE_KEYWORDS.get(genre, ['music vibes']))))\n",
        "        prompt = f\"Create {genre} lyrics with vivid imagery and emotional depth, inspired by: {', '.join(keywords)}. Avoid repeating the prompt words.\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=150,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        lyrics = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        lyrics = lyrics.replace(prompt, \"\").strip()\n",
        "    except TimeoutError:\n",
        "        logging.warning(\"Lyrics generation timed out for track %s\", track_id)\n",
        "        lyrics = f\"{genre} lyrics placeholder\"\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Lyrics generation failed for track %s: %s\", track_id, str(e))\n",
        "        lyrics = f\"{genre} lyrics placeholder\"\n",
        "    os.makedirs(lyrics_path, exist_ok=True)\n",
        "    with open(os.path.join(lyrics_path, f\"{track_id}.txt\"), 'w', encoding='utf-8') as f:\n",
        "        f.write(lyrics)\n",
        "    return track_id\n",
        "\n",
        "def evaluate_lyrics_quality(lyrics_dict, reference_lyrics=None):\n",
        "    \"\"\"Evaluate generated lyrics quality using BLEU score.\"\"\"\n",
        "    if not reference_lyrics:\n",
        "        logging.warning(\"No reference lyrics provided for BLEU evaluation; returning 0.0\")\n",
        "        return 0.0\n",
        "    bleu_scores = []\n",
        "    for track_id, generated in lyrics_dict.items():\n",
        "        reference = reference_lyrics.get(track_id, '')\n",
        "        if reference:\n",
        "            try:\n",
        "                score = sentence_bleu([reference.split()], generated.split(), weights=(0.5, 0.5))\n",
        "                bleu_scores.append(score)\n",
        "            except Exception as e:\n",
        "                logging.warning(\"BLEU score calculation failed for track %s: %s\", track_id, str(e))\n",
        "    return np.mean(bleu_scores) if bleu_scores else 0.0\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    \"\"\"Extract audio features using Librosa or generate placeholder features.\"\"\"\n",
        "    try:\n",
        "        if not audio_path or not os.path.exists(audio_path) or os.path.getsize(audio_path) < 100:\n",
        "            return np.random.randn(26) * 0.1\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        if len(y) == 0:\n",
        "            return np.random.randn(26) * 0.1\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        tempo = librosa.feature.tempo(y=y, sr=sr)\n",
        "        tempo = float(tempo[0]) if isinstance(tempo, np.ndarray) else float(tempo)\n",
        "        features = np.concatenate([\n",
        "            np.mean(mfccs, axis=1),\n",
        "            np.mean(chroma, axis=1),\n",
        "            np.mean(spectral_centroid, axis=1),\n",
        "            [tempo]\n",
        "        ])\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Error processing %s: %s\", audio_path, str(e))\n",
        "        return np.random.randn(26) * 0.1\n",
        "\n",
        "def load_fma_data(audio_dirs, metadata_path, artists_path, genres_path, lyrics_path, tags_path):\n",
        "    \"\"\"Load FMA dataset with audio files from specified directories.\"\"\"\n",
        "    logging.info(\"Loading FMA data...\")\n",
        "    print(\"Loading FMA data...\")\n",
        "    mp3_files = set()\n",
        "    for audio_dir in audio_dirs:\n",
        "        if os.path.exists(audio_dir):\n",
        "            files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n",
        "            mp3_files.update(f.replace('.mp3', '') for f in files)\n",
        "    if not mp3_files:\n",
        "        logging.warning(\"No MP3 files found in audio directories: %s\", audio_dirs)\n",
        "        print(f\"Warning: No MP3 files found in {audio_dirs}\")\n",
        "\n",
        "    if not os.path.exists(metadata_path):\n",
        "        logging.error(\"Metadata file not found at %s\", metadata_path)\n",
        "        raise FileNotFoundError(f\"Metadata file not found at {metadata_path}\")\n",
        "\n",
        "    # Load metadata with multi-level headers, skipping malformed first row\n",
        "    try:\n",
        "        df_metadata = pd.read_csv(metadata_path, header=[0, 1], skiprows=1, low_memory=False)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to load metadata: %s\", str(e))\n",
        "        print(f\"Failed to load metadata: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    # Flatten multi-level columns\n",
        "    df_metadata.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df_metadata.columns]\n",
        "\n",
        "    print(\"\\nAvailable columns in tracks.csv:\")\n",
        "    print(df_metadata.columns.tolist())\n",
        "    print(\"\\nSample data (first 2 rows):\")\n",
        "    print(df_metadata.head(2).to_string())\n",
        "\n",
        "    # Define possible column names for required fields\n",
        "    possible_cols = {\n",
        "        'track_id': ['Unnamed: 0_level_0_track_id', 'Unnamed: 0_level_0_Unnamed: 0_level_1', 'track_id', 'track.id', 'id', 'trackid', 'track'],\n",
        "        'title': ['title_Unnamed: 52_level_1', 'track_title', 'title', 'track.name', 'name', 'song_title'],\n",
        "        'artist_id': ['id_Unnamed: 21_level_1', 'artist_id', 'artist.id', 'artist', 'artistid'],\n",
        "        'genre_top': ['genre_top_Unnamed: 40_level_1', 'track_genre_top', 'genre_top', 'track_genres', 'genres', 'genre', 'track_genre']\n",
        "    }\n",
        "\n",
        "    # Find matching columns\n",
        "    selected_cols = {}\n",
        "    for key, candidates in possible_cols.items():\n",
        "        for candidate in candidates:\n",
        "            matches = [col for col in df_metadata.columns if candidate.lower() in col.lower()]\n",
        "            if matches:\n",
        "                selected_cols[key] = matches[0]\n",
        "                break\n",
        "        if key not in selected_cols:\n",
        "            logging.warning(\"No matching column for %s. Candidates: %s\", key, candidates)\n",
        "            print(f\"Warning: No matching column for {key}. Candidates: {candidates}\")\n",
        "            if key == 'genre_top':\n",
        "                df_metadata['genre'] = 'Unknown'  # Fallback for missing genre\n",
        "                selected_cols[key] = 'genre'\n",
        "            elif key == 'title':\n",
        "                df_metadata['title'] = df_metadata[selected_cols.get('track_id', 'Unnamed: 0_level_0_track_id')].apply(lambda x: f\"Track_{x}\")\n",
        "                selected_cols[key] = 'title'\n",
        "            elif key == 'artist_id':\n",
        "                df_metadata['artist_id'] = df_metadata[selected_cols.get('track_id', 'Unnamed: 0_level_0_track_id')].apply(lambda x: f\"Artist_{x}\")\n",
        "                selected_cols[key] = 'artist_id'\n",
        "            else:\n",
        "                logging.error(\"No matching column found for %s\", key)\n",
        "                print(f\"Error: No matching column for {key}. Available columns: {df_metadata.columns.tolist()}\")\n",
        "                raise ValueError(f\"No matching column found for {key}\")\n",
        "\n",
        "    # Log selected columns\n",
        "    logging.info(\"Selected columns: %s\", selected_cols)\n",
        "    print(f\"Selected columns: {selected_cols}\")\n",
        "\n",
        "    # Validate track_id column\n",
        "    track_id_col = selected_cols['track_id']\n",
        "    if df_metadata[track_id_col].isnull().all() or df_metadata[track_id_col].eq('track_id').any():\n",
        "        logging.error(\"Invalid track_id column: %s contains all nulls or header value\", track_id_col)\n",
        "        print(f\"Error: Invalid track_id column: {track_id_col}\")\n",
        "        raise ValueError(f\"Invalid track_id column: {track_id_col}\")\n",
        "\n",
        "    # Validate artist_id column\n",
        "    artist_id_col = selected_cols['artist_id']\n",
        "    if df_metadata[artist_id_col].isnull().all():\n",
        "        logging.warning(\"Artist_id column %s contains all nulls. Assigning fallback values.\", artist_id_col)\n",
        "        print(f\"Warning: Artist_id column {artist_id_col} contains all nulls. Assigning fallback values.\")\n",
        "        df_metadata[artist_id_col] = df_metadata[track_id_col].apply(lambda x: f\"Artist_{x}\")\n",
        "        selected_cols['artist_id'] = artist_id_col\n",
        "\n",
        "    # Select and rename columns\n",
        "    df_metadata = df_metadata[list(selected_cols.values())].dropna(subset=[selected_cols['track_id']])\n",
        "    df_metadata = df_metadata.rename(columns={\n",
        "        selected_cols['track_id']: 'track_id',\n",
        "        selected_cols['title']: 'title',\n",
        "        selected_cols['artist_id']: 'artist_id',\n",
        "        selected_cols['genre_top']: 'genre'\n",
        "    })\n",
        "\n",
        "    # Standardize track_id and artist_id\n",
        "    df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "    df_metadata['artist_id'] = df_metadata['artist_id'].astype(str).str.zfill(6)\n",
        "\n",
        "    # Filter tracks with available audio files\n",
        "    df_metadata = df_metadata[df_metadata['track_id'].isin(mp3_files)]\n",
        "    if df_metadata.empty:\n",
        "        logging.warning(\"No tracks found with audio files. Using all metadata tracks up to MAX_TRACKS.\")\n",
        "        print(\"Warning: No tracks found with audio files. Using all metadata tracks up to MAX_TRACKS.\")\n",
        "        df_metadata = pd.read_csv(metadata_path, header=[0, 1], skiprows=1, low_memory=False)\n",
        "        df_metadata.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df_metadata.columns]\n",
        "        df_metadata = df_metadata[list(selected_cols.values())].dropna(subset=[selected_cols['track_id']])\n",
        "        df_metadata = df_metadata.rename(columns={\n",
        "            selected_cols['track_id']: 'track_id',\n",
        "            selected_cols['title']: 'title',\n",
        "            selected_cols['artist_id']: 'artist_id',\n",
        "            selected_cols['genre_top']: 'genre'\n",
        "        })\n",
        "        df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "        df_metadata['artist_id'] = df_metadata['artist_id'].astype(str).str.zfill(6)\n",
        "        df_metadata = df_metadata.head(MAX_TRACKS)\n",
        "\n",
        "    if len(df_metadata) > MAX_TRACKS:\n",
        "        logging.info(\"Limiting to %d tracks\", MAX_TRACKS)\n",
        "        df_metadata = df_metadata.head(MAX_TRACKS)\n",
        "\n",
        "    # Load artists\n",
        "    if os.path.exists(artists_path):\n",
        "        df_artists = pd.read_csv(artists_path, dtype={'artist_id': str})\n",
        "        df_artists['artist_id'] = df_artists['artist_id'].str.zfill(6)\n",
        "    else:\n",
        "        df_artists = pd.DataFrame({\n",
        "            'artist_id': df_metadata['artist_id'].unique(),\n",
        "            'artist_name': [f\"Artist_{i}\" for i in range(1, len(df_metadata['artist_id'].unique()) + 1)]\n",
        "        })\n",
        "    df_metadata = pd.merge(df_metadata, df_artists, on='artist_id', how='left')\n",
        "\n",
        "    # Load genres\n",
        "    if os.path.exists(genres_path):\n",
        "        try:\n",
        "            df_genres = pd.read_csv(genres_path, dtype={'genre_id': str})\n",
        "            # Check if 'genre_name' exists in genres.csv\n",
        "            if 'genre_name' not in df_genres.columns:\n",
        "                logging.warning(\"genres.csv does not contain 'genre_name' column. Creating fallback genres DataFrame.\")\n",
        "                print(\"Warning: genres.csv does not contain 'genre_name' column. Creating fallback genres DataFrame.\")\n",
        "                unique_genres = df_metadata['genre'].dropna().unique()\n",
        "                df_genres = pd.DataFrame({\n",
        "                    'genre_id': range(1, len(unique_genres) + 1),\n",
        "                    'genre_name': unique_genres\n",
        "                })\n",
        "            else:\n",
        "                logging.info(\"genres.csv loaded successfully with columns: %s\", df_genres.columns.tolist())\n",
        "                print(f\"genres.csv loaded successfully with columns: {df_genres.columns.tolist()}\")\n",
        "        except Exception as e:\n",
        "            logging.error(\"Failed to load genres.csv: %s\", str(e))\n",
        "            print(f\"Error loading genres.csv: {str(e)}\")\n",
        "            unique_genres = df_metadata['genre'].dropna().unique()\n",
        "            df_genres = pd.DataFrame({\n",
        "                'genre_id': range(1, len(unique_genres) + 1),\n",
        "                'genre_name': unique_genres\n",
        "            })\n",
        "    else:\n",
        "        logging.warning(\"genres.csv not found at %s. Creating fallback genres DataFrame.\", genres_path)\n",
        "        print(f\"genres.csv not found at {genres_path}. Creating fallback genres DataFrame.\")\n",
        "        unique_genres = df_metadata['genre'].dropna().unique()\n",
        "        df_genres = pd.DataFrame({\n",
        "            'genre_id': range(1, len(unique_genres) + 1),\n",
        "            'genre_name': unique_genres\n",
        "        })\n",
        "\n",
        "    # Log genre data for debugging\n",
        "    logging.info(\"Genres DataFrame head:\\n%s\", df_genres.head().to_string())\n",
        "    logging.info(\"Unique genres in metadata: %s\", df_metadata['genre'].dropna().unique().tolist())\n",
        "    print(\"Genres DataFrame head:\\n\", df_genres.head().to_string())\n",
        "    print(\"Unique genres in metadata:\", df_metadata['genre'].dropna().unique().tolist())\n",
        "\n",
        "    # Merge metadata with genres\n",
        "    try:\n",
        "        df_metadata = pd.merge(df_metadata, df_genres, left_on='genre', right_on='genre_name', how='left')\n",
        "        logging.info(\"Merged metadata with genres successfully.\")\n",
        "        print(\"Merged metadata with genres successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to merge genres with metadata: %s\", str(e))\n",
        "        print(f\"Error merging genres with metadata: {str(e)}\")\n",
        "        # Fallback: Assign genre_id based on genre column\n",
        "        genre_to_id = {g: i + 1 for i, g in enumerate(df_genres['genre_name'])}\n",
        "        df_metadata['genre_id'] = df_metadata['genre'].map(genre_to_id).fillna(0).astype(int)\n",
        "\n",
        "    # Handle missing genre_id\n",
        "    df_metadata['genre_id'] = df_metadata['genre_id'].fillna(0).astype(int)\n",
        "\n",
        "    # Select final columns\n",
        "    df_metadata = df_metadata[['track_id', 'artist_name', 'title', 'genre', 'genre_id']].dropna(subset=['genre'])\n",
        "\n",
        "    # Extract audio features\n",
        "    features = []\n",
        "    for track_id in df_metadata['track_id']:\n",
        "        audio_file = next((os.path.join(audio_dir, f\"{track_id}.mp3\") for audio_dir in audio_dirs if os.path.exists(os.path.join(audio_dir, f\"{track_id}.mp3\"))), None)\n",
        "        features.append([track_id] + extract_audio_features(audio_file).tolist())\n",
        "    feature_columns = ['track_id'] + [f'mfcc_{i+1}' for i in range(12)] + [f'chroma_{i+1}' for i in range(12)] + ['spectral_centroid', 'tempo']\n",
        "    df_features = pd.DataFrame(features, columns=feature_columns)\n",
        "\n",
        "    # Load or generate lyrics\n",
        "    lyrics_dict = {}\n",
        "    if os.path.exists(lyrics_path):\n",
        "        for lyric_file in Path(lyrics_path).glob(\"*.txt\"):\n",
        "            track_id = lyric_file.stem\n",
        "            if track_id in set(df_metadata['track_id']):\n",
        "                with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                    lyrics_dict[track_id] = f.read().strip() or 'music'\n",
        "    else:\n",
        "        try:\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            model = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
        "            lyrics_args = [(track_id, genre, lyrics_path, tokenizer, model) for track_id, genre in zip(df_metadata['track_id'], df_metadata['genre'])]\n",
        "            with Pool(processes=2) as pool:\n",
        "                list(tqdm(pool.imap(generate_lyrics, lyrics_args), total=len(lyrics_args), desc=\"Generating synthetic lyrics\"))\n",
        "        except Exception as e:\n",
        "            logging.error(\"Lyrics generation failed: %s\", str(e))\n",
        "            for track_id, genre in zip(df_metadata['track_id'], df_metadata['genre']):\n",
        "                os.makedirs(lyrics_path, exist_ok=True)\n",
        "                with open(os.path.join(lyrics_path, f\"{track_id}.txt\"), 'w', encoding='utf-8') as f:\n",
        "                    f.write(f\"{genre} lyrics placeholder\")\n",
        "        for track_id in df_metadata['track_id']:\n",
        "            lyric_file = os.path.join(lyrics_path, f\"{track_id}.txt\")\n",
        "            if os.path.exists(lyric_file):\n",
        "                with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                    lyrics_dict[track_id] = f.read().strip() or 'music'\n",
        "\n",
        "    # Incorporate tags\n",
        "    if os.path.exists(tags_path):\n",
        "        df_tags = pd.read_csv(tags_path)\n",
        "        df_tags['track_id'] = df_tags['track_id'].astype(str).str.zfill(6)\n",
        "        for _, row in df_tags.iterrows():\n",
        "            track_id = row['track_id']\n",
        "            if track_id in set(df_metadata['track_id']):\n",
        "                lyrics_dict[track_id] = lyrics_dict.get(track_id, '') + \" \" + str(row['tag'])\n",
        "\n",
        "    logging.info(\"Data loaded: Metadata %s, Features %s, Lyrics %d\", df_metadata.shape, df_features.shape, len(lyrics_dict))\n",
        "    print(f\"Data loaded: Metadata {df_metadata.shape}, Features {df_features.shape}, Lyrics {len(lyrics_dict)}\")\n",
        "    return df_metadata, df_features, lyrics_dict\n",
        "\n",
        "def generate_text_embeddings(lyrics_dict):\n",
        "    \"\"\"Generate semantic text embeddings using SentenceTransformer.\"\"\"\n",
        "    logging.info(\"Generating text embeddings...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    embeddings = {}\n",
        "    for track_id, text in tqdm(lyrics_dict.items(), desc=\"Generating text embeddings\"):\n",
        "        embeddings[track_id] = model.encode(text, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "def analyze_linguistic_patterns(df_metadata, lyrics_dict):\n",
        "    \"\"\"Analyze linguistic patterns and topics in lyrics.\"\"\"\n",
        "    print(\"\\nLinguistic Analysis:\")\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    genre_words = {g: [] for g in df_metadata['genre'].unique()}\n",
        "    for track_id, text in lyrics_dict.items():\n",
        "        genre = df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "        genre_words[genre].extend(tokens)\n",
        "    for genre, words in genre_words.items():\n",
        "        if words:\n",
        "            top_words = Counter(words).most_common(5)\n",
        "            print(f\"{genre} top words: {top_words}\")\n",
        "        else:\n",
        "            print(f\"{genre}: No words found after preprocessing\")\n",
        "    try:\n",
        "        vectorizer = CountVectorizer(stop_words='english', min_df=2)\n",
        "        X = vectorizer.fit_transform(lyrics_dict.values())\n",
        "        if X.shape[0] > 0 and X.shape[1] > 0:\n",
        "            lda = LatentDirichletAllocation(n_components=min(5, X.shape[1]), random_state=42)\n",
        "            lda.fit(X)\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "            for i, topic in enumerate(lda.components_):\n",
        "                top_words = [feature_names[j] for j in topic.argsort()[-5:]]\n",
        "                print(f\"Topic {i}: {top_words}\")\n",
        "        else:\n",
        "            print(\"Insufficient data for topic modeling\")\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Topic modeling failed: %s\", str(e))\n",
        "        print(\"Topic modeling failed due to insufficient or invalid data\")\n",
        "\n",
        "class AttentionFusion(nn.Module):\n",
        "    \"\"\"Attention-based fusion for audio and text features.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.audio_query = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_key = nn.Linear(text_dim, hidden_dim)\n",
        "        self.text_value = nn.Linear(text_dim, hidden_dim)\n",
        "        self.scale = 1.0 / (hidden_dim ** 0.5)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        query = self.audio_query(audio_features)\n",
        "        key = self.text_key(text_features)\n",
        "        value = self.text_value(text_features)\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) * self.scale\n",
        "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "        fused = torch.matmul(attention_weights, value)\n",
        "        return fused\n",
        "\n",
        "class AudioTextBERTClassifier(nn.Module):\n",
        "    \"\"\"DistilBERT with attention-based audio feature integration.\"\"\"\n",
        "    def __init__(self, bert_model, audio_dim, num_classes, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.attention = AttentionFusion(audio_dim, bert_model.config.hidden_size, hidden_dim)\n",
        "        self.combined_layer = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features):\n",
        "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_out = bert_outputs.pooler_output\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        fused = self.attention(audio_out, text_out)\n",
        "        combined = torch.cat([fused, audio_out], dim=1)\n",
        "        combined = torch.relu(self.combined_layer(combined))\n",
        "        combined = self.dropout(combined)\n",
        "        return self.fc(combined)\n",
        "\n",
        "class HybridRecommender(nn.Module):\n",
        "    \"\"\"Content-based recommender with DPP for diversity.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "        return torch.sigmoid(self.fc(combined))\n",
        "\n",
        "def dpp_diversity(outputs, k, genres, lambda_tradeoff=0.1):\n",
        "    \"\"\"Apply Determinantal Point Process for diverse recommendations.\"\"\"\n",
        "    scores = outputs.cpu().numpy().flatten()\n",
        "    n = len(scores)\n",
        "    L = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            genre_sim = 1.0 if genres[i] == genres[j] else 0.5\n",
        "            L[i, j] = genre_sim * np.exp(-((i - j) ** 2) / 2)\n",
        "    L = L + lambda_tradeoff * np.diag(scores)\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
        "    selected_indices = []\n",
        "    for _ in range(min(k, n)):\n",
        "        max_det = -np.inf\n",
        "        best_idx = None\n",
        "        for i in range(n):\n",
        "            if i not in selected_indices:\n",
        "                temp_indices = selected_indices + [i]\n",
        "                det = np.prod([eigenvalues[j] for j in temp_indices])\n",
        "                if det > max_det:\n",
        "                    max_det = det\n",
        "                    best_idx = i\n",
        "        if best_idx is not None:\n",
        "            selected_indices.append(best_idx)\n",
        "    return selected_indices[:k]\n",
        "\n",
        "def train_recommender(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Train the recommender model.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, ratings in train_loader:\n",
        "        audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, ratings.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_recommender(model, test_loader, df_metadata, test_idx_resampled, original_indices_resampled, genres, k=10):\n",
        "    \"\"\"Evaluate recommender with DPP for diversity.\"\"\"\n",
        "    model.eval()\n",
        "    precisions, recalls, maps, ndcgs, diversities = [], [], [], [], []\n",
        "    global train_indices\n",
        "    train_track_ids = set(df_metadata.iloc[train_indices]['track_id'])\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (audio_features, text_features, ratings) in enumerate(test_loader):\n",
        "            audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            batch_start = batch_idx * test_loader.batch_size\n",
        "            batch_end = batch_start + len(audio_features)\n",
        "            batch_test_idx_resampled = test_idx_resampled[batch_start:batch_end]\n",
        "            batch_original_indices = original_indices_resampled[batch_test_idx_resampled]\n",
        "            batch_genres = df_metadata.iloc[batch_original_indices]['genre'].values\n",
        "            top_k_indices = dpp_diversity(outputs, k, batch_genres)\n",
        "            top_k_original_indices = batch_original_indices[top_k_indices]\n",
        "            top_k_ids = df_metadata.iloc[top_k_original_indices]['track_id']\n",
        "            top_k_genres = df_metadata.iloc[top_k_original_indices]['genre']\n",
        "            binary_ratings = (ratings > 0.6).float()\n",
        "            relevant = binary_ratings[top_k_indices]\n",
        "            precision = relevant.mean().item()\n",
        "            recall = relevant.sum().item() / binary_ratings.sum().item() if binary_ratings.sum() > 0 else 0.0\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            map_score = average_precision_score(binary_ratings.cpu().numpy(), outputs.cpu().numpy()) if binary_ratings.sum() > 0 else 0.0\n",
        "            ndcg = ndcg_score(binary_ratings.cpu().numpy().reshape(1, -1), outputs.cpu().numpy().reshape(1, -1), k=k) if binary_ratings.sum() > 0 else 0.0\n",
        "            maps.append(map_score)\n",
        "            ndcgs.append(ndcg)\n",
        "            diversity = len(set(top_k_genres)) / len(genres) if len(genres) > 0 else 1.0\n",
        "            diversities.append(diversity)\n",
        "            novelty = 1 - len(set(top_k_ids).intersection(train_track_ids)) / len(top_k_ids) if len(top_k_ids) > 0 else 1.0\n",
        "    return {\n",
        "        'precision@k': np.mean(precisions) if precisions else 0.0,\n",
        "        'recall@k': np.mean(recalls) if recalls else 0.0,\n",
        "        'map': np.mean(maps) if maps else 0.0,\n",
        "        'ndcg@k': np.mean(ndcgs) if ndcgs else 0.0,\n",
        "        'diversity': np.mean(diversities) if diversities else 1.0,\n",
        "        'novelty': novelty\n",
        "    }\n",
        "\n",
        "def baseline_classifiers(X_train, y_train, X_test, y_test, genres):\n",
        "    \"\"\"Train and evaluate baseline classifiers (SVM, Random Forest).\"\"\"\n",
        "    svm = SVC(kernel='rbf', random_state=42)\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    svm.fit(X_train, y_train)\n",
        "    rf.fit(X_train, y_train)\n",
        "    svm_pred = svm.predict(X_test)\n",
        "    rf_pred = rf.predict(X_test)\n",
        "    svm_metrics = precision_recall_fscore_support(y_test, svm_pred, average='weighted', zero_division=0)\n",
        "    rf_metrics = precision_recall_fscore_support(y_test, rf_pred, average='weighted', zero_division=0)\n",
        "    return {\n",
        "        'SVM': {'precision': svm_metrics[0], 'recall': svm_metrics[1], 'f1': svm_metrics[2]},\n",
        "        'RandomForest': {'precision': rf_metrics[0], 'recall': rf_metrics[1], 'f1': rf_metrics[2]}\n",
        "    }\n",
        "\n",
        "def collaborative_filtering(user_data, df_metadata, k=10):\n",
        "    \"\"\"Collaborative filtering using SVD with user profiles.\"\"\"\n",
        "    valid_track_ids = set(df_metadata['track_id'])\n",
        "    user_data = user_data[user_data['track_id'].isin(valid_track_ids)]\n",
        "    if user_data.empty:\n",
        "        return {'precision@k': 0.0}\n",
        "    train_data, test_data = train_test_split(user_data, test_size=0.2, random_state=42)\n",
        "    user_item_matrix = train_data.pivot(index='user_id', columns='track_id', values='rating').fillna(0)\n",
        "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
        "    user_features = svd.fit_transform(user_item_matrix)\n",
        "    item_features = svd.components_\n",
        "    predictions = np.dot(user_features, item_features)\n",
        "    predicted_ratings = pd.DataFrame(predictions, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
        "    precisions = []\n",
        "    for user_id in test_data['user_id'].unique():\n",
        "        user_ratings = test_data[test_data['user_id'] == user_id]\n",
        "        true_ratings = user_ratings.set_index('track_id')['rating']\n",
        "        pred_ratings = predicted_ratings.loc[user_id]\n",
        "        valid_top_k = pred_ratings.sort_values(ascending=False).index[:k]\n",
        "        relevant = (true_ratings[true_ratings.index.isin(valid_top_k)] > 3.0).astype(int)\n",
        "        precision = relevant.mean() if len(relevant) > 0 else 0.0\n",
        "        precisions.append(precision)\n",
        "    return {'precision@k': np.mean(precisions) if precisions else 0.0}\n",
        "\n",
        "class GenreClassifier(nn.Module):\n",
        "    \"\"\"Custom genre classifier with audio and text features.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, num_classes, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.attention = AttentionFusion(hidden_dim, hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        fused = self.attention(audio_out, text_out)\n",
        "        combined = self.dropout(fused)\n",
        "        return self.fc(combined)\n",
        "\n",
        "def train_classifier(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Train the classifier model.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, labels in train_loader:\n",
        "        audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_classifier(model, test_loader, genres):\n",
        "    \"\"\"Evaluate classifier with detailed error analysis.\"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    errors = [(i, genres[true], genres[pred]) for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
        "    error_counts = Counter((true, pred) for _, true, pred in errors)\n",
        "    print(\"\\nMisclassification Patterns:\")\n",
        "    for (true, pred), count in error_counts.most_common():\n",
        "        print(f\"True: {true}, Predicted: {pred}, Count: {count}\")\n",
        "    return {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n",
        "\n",
        "def bert_classifier(df_metadata, text_embeddings, audio_features, genres):\n",
        "    \"\"\"DistilBERT-based genre classification with audio features.\"\"\"\n",
        "    print(\"\\nTraining DistilBERT Classifier...\")\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    bert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(genres))\n",
        "    model = AudioTextBERTClassifier(bert_model, audio_dim=audio_features.shape[1], num_classes=len(genres)).to(DEVICE)\n",
        "\n",
        "    # Filter valid rows and ensure genres are in genres list\n",
        "    valid_rows = df_metadata[['track_id', 'title', 'genre']].dropna(subset=['genre'])\n",
        "    genres_list = genres.tolist()\n",
        "    valid_rows = valid_rows[valid_rows['genre'].isin(genres_list)]\n",
        "\n",
        "    if valid_rows.empty:\n",
        "        logging.error(\"No valid rows for DistilBERT classifier after filtering genres\")\n",
        "        print(\"Error: No valid rows for DistilBERT classifier after filtering genres\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    valid_track_ids = valid_rows['track_id'].values\n",
        "    logging.info(\"Number of valid tracks for DistilBERT: %d\", len(valid_track_ids))\n",
        "    print(f\"Number of valid tracks for DistilBERT: {len(valid_track_ids)}\")\n",
        "\n",
        "    # Create genre-to-index mapping\n",
        "    genre_to_idx = {genre: idx for idx, genre in enumerate(genres_list)}\n",
        "    logging.info(\"Genre to index mapping: %s\", genre_to_idx)\n",
        "    print(f\"Genre to index mapping: {genre_to_idx}\")\n",
        "\n",
        "    # Prepare audio features\n",
        "    audio_features_df = pd.DataFrame(audio_features, index=df_metadata['track_id'])\n",
        "    audio_features = audio_features_df.loc[valid_track_ids].values\n",
        "\n",
        "    # Prepare texts and labels\n",
        "    texts = [f\"{row['title']} {row['genre']}\" for _, row in valid_rows.iterrows()]\n",
        "    labels = []\n",
        "    for _, row in valid_rows.iterrows():\n",
        "        try:\n",
        "            label = genre_to_idx[row['genre']]\n",
        "            labels.append(label)\n",
        "        except KeyError:\n",
        "            logging.warning(\"Genre %s not found in genre_to_idx for track %s\", row['genre'], row['track_id'])\n",
        "            continue\n",
        "\n",
        "    if not labels:\n",
        "        logging.error(\"No valid labels generated for DistilBERT classifier\")\n",
        "        print(\"Error: No valid labels generated for DistilBERT classifier\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    # Tokenize texts\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset_list = [\n",
        "        {\n",
        "            'input_ids': inputs['input_ids'][i].numpy(),\n",
        "            'attention_mask': inputs['attention_mask'][i].numpy(),\n",
        "            'audio_features': audio_features[i].astype(np.float32),\n",
        "            'labels': int(labels[i])\n",
        "        }\n",
        "        for i in range(len(valid_rows))\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        dataset = Dataset.from_list(dataset_list)\n",
        "        dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to create dataset for DistilBERT: %s\", str(e))\n",
        "        print(f\"Failed to create dataset for DistilBERT: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        return {\n",
        "            'input_ids': torch.tensor([item['input_ids'] for item in batch], dtype=torch.long).to(DEVICE),\n",
        "            'attention_mask': torch.tensor([item['attention_mask'] for item in batch], dtype=torch.long).to(DEVICE),\n",
        "            'audio_features': torch.tensor([item['audio_features'] for item in batch], dtype=torch.float32).to(DEVICE),\n",
        "            'labels': torch.tensor([item['labels'] for item in batch], dtype=torch.long).to(DEVICE)\n",
        "        }\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(OUTPUT_DIR, \"distilbert-finetuned\"),\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        data_collator=collate_fn,\n",
        "        compute_metrics=lambda pred: {\n",
        "            'precision': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[0],\n",
        "            'recall': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[1],\n",
        "            'f1': precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='weighted', zero_division=0)[2]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        eval_results = trainer.evaluate()\n",
        "        return {\n",
        "            'precision': eval_results.get('eval_precision', 0.0),\n",
        "            'recall': eval_results.get('eval_recall', 0.0),\n",
        "            'f1': eval_results.get('eval_f1', 0.0)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logging.error(\"DistilBERT training failed: %s\", str(e))\n",
        "        print(f\"DistilBERT training failed: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "def music_search(query, df, text_embeddings, audio_features_df, model, scaler, k=10):\n",
        "    \"\"\"Content-based music search with cross-modal alignment.\"\"\"\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
        "    text_embedding_matrix = np.array([text_embeddings.get(tid, np.zeros(TEXT_EMBEDDING_DIM)) for tid in df['track_id']])\n",
        "    text_index = faiss.IndexFlatIP(TEXT_EMBEDDING_DIM)\n",
        "    text_index.add(text_embedding_matrix.astype(np.float32))\n",
        "    text_scores, text_indices = text_index.search(query_embedding.reshape(1, -1).astype(np.float32), k=20)\n",
        "    audio_features = audio_features_df[[col for col in audio_features_df.columns if col != 'track_id']].values\n",
        "    audio_features = scaler.transform(np.nan_to_num(audio_features))\n",
        "    combined_scores = {}\n",
        "    for idx, score in zip(text_indices[0], text_scores[0]):\n",
        "        track_id = df['track_id'].iloc[idx]\n",
        "        genre = df[df['track_id'] == track_id]['genre'].iloc[0]\n",
        "        genre_weight = 1.5 if genre.lower() in query.lower() else 1.0\n",
        "        combined_scores[track_id] = 0.7 * score * genre_weight\n",
        "        audio_idx = audio_features_df.index[audio_features_df['track_id'] == track_id].tolist()[0]\n",
        "        genre_tracks = df[df['genre'] == genre]['track_id']\n",
        "        genre_audio_features = audio_features[audio_features_df['track_id'].isin(genre_tracks)]\n",
        "        if len(genre_audio_features) > 0:\n",
        "            avg_genre_audio = np.mean(genre_audio_features, axis=0)\n",
        "            audio_similarity = 1 - cosine(audio_features[audio_idx], avg_genre_audio)\n",
        "            combined_scores[track_id] += 0.3 * audio_similarity * genre_weight\n",
        "    top_tracks = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "    result_ids = [track_id for track_id, _ in top_tracks]\n",
        "    results = df[df['track_id'].isin(result_ids)][['track_id', 'artist_name', 'title', 'genre']]\n",
        "    relevant_tracks = df[df['genre'].str.lower().isin(query.lower().split())]['track_id'].values\n",
        "    y_true = [1 if track_id in relevant_tracks else 0 for track_id in result_ids]\n",
        "    y_scores = [combined_scores[track_id] for track_id in result_ids]\n",
        "    precision = sum(y_true) / len(y_true) if len(y_true) > 0 else 0.0\n",
        "    recall = sum(y_true) / len(relevant_tracks) if len(relevant_tracks) > 0 else 0.0\n",
        "    map_score = average_precision_score(y_true, y_scores) if sum(y_true) > 0 else 0.0\n",
        "    ndcg = ndcg_score(np.array(y_true).reshape(1, -1), np.array(y_scores).reshape(1, -1), k=k) if sum(y_true) > 0 else 0.0\n",
        "    diversity = len(set(results['genre'])) / len(df['genre'].unique()) if len(df['genre'].unique()) > 0 else 1.0\n",
        "    novelty = len(set(result_ids) - set(df['track_id'].iloc[train_indices])) / len(result_ids) if len(result_ids) > 0 else 1.0\n",
        "    return results, {'precision@k': precision, 'recall@k': recall, 'map': map_score, 'ndcg@k': ndcg, 'diversity': diversity, 'novelty': novelty}\n",
        "\n",
        "def streamlit_ui(df_metadata, text_embeddings, audio_features_df, model, scaler):\n",
        "    \"\"\"Interactive Streamlit UI for music search and recommendation.\"\"\"\n",
        "    st.title(\"Music Recommender System\")\n",
        "    query = st.text_input(\"Enter a music query (e.g., 'upbeat rock songs'):\", \"upbeat rock songs\")\n",
        "    k = st.slider(\"Number of results:\", 1, 20, 10)\n",
        "    if st.button(\"Search\"):\n",
        "        results, metrics = music_search(query, df_metadata, text_embeddings, audio_features_df, model, scaler, k)\n",
        "        st.write(\"### Search Results\")\n",
        "        st.dataframe(results)\n",
        "        st.write(\"### Retrieval Metrics\")\n",
        "        st.json(metrics)\n",
        "    user_id = st.text_input(\"Enter user ID for recommendations:\", \"user_1\")\n",
        "    if st.button(\"Recommend\"):\n",
        "        user_data = pd.read_csv(USER_DATA_PATH)\n",
        "        user_ratings = user_data[user_data['user_id'] == user_id]\n",
        "        if not user_ratings.empty:\n",
        "            track_ids = user_ratings['track_id'].values\n",
        "            audio_features = audio_features_df[audio_features_df['track_id'].isin(track_ids)][[col for col in audio_features_df.columns if col != 'track_id']].values\n",
        "            text_features = np.array([text_embeddings.get(tid, np.zeros(TEXT_EMBEDDING_DIM)) for tid in track_ids])\n",
        "            recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=TEXT_EMBEDDING_DIM).to(DEVICE)\n",
        "            recommender.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = recommender(torch.tensor(audio_features, dtype=torch.float32).to(DEVICE), torch.tensor(text_features, dtype=torch.float32).to(DEVICE))\n",
        "                top_k = torch.topk(outputs, k=k).indices.flatten()\n",
        "                recommended_ids = track_ids[top_k.cpu()]\n",
        "                st.write(\"### Recommended Tracks\")\n",
        "                st.dataframe(df_metadata[df_metadata['track_id'].isin(recommended_ids)][['track_id', 'artist_name', 'title', 'genre']])\n",
        "\n",
        "def analyze_feature_contribution(model, test_loader, feature_columns):\n",
        "    \"\"\"Analyze feature contributions to model predictions.\"\"\"\n",
        "    model.eval()\n",
        "    contributions = {col: [] for col in feature_columns}\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, _ in test_loader:\n",
        "            audio_features, text_features = audio_features.to(DEVICE), text_features.to(DEVICE)\n",
        "            baseline_output = model(audio_features, text_features)\n",
        "            for i, col in enumerate(feature_columns):\n",
        "                modified_features = audio_features.clone()\n",
        "                modified_features[:, i] = 0\n",
        "                modified_output = model(modified_features, text_features)\n",
        "                diff = torch.mean(torch.abs(baseline_output - modified_output)).item()\n",
        "                contributions[col].append(diff)\n",
        "    for col in feature_columns:\n",
        "        print(f\"{col}: Mean contribution = {np.mean(contributions[col]):.4f}\")\n",
        "    return contributions\n",
        "\n",
        "class TestMusicRecommender(unittest.TestCase):\n",
        "    \"\"\"Unit tests for music recommender system.\"\"\"\n",
        "    def setUp(self):\n",
        "        self.df = pd.DataFrame({\n",
        "            'track_id': ['000001', '000002'],\n",
        "            'artist_name': ['Artist_1', 'Artist_2'],\n",
        "            'title': ['Track_1', 'Track_2'],\n",
        "            'genre': ['Rock', 'Pop'],\n",
        "            'genre_id': [1, 2]\n",
        "        })\n",
        "        self.lyrics_dict = {'000001': 'rock anthem roars', '000002': 'pop fever rises'}\n",
        "        self.audio_features = np.random.randn(2, 26)\n",
        "        self.text_embeddings = {'000001': np.random.randn(384), '000002': np.random.randn(384)}\n",
        "\n",
        "    def test_generate_lyrics(self):\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        track_id = generate_lyrics(('000001', 'Rock', LYRICS_PATH, tokenizer, model))\n",
        "        self.assertEqual(track_id, '000001')\n",
        "        self.assertTrue(os.path.exists(os.path.join(LYRICS_PATH, '000001.txt')))\n",
        "\n",
        "    def test_music_search(self):\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        scaler = StandardScaler().fit(self.audio_features)\n",
        "        df_features = pd.DataFrame(self.audio_features, columns=['track_id'] + [f'feat_{i}' for i in range(26)])\n",
        "        df_features['track_id'] = self.df['track_id']\n",
        "        results, metrics = music_search(\"rock\", self.df, self.text_embeddings, df_features, model, scaler)\n",
        "        self.assertEqual(len(results), 2)\n",
        "        self.assertIn('precision@k', metrics)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate music recommender tasks.\"\"\"\n",
        "    logging.info(\"Starting main execution...\")\n",
        "    try:\n",
        "        df_metadata, df_features, lyrics_dict = load_fma_data(FMA_AUDIO_DIRS, METADATA_PATH, ARTISTS_PATH, GENRES_PATH, LYRICS_PATH, TAGS_PATH)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Data loading failed: %s\", str(e))\n",
        "        print(f\"Data loading failed: {str(e)}\")\n",
        "        return\n",
        "    if df_metadata.empty or df_features.empty or not lyrics_dict:\n",
        "        logging.error(\"Data loading resulted in empty datasets\")\n",
        "        print(\"Data loading resulted in empty datasets\")\n",
        "        return\n",
        "    text_embeddings = generate_text_embeddings(lyrics_dict)\n",
        "    reference_lyrics = {\n",
        "        tid: f\"Sample {row['title']} lyrics in {row['genre']}\"\n",
        "        for tid, row in df_metadata[['track_id', 'title', 'genre']].iterrows()\n",
        "    }\n",
        "    bleu_score = evaluate_lyrics_quality(lyrics_dict, reference_lyrics)\n",
        "    print(f\"Average BLEU Score for Lyrics: {bleu_score:.4f}\")\n",
        "    analyze_linguistic_patterns(df_metadata, lyrics_dict)\n",
        "    valid_track_ids = df_features['track_id'].tolist()\n",
        "    df_metadata = df_metadata[df_metadata['track_id'].isin(valid_track_ids)]\n",
        "    text_features = np.array([text_embeddings.get(tid, np.zeros(TEXT_EMBEDDING_DIM)) for tid in df_metadata['track_id']])\n",
        "    scaler = StandardScaler()\n",
        "    audio_features = scaler.fit_transform(df_features[[col for col in df_features.columns if col != 'track_id']].values)\n",
        "    audio_features = np.nan_to_num(audio_features)\n",
        "\n",
        "    # Load or generate synthetic user ratings\n",
        "    if os.path.exists(USER_DATA_PATH):\n",
        "        try:\n",
        "            user_data = pd.read_csv(USER_DATA_PATH)\n",
        "            user_data['track_id'] = user_data['track_id'].astype(str).str.zfill(6)\n",
        "            logging.info(\"User ratings loaded from %s\", USER_DATA_PATH)\n",
        "            print(f\"User ratings loaded from {USER_DATA_PATH}\")\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Failed to load ratings.csv: %s. Generating synthetic ratings.\", str(e))\n",
        "            print(f\"Failed to load ratings.csv: {str(e)}. Generating synthetic ratings.\")\n",
        "            user_ids = [f\"user_{i+1}\" for i in range(10)]  # 10 synthetic users\n",
        "            user_data = pd.DataFrame({\n",
        "                'user_id': [random.choice(user_ids) for _ in range(len(df_metadata))],\n",
        "                'track_id': df_metadata['track_id'],\n",
        "                'rating': np.random.randint(1, 6, size=len(df_metadata))\n",
        "            })\n",
        "            os.makedirs(os.path.dirname(USER_DATA_PATH), exist_ok=True)\n",
        "            user_data.to_csv(USER_DATA_PATH, index=False)\n",
        "            logging.info(\"Synthetic ratings saved to %s\", USER_DATA_PATH)\n",
        "            print(f\"Synthetic ratings saved to {USER_DATA_PATH}\")\n",
        "    else:\n",
        "        logging.warning(\"ratings.csv not found at %s. Generating synthetic ratings.\", USER_DATA_PATH)\n",
        "        print(f\"ratings.csv not found at {USER_DATA_PATH}. Generating synthetic ratings.\")\n",
        "        user_ids = [f\"user_{i+1}\" for i in range(10)]  # 10 synthetic users\n",
        "        user_data = pd.DataFrame({\n",
        "            'user_id': [random.choice(user_ids) for _ in range(len(df_metadata))],\n",
        "            'track_id': df_metadata['track_id'],\n",
        "            'rating': np.random.randint(1, 6, size=len(df_metadata))\n",
        "        })\n",
        "        os.makedirs(os.path.dirname(USER_DATA_PATH), exist_ok=True)\n",
        "        user_data.to_csv(USER_DATA_PATH, index=False)\n",
        "        logging.info(\"Synthetic ratings saved to %s\", USER_DATA_PATH)\n",
        "        print(f\"Synthetic ratings saved to {USER_DATA_PATH}\")\n",
        "\n",
        "    user_data = user_data[user_data['track_id'].isin(df_metadata['track_id'])]\n",
        "    ratings_agg = user_data.groupby('track_id')['rating'].mean().reindex(df_metadata['track_id']).fillna(3.0).values\n",
        "    ratings = np.clip(ratings_agg / 5.0, 0.0, 1.0)\n",
        "    X = np.hstack([audio_features, text_features])\n",
        "    original_indices = np.arange(len(X))\n",
        "    binary_ratings = (ratings > 0.5).astype(int)\n",
        "    # Check genre distribution and adjust k_neighbors for SMOTE\n",
        "    genre_counts = df_metadata['genre'].value_counts()\n",
        "    print(\"\\nGenre Distribution:\")\n",
        "    for genre, count in genre_counts.items():\n",
        "        print(f\"{genre}: {count} tracks\")\n",
        "    min_samples = min(genre_counts) if not genre_counts.empty else 1\n",
        "    k_neighbors = min(5, max(1, min_samples - 1))\n",
        "    logging.info(\"Using k_neighbors=%d for SMOTE based on minimum class size %d\", k_neighbors, min_samples)\n",
        "    try:\n",
        "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "        X_resampled, ratings_resampled = smote.fit_resample(X, binary_ratings)\n",
        "        original_indices_resampled = np.concatenate([original_indices, np.random.choice(original_indices, size=len(X_resampled) - len(X), replace=True)])\n",
        "    except ValueError as e:\n",
        "        logging.warning(\"SMOTE failed: %s. Falling back to original data.\", str(e))\n",
        "        print(f\"SMOTE failed: {str(e)}. Using original data.\")\n",
        "        X_resampled, ratings_resampled = X, binary_ratings\n",
        "        original_indices_resampled = original_indices\n",
        "    shuffle_indices = np.random.permutation(len(X_resampled))\n",
        "    X_resampled = X_resampled[shuffle_indices]\n",
        "    ratings_resampled = ratings_resampled[shuffle_indices]\n",
        "    original_indices_resampled = original_indices_resampled[shuffle_indices]\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rec_metrics_all = []\n",
        "    global train_indices\n",
        "    for fold, (train_idx_resampled, test_idx_resampled) in enumerate(kf.split(X_resampled)):\n",
        "        print(f\"\\nFold {fold+1}\")\n",
        "        train_indices = original_indices_resampled[train_idx_resampled]\n",
        "        X_train_rec, X_test_rec = X_resampled[train_idx_resampled], X_resampled[test_idx_resampled]\n",
        "        y_train_rec, y_test_rec = ratings_resampled[train_idx_resampled], ratings_resampled[test_idx_resampled]\n",
        "        train_dataset_rec = torch.utils.data.TensorDataset(\n",
        "            torch.tensor(X_train_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "            torch.tensor(X_train_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "            torch.tensor(y_train_rec, dtype=torch.float32)\n",
        "        )\n",
        "        train_loader_rec = torch.utils.data.DataLoader(train_dataset_rec, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "        test_dataset_rec = torch.utils.data.TensorDataset(\n",
        "            torch.tensor(X_test_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "            torch.tensor(X_test_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "            torch.tensor(y_test_rec, dtype=torch.float32)\n",
        "        )\n",
        "        test_loader_rec = torch.utils.data.DataLoader(test_dataset_rec, batch_size=BATCH_SIZE, num_workers=2)\n",
        "        recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=TEXT_EMBEDDING_DIM).to(DEVICE)\n",
        "        criterion_rec = nn.BCELoss()\n",
        "        optimizer_rec = torch.optim.Adam(recommender.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "        for epoch in range(NUM_EPOCHS_REC):\n",
        "            loss = train_recommender(recommender, train_loader_rec, criterion_rec, optimizer_rec)\n",
        "            print(f\"Recommendation Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "        rec_metrics = evaluate_recommender(recommender, test_loader_rec, df_metadata, test_idx_resampled, original_indices_resampled, df_metadata['genre'].unique())\n",
        "        rec_metrics_all.append(rec_metrics)\n",
        "        print(f\"Recommendation Metrics: {rec_metrics}\")\n",
        "    avg_rec_metrics = {k: np.mean([m[k] for m in rec_metrics_all]) for k in rec_metrics_all[0]}\n",
        "    print(f\"Average Recommendation Metrics: {avg_rec_metrics}\")\n",
        "    cf_metrics = collaborative_filtering(user_data, df_metadata)\n",
        "    print(f\"Collaborative Filtering Metrics: {cf_metrics}\")\n",
        "    genres = df_metadata['genre'].unique()\n",
        "    genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
        "    labels = df_metadata['genre'].map(genre_to_idx).values\n",
        "    cls_metrics_all = []\n",
        "    baseline_metrics = []\n",
        "    for fold, (train_idx_resampled, test_idx_resampled) in enumerate(kf.split(X_resampled)):\n",
        "        print(f\"\\nFold {fold+1}\")\n",
        "        X_train_cls, X_test_cls = X_resampled[train_idx_resampled], X_resampled[test_idx_resampled]\n",
        "        y_train_cls = labels[original_indices_resampled[train_idx_resampled]]\n",
        "        y_test_cls = labels[original_indices_resampled[test_idx_resampled]]\n",
        "        train_dataset_cls = torch.utils.data.TensorDataset(\n",
        "            torch.tensor(X_train_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "            torch.tensor(X_train_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "            torch.tensor(y_train_cls, dtype=torch.long)\n",
        "        )\n",
        "        train_loader_cls = torch.utils.data.DataLoader(train_dataset_cls, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "        test_dataset_cls = torch.utils.data.TensorDataset(\n",
        "            torch.tensor(X_test_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "            torch.tensor(X_test_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "            torch.tensor(y_test_cls, dtype=torch.long)\n",
        "        )\n",
        "        test_loader_cls = torch.utils.data.DataLoader(test_dataset_cls, batch_size=BATCH_SIZE, num_workers=2)\n",
        "        classifier = GenreClassifier(\n",
        "            audio_dim=audio_features.shape[1],\n",
        "            text_dim=TEXT_EMBEDDING_DIM,\n",
        "            num_classes=len(genres)\n",
        "        ).to(DEVICE)\n",
        "        criterion_cls = nn.CrossEntropyLoss()\n",
        "        optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "        for epoch in range(NUM_EPOCHS_CLS):\n",
        "            loss = train_classifier(classifier, train_loader_cls, criterion_cls, optimizer_cls)\n",
        "            print(f\"Classification Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "        cls_metrics = evaluate_classifier(classifier, test_loader_cls, genres)\n",
        "        cls_metrics_all.append(cls_metrics)\n",
        "        print(f\"Classification Metrics: {cls_metrics}\")\n",
        "        baseline_metrics.append(baseline_classifiers(X_train_cls, y_train_cls, X_test_cls, y_test_cls, genres))\n",
        "    avg_cls_metrics = {k: np.mean([m[k] for m in cls_metrics_all]) for k in cls_metrics_all[0]}\n",
        "    print(f\"Average Classification Metrics: {avg_cls_metrics}\")\n",
        "    avg_baseline_metrics = {\n",
        "        model: {k: np.mean([m[model][k] for m in baseline_metrics]) for k in baseline_metrics[0][model]}\n",
        "        for model in baseline_metrics[0]\n",
        "    }\n",
        "    print(f\"Baseline Metrics: {avg_baseline_metrics}\")\n",
        "    bert_metrics = bert_classifier(df_metadata, text_embeddings, audio_features, genres)\n",
        "    print(f\"DistilBERT Classifier Metrics: {bert_metrics}\")\n",
        "    search_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    queries = [\"upbeat rock songs\", \"smooth jazz vibes\", \"energetic pop tracks\", \"classical orchestral\"]\n",
        "    for query in queries:\n",
        "        results, search_metrics = music_search(query, df_metadata, text_embeddings, df_features, search_model, scaler)\n",
        "        print(f\"\\nSearch Metrics for '{query}': {search_metrics}\")\n",
        "    feature_columns = [col for col in df_features.columns if col != 'track_id']\n",
        "    analyze_feature_contribution(classifier, test_loader_cls, feature_columns)\n",
        "    if 'streamlit' in globals():\n",
        "        streamlit_ui(df_metadata, text_embeddings, df_features, search_model, scaler)\n",
        "    unittest.main(argv=[''], exit=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ssl9XfPLe2w2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327cc620-d184-4ff4-fc81-37d0e939ab93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing faiss-cpu...\n",
            "Successfully installed faiss-cpu\n",
            "Installing scikit-learn...\n",
            "Successfully installed scikit-learn\n",
            "Mounted at /content/drive\n",
            "Loading FMA data...\n",
            "\n",
            "Available columns in tracks.csv:\n",
            "['Unnamed: 0_level_0_track_id', 'comments_Unnamed: 1_level_1', 'date_created_Unnamed: 2_level_1', 'date_released_Unnamed: 3_level_1', 'engineer_Unnamed: 4_level_1', 'favorites_Unnamed: 5_level_1', 'id_Unnamed: 6_level_1', 'information_Unnamed: 7_level_1', 'listens_Unnamed: 8_level_1', 'producer_Unnamed: 9_level_1', 'tags_Unnamed: 10_level_1', 'title_Unnamed: 11_level_1', 'tracks_Unnamed: 12_level_1', 'type_Unnamed: 13_level_1', 'active_year_begin_Unnamed: 14_level_1', 'active_year_end_Unnamed: 15_level_1', 'associated_labels_Unnamed: 16_level_1', 'bio_Unnamed: 17_level_1', 'comments_Unnamed: 18_level_1', 'date_created_Unnamed: 19_level_1', 'favorites_Unnamed: 20_level_1', 'id_Unnamed: 21_level_1', 'latitude_Unnamed: 22_level_1', 'location_Unnamed: 23_level_1', 'longitude_Unnamed: 24_level_1', 'members_Unnamed: 25_level_1', 'name_Unnamed: 26_level_1', 'related_projects_Unnamed: 27_level_1', 'tags_Unnamed: 28_level_1', 'website_Unnamed: 29_level_1', 'wikipedia_page_Unnamed: 30_level_1', 'split_Unnamed: 31_level_1', 'subset_Unnamed: 32_level_1', 'bit_rate_Unnamed: 33_level_1', 'comments_Unnamed: 34_level_1', 'composer_Unnamed: 35_level_1', 'date_created_Unnamed: 36_level_1', 'date_recorded_Unnamed: 37_level_1', 'duration_Unnamed: 38_level_1', 'favorites_Unnamed: 39_level_1', 'genre_top_Unnamed: 40_level_1', 'genres_Unnamed: 41_level_1', 'genres_all_Unnamed: 42_level_1', 'information_Unnamed: 43_level_1', 'interest_Unnamed: 44_level_1', 'language_code_Unnamed: 45_level_1', 'license_Unnamed: 46_level_1', 'listens_Unnamed: 47_level_1', 'lyricist_Unnamed: 48_level_1', 'number_Unnamed: 49_level_1', 'publisher_Unnamed: 50_level_1', 'tags_Unnamed: 51_level_1', 'title_Unnamed: 52_level_1']\n",
            "\n",
            "Sample data (first 2 rows):\n",
            "   Unnamed: 0_level_0_track_id  comments_Unnamed: 1_level_1 date_created_Unnamed: 2_level_1 date_released_Unnamed: 3_level_1 engineer_Unnamed: 4_level_1  favorites_Unnamed: 5_level_1  id_Unnamed: 6_level_1 information_Unnamed: 7_level_1  listens_Unnamed: 8_level_1 producer_Unnamed: 9_level_1 tags_Unnamed: 10_level_1 title_Unnamed: 11_level_1  tracks_Unnamed: 12_level_1 type_Unnamed: 13_level_1 active_year_begin_Unnamed: 14_level_1 active_year_end_Unnamed: 15_level_1 associated_labels_Unnamed: 16_level_1                                                   bio_Unnamed: 17_level_1  comments_Unnamed: 18_level_1 date_created_Unnamed: 19_level_1  favorites_Unnamed: 20_level_1  id_Unnamed: 21_level_1  latitude_Unnamed: 22_level_1 location_Unnamed: 23_level_1  longitude_Unnamed: 24_level_1                                                                                                      members_Unnamed: 25_level_1 name_Unnamed: 26_level_1                                                                                                                                                                                       related_projects_Unnamed: 27_level_1 tags_Unnamed: 28_level_1              website_Unnamed: 29_level_1 wikipedia_page_Unnamed: 30_level_1 split_Unnamed: 31_level_1 subset_Unnamed: 32_level_1  bit_rate_Unnamed: 33_level_1  comments_Unnamed: 34_level_1 composer_Unnamed: 35_level_1 date_created_Unnamed: 36_level_1 date_recorded_Unnamed: 37_level_1  duration_Unnamed: 38_level_1  favorites_Unnamed: 39_level_1 genre_top_Unnamed: 40_level_1 genres_Unnamed: 41_level_1 genres_all_Unnamed: 42_level_1 information_Unnamed: 43_level_1  interest_Unnamed: 44_level_1 language_code_Unnamed: 45_level_1                             license_Unnamed: 46_level_1  listens_Unnamed: 47_level_1 lyricist_Unnamed: 48_level_1  number_Unnamed: 49_level_1 publisher_Unnamed: 50_level_1 tags_Unnamed: 51_level_1 title_Unnamed: 52_level_1\n",
            "0                            2                            0             2008-11-26 01:44:45              2009-01-05 00:00:00                         NaN                             4                      1                        <p></p>                        6073                         NaN                       []      AWOL - A Way Of Life                           7                    Album                   2006-01-01 00:00:00                                 NaN                                   NaN  <p>A Way Of Life, A Collective of Hip-Hop from NJ...................</p>                             0              2008-11-26 01:42:32                              9                       1                     40.058324                   New Jersey                     -74.405661  Sajje Morocco,Brownbum,ZawidaGod,Custodian of Records,Zooberelli the Don,F.A.H,MadSicka,Damien Omenicci..and a van load more...                     AWOL  The list of past projects is 2 long but every1 and every style from Tabby Bonet 2 M.O.P..Azillion Records Flagship trackmaster DJ BrownBum is a beat Wizard.....A-2-Z..illion....(right now working with JerseyBlock Ent)                 ['awol']  http://www.AzillionRecords.blogspot.com                                NaN                  training                      small                        256000                             0                          NaN              2008-11-26 01:48:12               2008-11-26 00:00:00                           168                              2                       Hip-Hop                       [21]                           [21]                             NaN                          4656                                en  Attribution-NonCommercial-ShareAlike 3.0 International                         1293                          NaN                           3                           NaN                       []                      Food\n",
            "1                            3                            0             2008-11-26 01:44:45              2009-01-05 00:00:00                         NaN                             4                      1                        <p></p>                        6073                         NaN                       []      AWOL - A Way Of Life                           7                    Album                   2006-01-01 00:00:00                                 NaN                                   NaN  <p>A Way Of Life, A Collective of Hip-Hop from NJ...................</p>                             0              2008-11-26 01:42:32                              9                       1                     40.058324                   New Jersey                     -74.405661  Sajje Morocco,Brownbum,ZawidaGod,Custodian of Records,Zooberelli the Don,F.A.H,MadSicka,Damien Omenicci..and a van load more...                     AWOL  The list of past projects is 2 long but every1 and every style from Tabby Bonet 2 M.O.P..Azillion Records Flagship trackmaster DJ BrownBum is a beat Wizard.....A-2-Z..illion....(right now working with JerseyBlock Ent)                 ['awol']  http://www.AzillionRecords.blogspot.com                                NaN                  training                     medium                        256000                             0                          NaN              2008-11-26 01:48:14               2008-11-26 00:00:00                           237                              1                       Hip-Hop                       [21]                           [21]                             NaN                          1470                                en  Attribution-NonCommercial-ShareAlike 3.0 International                          514                          NaN                           4                           NaN                       []              Electric Ave\n",
            "Selected columns: {'track_id': 'Unnamed: 0_level_0_track_id', 'title': 'title_Unnamed: 52_level_1', 'artist_id': 'id_Unnamed: 21_level_1', 'genre_top': 'genre_top_Unnamed: 40_level_1'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:genres.csv does not contain 'genre_name' column. Creating fallback genres DataFrame.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: genres.csv does not contain 'genre_name' column. Creating fallback genres DataFrame.\n",
            "Genres DataFrame head:\n",
            "    genre_id    genre_name\n",
            "0         1       Hip-Hop\n",
            "1         2           Pop\n",
            "2         3          Folk\n",
            "3         4  Experimental\n",
            "4         5          Rock\n",
            "Unique genres in metadata: ['Hip-Hop', 'Pop', 'Folk', 'Experimental', 'Rock', 'International', 'Electronic']\n",
            "Merged metadata with genres successfully.\n",
            "Data loaded: Metadata (100, 5), Features (100, 27), Lyrics 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating text embeddings:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "Generating text embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 17.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score for Lyrics: 0.0000\n",
            "\n",
            "Linguistic Analysis:\n",
            "Hip-Hop top words: [('lil', 6), ('wayne', 6), ('lyrics', 5), ('kanye', 5), ('west', 4)]\n",
            "Pop top words: [('song', 16), ('pop', 13), ('lyrics', 11), ('songs', 11), ('use', 10)]\n",
            "Folk top words: [('song', 25), ('lyrics', 23), ('folk', 23), ('songs', 23), ('music', 22)]\n",
            "Experimental top words: [('lyrics', 15), ('song', 14), ('music', 13), ('sound', 11), ('use', 10)]\n",
            "Rock top words: [('rock', 23), ('lyrics', 18), ('song', 17), ('band', 11), ('use', 11)]\n",
            "International top words: [('use', 15), ('lyrics', 12), ('song', 12), ('like', 9), ('share', 6)]\n",
            "Electronic top words: [('future', 2), ('aware', 1), ('lyrics', 1), ('emotions', 1), ('lot', 1)]\n",
            "Topic 0: ['word', 'use', 'don', 'create', 'song']\n",
            "Topic 1: ['music', 'use', 'songs', 'lyrics', 'song']\n",
            "Topic 2: ['ll', 'music', 'song', 'use', 'lyrics']\n",
            "Topic 3: ['new', 'released', 'band', 'sound', 'album']\n",
            "Topic 4: ['songs', 'folk', 'use', 'lyrics', 'music']\n",
            "User ratings loaded from /content/fma/user_data/ratings.csv\n",
            "\n",
            "Genre Distribution:\n",
            "Folk: 32 tracks\n",
            "Experimental: 20 tracks\n",
            "Rock: 17 tracks\n",
            "International: 15 tracks\n",
            "Pop: 10 tracks\n",
            "Hip-Hop: 5 tracks\n",
            "Electronic: 1 tracks\n",
            "\n",
            "Fold 1\n",
            "Recommendation Epoch 1, Loss: 0.7003\n",
            "Recommendation Epoch 2, Loss: 0.6786\n",
            "Recommendation Epoch 3, Loss: 0.6508\n",
            "Recommendation Epoch 4, Loss: 0.6476\n",
            "Recommendation Epoch 5, Loss: 0.6378\n",
            "Recommendation Metrics: {'precision@k': np.float64(0.42500001192092896), 'recall@k': np.float64(0.8), 'map': np.float64(0.5660181485181486), 'ndcg@k': np.float64(0.5964816176470057), 'diversity': np.float64(0.7857142857142857), 'novelty': 0.625}\n",
            "\n",
            "Fold 2\n",
            "Recommendation Epoch 1, Loss: 0.6993\n",
            "Recommendation Epoch 2, Loss: 0.6931\n",
            "Recommendation Epoch 3, Loss: 0.6704\n",
            "Recommendation Epoch 4, Loss: 0.6566\n",
            "Recommendation Epoch 5, Loss: 0.6537\n",
            "Recommendation Metrics: {'precision@k': np.float64(0.5857143104076385), 'recall@k': np.float64(0.875), 'map': np.float64(0.8735119047619048), 'ndcg@k': np.float64(0.8937924520756649), 'diversity': np.float64(0.7142857142857143), 'novelty': 0.8571428571428572}\n",
            "\n",
            "Fold 3\n",
            "Recommendation Epoch 1, Loss: 0.7020\n",
            "Recommendation Epoch 2, Loss: 0.6824\n",
            "Recommendation Epoch 3, Loss: 0.6604\n",
            "Recommendation Epoch 4, Loss: 0.6443\n",
            "Recommendation Epoch 5, Loss: 0.6387\n",
            "Recommendation Metrics: {'precision@k': np.float64(0.3928571492433548), 'recall@k': np.float64(0.7777777777777778), 'map': np.float64(0.4855429292929293), 'ndcg@k': np.float64(0.5893885997854837), 'diversity': np.float64(0.7142857142857143), 'novelty': 0.8571428571428572}\n",
            "\n",
            "Fold 4\n",
            "Recommendation Epoch 1, Loss: 0.7115\n",
            "Recommendation Epoch 2, Loss: 0.6962\n",
            "Recommendation Epoch 3, Loss: 0.6836\n",
            "Recommendation Epoch 4, Loss: 0.6805\n",
            "Recommendation Epoch 5, Loss: 0.6480\n",
            "Recommendation Metrics: {'precision@k': np.float64(0.4357143044471741), 'recall@k': np.float64(0.7142857142857143), 'map': np.float64(0.6972943722943723), 'ndcg@k': np.float64(0.7586612586854581), 'diversity': np.float64(0.6428571428571428), 'novelty': 0.8571428571428572}\n",
            "\n",
            "Fold 5\n",
            "Recommendation Epoch 1, Loss: 0.7033\n",
            "Recommendation Epoch 2, Loss: 0.6789\n",
            "Recommendation Epoch 3, Loss: 0.6745\n",
            "Recommendation Epoch 4, Loss: 0.6506\n",
            "Recommendation Epoch 5, Loss: 0.6391\n",
            "Recommendation Metrics: {'precision@k': np.float64(0.5357142984867096), 'recall@k': np.float64(0.8125), 'map': np.float64(0.6287337662337662), 'ndcg@k': np.float64(0.6916240515851307), 'diversity': np.float64(0.6428571428571428), 'novelty': 0.8571428571428572}\n",
            "Average Recommendation Metrics: {'precision@k': np.float64(0.47500001490116117), 'recall@k': np.float64(0.7959126984126985), 'map': np.float64(0.6502202242202242), 'ndcg@k': np.float64(0.7059895959557487), 'diversity': np.float64(0.7), 'novelty': np.float64(0.8107142857142857)}\n",
            "Collaborative Filtering Metrics: {'precision@k': np.float64(0.0)}\n",
            "\n",
            "Fold 1\n",
            "Classification Epoch 1, Loss: 1.9264\n",
            "Classification Epoch 2, Loss: 1.8747\n",
            "Classification Epoch 3, Loss: 1.8156\n",
            "Classification Epoch 4, Loss: 1.7122\n",
            "Classification Epoch 5, Loss: 1.6933\n",
            "Classification Epoch 6, Loss: 1.6664\n",
            "Classification Epoch 7, Loss: 1.6701\n",
            "Classification Epoch 8, Loss: 1.6514\n",
            "Classification Epoch 9, Loss: 1.6426\n",
            "Classification Epoch 10, Loss: 1.6089\n",
            "\n",
            "Misclassification Patterns:\n",
            "True: Pop, Predicted: Folk, Count: 5\n",
            "True: Experimental, Predicted: Folk, Count: 5\n",
            "True: International, Predicted: Folk, Count: 4\n",
            "True: Rock, Predicted: Folk, Count: 2\n",
            "True: Electronic, Predicted: Folk, Count: 1\n",
            "True: Hip-Hop, Predicted: Folk, Count: 1\n",
            "Classification Metrics: {'precision': 0.0625, 'recall': 0.25, 'f1': 0.10000000000000002}\n",
            "\n",
            "Fold 2\n",
            "Classification Epoch 1, Loss: 1.9124\n",
            "Classification Epoch 2, Loss: 1.8545\n",
            "Classification Epoch 3, Loss: 1.7929\n",
            "Classification Epoch 4, Loss: 1.7293\n",
            "Classification Epoch 5, Loss: 1.7244\n",
            "Classification Epoch 6, Loss: 1.7261\n",
            "Classification Epoch 7, Loss: 1.7211\n",
            "Classification Epoch 8, Loss: 1.6985\n",
            "Classification Epoch 9, Loss: 1.6916\n",
            "Classification Epoch 10, Loss: 1.6794\n",
            "\n",
            "Misclassification Patterns:\n",
            "True: Rock, Predicted: Folk, Count: 5\n",
            "True: Experimental, Predicted: Folk, Count: 5\n",
            "True: International, Predicted: Folk, Count: 4\n",
            "True: Pop, Predicted: Folk, Count: 2\n",
            "True: Hip-Hop, Predicted: Folk, Count: 1\n",
            "Classification Metrics: {'precision': 0.06805293005671077, 'recall': 0.2608695652173913, 'f1': 0.10794602698650675}\n",
            "\n",
            "Fold 3\n",
            "Classification Epoch 1, Loss: 1.9215\n",
            "Classification Epoch 2, Loss: 1.8750\n",
            "Classification Epoch 3, Loss: 1.8053\n",
            "Classification Epoch 4, Loss: 1.7314\n",
            "Classification Epoch 5, Loss: 1.7146\n",
            "Classification Epoch 6, Loss: 1.7149\n",
            "Classification Epoch 7, Loss: 1.7026\n",
            "Classification Epoch 8, Loss: 1.6971\n",
            "Classification Epoch 9, Loss: 1.6910\n",
            "Classification Epoch 10, Loss: 1.6518\n",
            "\n",
            "Misclassification Patterns:\n",
            "True: Rock, Predicted: Folk, Count: 7\n",
            "True: International, Predicted: Folk, Count: 3\n",
            "True: Experimental, Predicted: Folk, Count: 3\n",
            "True: Hip-Hop, Predicted: Folk, Count: 2\n",
            "True: Pop, Predicted: Folk, Count: 1\n",
            "Classification Metrics: {'precision': 0.09262759924385634, 'recall': 0.30434782608695654, 'f1': 0.14202898550724638}\n",
            "\n",
            "Fold 4\n",
            "Classification Epoch 1, Loss: 1.9327\n",
            "Classification Epoch 2, Loss: 1.8771\n",
            "Classification Epoch 3, Loss: 1.8246\n",
            "Classification Epoch 4, Loss: 1.7869\n",
            "Classification Epoch 5, Loss: 1.7477\n",
            "Classification Epoch 6, Loss: 1.7470\n",
            "Classification Epoch 7, Loss: 1.7493\n",
            "Classification Epoch 8, Loss: 1.7410\n",
            "Classification Epoch 9, Loss: 1.7164\n",
            "Classification Epoch 10, Loss: 1.6990\n",
            "\n",
            "Misclassification Patterns:\n",
            "True: Experimental, Predicted: Folk, Count: 5\n",
            "True: Rock, Predicted: Folk, Count: 5\n",
            "True: Pop, Predicted: Folk, Count: 3\n",
            "True: International, Predicted: Folk, Count: 2\n",
            "Classification Metrics: {'precision': 0.12098298676748583, 'recall': 0.34782608695652173, 'f1': 0.17952314165497896}\n",
            "\n",
            "Fold 5\n",
            "Classification Epoch 1, Loss: 1.9223\n",
            "Classification Epoch 2, Loss: 1.8757\n",
            "Classification Epoch 3, Loss: 1.8093\n",
            "Classification Epoch 4, Loss: 1.7604\n",
            "Classification Epoch 5, Loss: 1.7294\n",
            "Classification Epoch 6, Loss: 1.7310\n",
            "Classification Epoch 7, Loss: 1.7214\n",
            "Classification Epoch 8, Loss: 1.7228\n",
            "Classification Epoch 9, Loss: 1.7052\n",
            "Classification Epoch 10, Loss: 1.6789\n",
            "\n",
            "Misclassification Patterns:\n",
            "True: Experimental, Predicted: Folk, Count: 4\n",
            "True: Rock, Predicted: Folk, Count: 3\n",
            "True: International, Predicted: Folk, Count: 3\n",
            "True: Pop, Predicted: Folk, Count: 3\n",
            "True: Hip-Hop, Predicted: Folk, Count: 1\n",
            "Classification Metrics: {'precision': 0.15311909262759923, 'recall': 0.391304347826087, 'f1': 0.22010869565217392}\n",
            "Average Classification Metrics: {'precision': np.float64(0.09945652173913042), 'recall': np.float64(0.3108695652173913), 'f1': np.float64(0.1499213699601812)}\n",
            "Baseline Metrics: {'SVM': {'precision': np.float64(0.3721502651937435), 'recall': np.float64(0.4749999999999999), 'f1': np.float64(0.4014931454891409)}, 'RandomForest': {'precision': np.float64(0.3328199879830315), 'recall': np.float64(0.3971014492753623), 'f1': np.float64(0.326033720495963)}}\n",
            "\n",
            "Training DistilBERT Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of valid tracks for DistilBERT: 100\n",
            "Genre to index mapping: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Experimental': 3, 'Rock': 4, 'International': 5, 'Electronic': 6}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:DistilBERT training failed: 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT training failed: 'labels'\n",
            "DistilBERT Classifier Metrics: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search Metrics for 'upbeat rock songs': {'precision@k': 0.7, 'recall@k': 0.4117647058823529, 'map': np.float64(0.9571428571428571), 'ndcg@k': np.float64(0.9878316351280036), 'diversity': 0.5714285714285714, 'novelty': 0.0}\n",
            "\n",
            "Search Metrics for 'smooth jazz vibes': {'precision@k': 0.0, 'recall@k': 0.0, 'map': 0.0, 'ndcg@k': 0.0, 'diversity': 0.7142857142857143, 'novelty': 0.1}\n",
            "\n",
            "Search Metrics for 'energetic pop tracks': {'precision@k': 0.5, 'recall@k': 0.5, 'map': np.float64(1.0), 'ndcg@k': np.float64(1.0), 'diversity': 0.5714285714285714, 'novelty': 0.1}\n",
            "\n",
            "Search Metrics for 'classical orchestral': {'precision@k': 0.0, 'recall@k': 0.0, 'map': 0.0, 'ndcg@k': 0.0, 'diversity': 0.5714285714285714, 'novelty': 0.0}\n",
            "mfcc_1: Mean contribution = 0.0066\n",
            "mfcc_2: Mean contribution = 0.0097\n",
            "mfcc_3: Mean contribution = 0.0057\n",
            "mfcc_4: Mean contribution = 0.0021\n",
            "mfcc_5: Mean contribution = 0.0026\n",
            "mfcc_6: Mean contribution = 0.0075\n",
            "mfcc_7: Mean contribution = 0.0018\n",
            "mfcc_8: Mean contribution = 0.0116\n",
            "mfcc_9: Mean contribution = 0.0062\n",
            "mfcc_10: Mean contribution = 0.0065\n",
            "mfcc_11: Mean contribution = 0.0055\n",
            "mfcc_12: Mean contribution = 0.0064\n",
            "chroma_1: Mean contribution = 0.0063\n",
            "chroma_2: Mean contribution = 0.0059\n",
            "chroma_3: Mean contribution = 0.0027\n",
            "chroma_4: Mean contribution = 0.0088\n",
            "chroma_5: Mean contribution = 0.0033\n",
            "chroma_6: Mean contribution = 0.0033\n",
            "chroma_7: Mean contribution = 0.0009\n",
            "chroma_8: Mean contribution = 0.0068\n",
            "chroma_9: Mean contribution = 0.0054\n",
            "chroma_10: Mean contribution = 0.0013\n",
            "chroma_11: Mean contribution = 0.0067\n",
            "chroma_12: Mean contribution = 0.0054\n",
            "spectral_centroid: Mean contribution = 0.0104\n",
            "tempo: Mean contribution = 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".E\n",
            "======================================================================\n",
            "ERROR: test_music_search (__main__.TestMusicRecommender.test_music_search)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2034136229.py\", line 930, in test_music_search\n",
            "    df_features = pd.DataFrame(self.audio_features, columns=['track_id'] + [f'feat_{i}' for i in range(26)])\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 827, in __init__\n",
            "    mgr = ndarray_to_mgr(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\", line 336, in ndarray_to_mgr\n",
            "    _check_values_indices_shape_match(values, index, columns)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\", line 420, in _check_values_indices_shape_match\n",
            "    raise ValueError(f\"Shape of passed values is {passed}, indices imply {implied}\")\n",
            "ValueError: Shape of passed values is (2, 26), indices imply (2, 27)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 16.765s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}