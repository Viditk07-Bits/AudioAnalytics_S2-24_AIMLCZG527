{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viditk07-Bits/AudioAnalytics_S2-24_AIMLCZG527/blob/main/AA_Assignment2_V1.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Analysis Assignment 2\n",
        "# GROUP 8 - Members\n",
        "\n",
        "1. VIDIT KUMAR KALE - (2023ac05613)\n",
        "2. MAYANK GROVER - (2023ac05486)\n",
        "3. AMIT KUMAR ANAND - (2023ac05670)"
      ],
      "metadata": {
        "id": "l7PIgYsUKkRE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EN9uWs6-RQ"
      },
      "source": [
        "# Music Information Retrieval System\n",
        "\n",
        "## Assignment Objective\n",
        "This assignment implements a comprehensive Music Information Retrieval (MIR) system using Large Language Models (LLMs) and deep learning techniques. It includes music recommendation, genre classification, and semantic search applications, combining audio analysis with natural language processing.\n",
        "\n",
        "## Dataset Setup\n",
        "Using the Free Music Archive (FMA) dataset with audio files, metadata, and synthetic user data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor, Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, average_precision_score, ndcg_score\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import hashlib\n",
        "import subprocess\n",
        "import logging\n",
        "import random\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from datasets import Dataset\n",
        "import uuid\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/fma/data\"\n",
        "AUDIO_PATH = os.path.join(DATA_PATH, \"audio_files\")\n",
        "METADATA_PATH = os.path.join(DATA_PATH, \"metadata/tracks.csv\")\n",
        "ARTISTS_PATH = os.path.join(DATA_PATH, \"metadata/artists.csv\")\n",
        "GENRES_PATH = os.path.join(DATA_PATH, \"metadata/genres.csv\")\n",
        "LYRICS_PATH = os.path.join(DATA_PATH, \"lyrics\")\n",
        "USER_DATA_PATH = os.path.join(DATA_PATH, \"user_data/ratings.csv\")\n",
        "TAGS_PATH = os.path.join(DATA_PATH, \"descriptions/tags.csv\")\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "TEMP_DIR = \"/content/fma\"\n",
        "NUM_EPOCHS_REC = 5\n",
        "NUM_EPOCHS_CLS = 10\n",
        "BATCH_SIZE = 32\n",
        "MAX_TRACKS = 500\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TIMEOUT_SECONDS = 300  # 5 minutes timeout for metadata processing\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Genre-specific keyword pools for tag generation\n",
        "GENRE_KEYWORDS = {\n",
        "    'Rock': ['guitar', 'energetic', 'rebellious', 'grunge', 'classic', 'indie', 'punk'],\n",
        "    'Pop': ['catchy', 'upbeat', 'dance', 'melodic', 'mainstream', 'vibrant'],\n",
        "    'Jazz': ['smooth', 'improvisation', 'saxophone', 'bluesy', 'swing', 'soulful'],\n",
        "    'Classical': ['orchestral', 'symphonic', 'piano', 'elegant', 'baroque', 'romantic'],\n",
        "    'Hip-Hop': ['beats', 'rap', 'urban', 'flow', 'rhythmic', 'street'],\n",
        "    'Electronic': ['synth', 'techno', 'ambient', 'dance', 'futuristic', 'edm'],\n",
        "    'Folk': ['acoustic', 'storytelling', 'rustic', 'traditional', 'gentle'],\n",
        "    'Blues': ['soulful', 'guitar', 'melancholy', 'raw', 'emotional'],\n",
        "    'Country': ['twang', 'heartfelt', 'rural', 'banjo', 'storytelling'],\n",
        "    'Reggae': ['rasta', 'chill', 'rhythmic', 'island', 'vibes'],\n",
        "    'International': ['cultural', 'exotic', 'world', 'fusion', 'traditional'],\n",
        "    'Instrumental': ['ambient', 'melodic', 'orchestral', 'calm', 'instrumental'],\n",
        "    'Experimental': ['avant-garde', 'abstract', 'innovative', 'unconventional', 'soundscape']\n",
        "}\n",
        "\n",
        "def move_mp3_file(args):\n",
        "    \"\"\"Helper function to move MP3 files in parallel.\"\"\"\n",
        "    mp3_file, target_dir, existing_mp3s = args\n",
        "    target = os.path.join(target_dir, mp3_file.name)\n",
        "    if mp3_file.name not in existing_mp3s:\n",
        "        os.rename(mp3_file, target)\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def generate_lyrics(args):\n",
        "    \"\"\"Helper function to generate and write lyrics in parallel.\"\"\"\n",
        "    track_id, genre, lyrics_path, model, tokenizer = args\n",
        "    if model and tokenizer:\n",
        "        prompt = f\"Write lyrics for a {genre} song titled Track_{track_id}:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=50, truncation=True).to(DEVICE)\n",
        "        try:\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=100,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                temperature=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            lyrics = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Lyrics generation failed for track %s: %s\", track_id, str(e))\n",
        "            lyrics = f\"Synthetic lyrics for track {track_id} in {genre}\"\n",
        "    else:\n",
        "        lyrics = f\"Synthetic lyrics for track {track_id} in {genre}\"\n",
        "    with open(os.path.join(lyrics_path, f\"{track_id}.txt\"), 'w', encoding='utf-8') as f:\n",
        "        f.write(lyrics)\n",
        "    return track_id\n",
        "\n",
        "def setup_fma_dataset():\n",
        "    \"\"\"Task A1, A3, A5: Download and set up FMA dataset with tracks, genres, ratings, and synthetic lyrics.\"\"\"\n",
        "    print(\"=== Dataset Setup ===\")\n",
        "    logging.info(\"Starting dataset setup...\")\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "    os.makedirs(DATA_PATH, exist_ok=True)\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n",
        "    os.makedirs(LYRICS_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(USER_DATA_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(TAGS_PATH), exist_ok=True)\n",
        "\n",
        "    # Initialize GPT-2 for lyrics generation\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(DEVICE)\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Failed to load DistilGPT-2: %s. Using basic synthetic lyrics.\", str(e))\n",
        "        print(f\"Failed to load DistilGPT-2: {str(e)}. Using basic synthetic lyrics.\")\n",
        "        tokenizer = None\n",
        "        model = None\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        os.chdir(TEMP_DIR)\n",
        "        logging.info(\"Changed to TEMP_DIR: %s\", TEMP_DIR)\n",
        "        if not os.path.exists(os.path.join(TEMP_DIR, \"fma\")):\n",
        "            logging.info(\"Cloning FMA repository...\")\n",
        "            print(\"Cloning FMA repository...\")\n",
        "            subprocess.run([\"git\", \"clone\", \"https://github.com/mdeff/fma.git\"], check=True, capture_output=True, text=True)\n",
        "        os.chdir(os.path.join(TEMP_DIR, \"fma\"))\n",
        "        logging.info(\"Changed to FMA directory\")\n",
        "\n",
        "        # Define paths for FMA dataset files\n",
        "        fma_small_zip = \"fma_small.zip\"\n",
        "        fma_metadata_zip = \"fma_metadata.zip\"\n",
        "\n",
        "        # Check if files already exist\n",
        "        if os.path.exists(fma_small_zip):\n",
        "            logging.info(\"fma_small.zip already exists, skipping download.\")\n",
        "            print(\"fma_small.zip already exists, skipping download.\")\n",
        "        else:\n",
        "            logging.info(\"Downloading fma_small.zip...\")\n",
        "            print(\"Downloading fma_small.zip...\")\n",
        "            subprocess.run([\"wget\", \"-O\", fma_small_zip, \"https://os.unil.cloud.switch.ch/fma/fma_small.zip\"], check=True, capture_output=True, text=True)\n",
        "\n",
        "        if os.path.exists(fma_metadata_zip):\n",
        "            logging.info(\"fma_metadata.zip already exists, skipping download.\")\n",
        "            print(\"fma_metadata.zip already exists, skipping download.\")\n",
        "        else:\n",
        "            logging.info(\"Downloading fma_metadata.zip...\")\n",
        "            print(\"Downloading fma_metadata.zip...\")\n",
        "            subprocess.run([\"wget\", \"-O\", fma_metadata_zip, \"https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\"], check=True, capture_output=True, text=True)\n",
        "\n",
        "        # Verify checksums\n",
        "        def sha1_checksum(file_path):\n",
        "            sha1 = hashlib.sha1()\n",
        "            with open(file_path, 'rb') as f:\n",
        "                while chunk := f.read(8192):\n",
        "                    sha1.update(chunk)\n",
        "            return sha1.hexdigest()\n",
        "\n",
        "        logging.info(\"Verifying checksums...\")\n",
        "        print(\"Verifying checksums...\")\n",
        "        if sha1_checksum(fma_small_zip) != \"ade154f733639d52e35e32f5593efe5be76c6d70\":\n",
        "            logging.warning(\"fma_small.zip checksum failed. Proceeding anyway.\")\n",
        "        if sha1_checksum(fma_metadata_zip) != \"f0df49ffe5f2a6008d7dc83c6915b31835dfe733\":\n",
        "            logging.warning(\"fma_metadata.zip checksum failed. Proceeding anyway.\")\n",
        "\n",
        "        # Unzip datasets\n",
        "        if not os.path.exists(os.path.join(DATA_PATH, \"fma_small\")):\n",
        "            logging.info(\"Unzipping fma_small.zip...\")\n",
        "            print(\"Unzipping fma_small.zip...\")\n",
        "            try:\n",
        "                subprocess.run([\"unzip\", \"-q\", fma_small_zip, \"-d\", DATA_PATH], check=True, capture_output=True, text=True)\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                logging.error(\"Unzip fma_small.zip failed: %s\", e.stderr)\n",
        "                raise\n",
        "        if not os.path.exists(os.path.join(DATA_PATH, \"fma_metadata\")):\n",
        "            logging.info(\"Unzipping fma_metadata.zip...\")\n",
        "            print(\"Unzipping fma_metadata.zip...\")\n",
        "            try:\n",
        "                subprocess.run([\"unzip\", \"-q\", fma_metadata_zip, \"-d\", DATA_PATH], check=True, capture_output=True, text=True)\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                logging.error(\"Unzip fma_metadata.zip failed: %s\", e.stderr)\n",
        "                raise\n",
        "\n",
        "        # Check if audio files are already in place\n",
        "        existing_mp3s = set(f.name for f in Path(AUDIO_PATH).glob(\"*.mp3\"))\n",
        "        if len(existing_mp3s) >= 8000:\n",
        "            logging.info(\"Sufficient MP3 files found in %s, skipping move.\", AUDIO_PATH)\n",
        "            print(f\"Sufficient MP3 files found in {AUDIO_PATH}, skipping move.\")\n",
        "            mp3_count = len(existing_mp3s)\n",
        "        else:\n",
        "            # Move MP3 files in parallel\n",
        "            logging.info(\"Checking and moving MP3 files...\")\n",
        "            print(\"Checking and moving MP3 files...\")\n",
        "            mp3_files = list(Path(DATA_PATH).rglob(\"*.mp3\"))\n",
        "            logging.info(\"MP3 files found: %d\", len(mp3_files))\n",
        "            with Pool(processes=4) as pool:\n",
        "                args = [(f, AUDIO_PATH, existing_mp3s) for f in mp3_files]\n",
        "                mp3_count = sum(tqdm(pool.imap(move_mp3_file, args), total=len(mp3_files), desc=\"Moving MP3 files\"))\n",
        "            logging.info(\"Moved %d MP3 files.\", mp3_count)\n",
        "            print(f\"Moved {mp3_count} MP3 files.\")\n",
        "            logging.info(\"Audio files after move: %s\", os.listdir(AUDIO_PATH)[:10])\n",
        "\n",
        "        # Process metadata without usecols to handle multi-index\n",
        "        logging.info(\"Processing metadata...\")\n",
        "        print(\"Processing metadata...\")\n",
        "        tracks = pd.read_csv(\n",
        "            os.path.join(DATA_PATH, \"fma_metadata\", \"tracks.csv\"),\n",
        "            index_col=0,\n",
        "            header=[0, 1]\n",
        "        )\n",
        "        # Select only required columns after loading\n",
        "        required_columns = [('track', 'title'), ('track', 'genre_top'), ('artist', 'name')]\n",
        "        missing_columns = [col for col in required_columns if col not in tracks.columns]\n",
        "        if missing_columns:\n",
        "            logging.error(\"Missing columns in tracks.csv: %s\", missing_columns)\n",
        "            raise ValueError(f\"Missing columns in tracks.csv: {missing_columns}\")\n",
        "        tracks = tracks[required_columns].copy()\n",
        "        tracks.columns = ['title', 'genre_top', 'artist_name']  # Flatten column names\n",
        "        tracks = tracks.reset_index().rename(columns={'track_id': 'track_id'})\n",
        "\n",
        "        genres = pd.read_csv(\n",
        "            os.path.join(DATA_PATH, \"fma_metadata\", \"genres.csv\"),\n",
        "            usecols=['genre_id', 'title']\n",
        "        )\n",
        "\n",
        "        df_artists = tracks[['track_id', 'artist_name']].rename(columns={'track_id': 'artist_id'})\n",
        "        df_artists['artist_id'] = df_artists['artist_id'].astype(str).str.zfill(6)\n",
        "        df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "        logging.info(\"Created artists.csv\")\n",
        "\n",
        "        df_genres = genres[['genre_id', 'title']].rename(columns={'title': 'genre_name'})\n",
        "        df_genres.to_csv(GENRES_PATH, index=False)\n",
        "        logging.info(\"Created genres.csv\")\n",
        "\n",
        "        # Filter tracks to match audio files\n",
        "        audio_ids = {f.stem for f in Path(AUDIO_PATH).glob(\"*.mp3\")}\n",
        "        df_tracks = tracks[['track_id', 'title', 'genre_top']].copy()\n",
        "        df_tracks['track_id'] = df_tracks['track_id'].astype(str).str.zfill(6)\n",
        "        df_tracks = df_tracks[df_tracks['track_id'].isin(audio_ids)]\n",
        "        df_tracks.loc[:, 'artist_id'] = df_tracks['track_id']\n",
        "        df_tracks.loc[:, 'genre_id'] = df_tracks['genre_top'].map(df_genres.set_index('genre_name')['genre_id'])\n",
        "        df_tracks = df_tracks.dropna()\n",
        "\n",
        "        # Sample tracks efficiently\n",
        "        if len(df_tracks) > MAX_TRACKS:\n",
        "            df_tracks = df_tracks.sample(n=MAX_TRACKS, random_state=42)\n",
        "        df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "        logging.info(\"Created tracks.csv\")\n",
        "\n",
        "        # Check timeout\n",
        "        if time.time() - start_time > TIMEOUT_SECONDS:\n",
        "            logging.warning(\"Metadata processing timeout. Falling back to synthetic dataset.\")\n",
        "            print(\"Metadata processing timeout. Falling back to synthetic dataset.\")\n",
        "            return create_synthetic_dataset()\n",
        "\n",
        "        # Generate synthetic ratings\n",
        "        genre_rating_bias = {\n",
        "            'Hip-Hop': 0.15, 'Pop': 0.2, 'Folk': -0.05, 'Experimental': -0.1, 'Rock': 0.1,\n",
        "            'International': 0.05, 'Electronic': 0.1, 'Instrumental': -0.2\n",
        "        }\n",
        "        ratings = []\n",
        "        for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top']):\n",
        "            for user_id in [f\"user_{i+1}\" for i in range(10)]:\n",
        "                rating = min(max(np.random.normal(3.5 + genre_rating_bias.get(genre, 0), 0.5), 1), 5)\n",
        "                ratings.append({'user_id': user_id, 'track_id': track_id, 'rating': rating})\n",
        "        ratings = pd.DataFrame(ratings)\n",
        "        ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "        logging.info(\"Created ratings.csv\")\n",
        "\n",
        "        # Generate synthetic tags\n",
        "        tags = []\n",
        "        for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top']):\n",
        "            num_tags = random.randint(2, 5)\n",
        "            track_tags = random.sample(GENRE_KEYWORDS.get(genre, ['generic']), min(num_tags, len(GENRE_KEYWORDS.get(genre, ['generic']))))\n",
        "            tags.extend([{'track_id': track_id, 'tag': tag} for tag in track_tags])\n",
        "        pd.DataFrame(tags).to_csv(TAGS_PATH, index=False)\n",
        "        logging.info(\"Created tags.csv\")\n",
        "\n",
        "        # Generate genre-specific lyrics in parallel\n",
        "        lyrics_args = [(track_id, genre, LYRICS_PATH, model, tokenizer)\n",
        "                       for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top'])]\n",
        "        with Pool(processes=4) as pool:\n",
        "            list(tqdm(pool.imap(generate_lyrics, lyrics_args), total=len(lyrics_args), desc=\"Generating lyrics\"))\n",
        "\n",
        "        print(\"Dataset setup completed successfully.\")\n",
        "        print(f\"Tracks shape: {df_tracks.shape}, Genres shape: {df_genres.shape}, Ratings shape: {ratings.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Dataset setup failed: %s. Creating synthetic dataset.\", str(e))\n",
        "        print(f\"Dataset setup failed: {str(e)}. Creating synthetic dataset.\")\n",
        "        create_synthetic_dataset()\n",
        "\n",
        "def create_synthetic_dataset():\n",
        "    \"\"\"Task A1, A3, A5: Create synthetic dataset with realistic lyrics, ratings, and tags.\"\"\"\n",
        "    logging.info(\"Creating synthetic dataset with enhanced generation...\")\n",
        "    print(\"Creating synthetic dataset with enhanced generation...\")\n",
        "\n",
        "    # Initialize GPT-2 for lyrics generation\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(DEVICE)\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to load DistilGPT-2: %s. Using basic synthetic data.\", str(e))\n",
        "        print(f\"Failed to load DistilGPT-2: {str(e)}. Using basic synthetic data.\")\n",
        "        tokenizer = None\n",
        "        model = None\n",
        "\n",
        "    # Create tracks DataFrame\n",
        "    df_tracks = pd.DataFrame({\n",
        "        'track_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "        'title': [f\"Track_{i}\" for i in range(1, MAX_TRACKS + 1)],\n",
        "        'artist_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "        'genre_id': [random.randint(1, 13) for _ in range(MAX_TRACKS)],\n",
        "        'genre_top': [random.choice(list(GENRE_KEYWORDS.keys())) for _ in range(MAX_TRACKS)]\n",
        "    })\n",
        "    df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "\n",
        "    # Create artists DataFrame\n",
        "    df_artists = pd.DataFrame({\n",
        "        'artist_id': df_tracks['artist_id'],\n",
        "        'artist_name': [f\"Artist_{i}\" for i in range(1, MAX_TRACKS + 1)]\n",
        "    })\n",
        "    df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "\n",
        "    # Create genres DataFrame\n",
        "    df_genres = pd.DataFrame({\n",
        "        'genre_id': range(1, 14),\n",
        "        'genre_name': list(GENRE_KEYWORDS.keys())\n",
        "    })\n",
        "    df_genres.to_csv(GENRES_PATH, index=False)\n",
        "\n",
        "    # Generate realistic ratings with genre-based bias\n",
        "    genre_rating_bias = {\n",
        "        'Pop': 0.2, 'Rock': 0.1, 'Jazz': -0.1, 'Classical': -0.2, 'Hip-Hop': 0.15,\n",
        "        'Electronic': 0.1, 'Folk': -0.05, 'Blues': -0.1, 'Country': -0.05, 'Reggae': 0.05,\n",
        "        'International': 0.05, 'Instrumental': -0.2, 'Experimental': -0.1\n",
        "    }\n",
        "    ratings = []\n",
        "    for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top']):\n",
        "        for user_id in [f\"user_{i+1}\" for i in range(10)]:\n",
        "            rating = min(max(np.random.normal(3.5 + genre_rating_bias.get(genre, 0), 0.5), 1), 5)\n",
        "            ratings.append({'user_id': user_id, 'track_id': track_id, 'rating': rating})\n",
        "    ratings = pd.DataFrame(ratings)\n",
        "    ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "\n",
        "    # Generate varied tags\n",
        "    tags = []\n",
        "    for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top']):\n",
        "        num_tags = random.randint(2, 5)\n",
        "        track_tags = random.sample(GENRE_KEYWORDS.get(genre, ['generic']), min(num_tags, len(GENRE_KEYWORDS.get(genre, ['generic']))))\n",
        "        tags.extend([{'track_id': track_id, 'tag': tag} for tag in track_tags])\n",
        "    pd.DataFrame(tags).to_csv(TAGS_PATH, index=False)\n",
        "\n",
        "    # Generate realistic lyrics\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    os.makedirs(LYRICS_PATH, exist_ok=True)\n",
        "    lyrics_args = [(track_id, genre, LYRICS_PATH, model, tokenizer)\n",
        "                   for track_id, genre in zip(df_tracks['track_id'], df_tracks['genre_top'])]\n",
        "    with Pool(processes=4) as pool:\n",
        "        list(tqdm(pool.imap(generate_lyrics, lyrics_args), total=len(lyrics_args), desc=\"Generating synthetic lyrics\"))\n",
        "\n",
        "    print(f\"Synthetic dataset created: Tracks shape: {df_tracks.shape}, Ratings shape: {ratings.shape}, Tags: {len(tags)}\")\n",
        "    logging.info(\"Synthetic dataset created: Tracks shape: %s, Ratings: %s, Tags: %d\", df_tracks.shape, ratings.shape, len(tags))\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    \"\"\"Task A1.1, A2: Extract audio features (MFCCs, chroma, spectral centroid, tempo) using Librosa.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(audio_path):\n",
        "            logging.warning(\"Audio file %s does not exist.\", audio_path)\n",
        "            return np.zeros(26)\n",
        "        if os.path.getsize(audio_path) < 100:\n",
        "            logging.warning(\"Skipping %s: File too small or empty.\", audio_path)\n",
        "            return np.zeros(26)\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=22050)\n",
        "        except Exception as load_e:\n",
        "            logging.warning(\"Librosa load failed for %s: %s\", audio_path, str(load_e))\n",
        "            return np.zeros(26)\n",
        "        if len(y) == 0:\n",
        "            logging.warning(\"Skipping %s: Empty audio data.\", audio_path)\n",
        "            return np.zeros(26)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        tempo = librosa.feature.tempo(y=y, sr=sr)\n",
        "        tempo = float(tempo[0]) if isinstance(tempo, np.ndarray) else float(tempo)\n",
        "        features = np.concatenate([\n",
        "            np.mean(mfccs, axis=1),\n",
        "            np.mean(chroma, axis=1),\n",
        "            np.mean(spectral_centroid, axis=1),\n",
        "            [tempo]\n",
        "        ])\n",
        "        logging.info(\"Extracted features for %s: %s\", audio_path, features[:5])\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Error processing %s: %s\", audio_path, str(e))\n",
        "        return np.zeros(26)\n",
        "\n",
        "def load_fma_data(audio_path, metadata_path, artists_path, genres_path, lyrics_path, tags_path):\n",
        "    \"\"\"Task A1, A3, A5: Load FMA dataset, process audio, metadata, lyrics, and tags.\"\"\"\n",
        "    logging.info(\"Loading FMA data...\")\n",
        "    print(\"Loading FMA data...\")\n",
        "    if not os.path.exists(metadata_path):\n",
        "        logging.error(\"Metadata not found. Creating synthetic dataset.\")\n",
        "        create_synthetic_dataset()\n",
        "\n",
        "    try:\n",
        "        df_tracks = pd.read_csv(metadata_path)\n",
        "        df_metadata = df_tracks[['track_id', 'title', 'artist_id', 'genre_id']].dropna()\n",
        "        df_artists = pd.read_csv(artists_path)[['artist_id', 'artist_name']]\n",
        "        df_genres = pd.read_csv(genres_path)[['genre_id', 'genre_name']]\n",
        "        df_metadata = pd.merge(df_metadata, df_artists, on='artist_id', how='left')\n",
        "        df_metadata = pd.merge(df_metadata, df_genres, on='genre_id', how='left')\n",
        "        df_metadata = df_metadata[['track_id', 'artist_name', 'title', 'genre_name']].dropna()\n",
        "        df_metadata.columns = ['track_id', 'artist_name', 'title', 'genre']\n",
        "        df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "        logging.info(\"Metadata shape: %s\", df_metadata.shape)\n",
        "        print(f\"Metadata shape: {df_metadata.shape}\")\n",
        "        print(f\"Genre distribution:\\n{df_metadata['genre'].value_counts()}\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading metadata: %s. Using synthetic metadata.\", str(e))\n",
        "        print(f\"Error loading metadata: {str(e)}. Using synthetic metadata.\")\n",
        "        df_metadata = pd.DataFrame({\n",
        "            'track_id': [str(i).zfill(6) for i in range(1, MAX_TRACKS + 1)],\n",
        "            'artist_name': [f\"Artist_{i}\" for i in range(1, MAX_TRACKS + 1)],\n",
        "            'title': [f\"Track_{i}\" for i in range(1, MAX_TRACKS + 1)],\n",
        "            'genre': [random.choice(list(GENRE_KEYWORDS.keys())) for _ in range(MAX_TRACKS)]\n",
        "        })\n",
        "\n",
        "    features = []\n",
        "    audio_files = list(Path(audio_path).glob(\"*.mp3\"))\n",
        "    valid_track_ids = set(df_metadata['track_id'].astype(str).str.zfill(6).tolist())\n",
        "    logging.info(\"Found %d audio files.\", len(audio_files))\n",
        "    print(f\"Found {len(audio_files)} audio files.\")\n",
        "    valid_audio_count = 0\n",
        "    valid_audio_files = []\n",
        "    for audio_file in tqdm(audio_files, desc=\"Processing audio files\"):\n",
        "        track_id = audio_file.stem\n",
        "        if track_id in valid_track_ids:\n",
        "            audio_features = extract_audio_features(audio_file)\n",
        "            if not np.all(audio_features == 0):\n",
        "                valid_audio_count += 1\n",
        "                valid_audio_files.append(str(audio_file))\n",
        "            features.append([track_id] + audio_features.tolist())\n",
        "    logging.info(\"Processed %d valid audio files.\", valid_audio_count)\n",
        "    print(f\"Processed {valid_audio_count} valid audio files.\")\n",
        "    if valid_audio_files:\n",
        "        print(f\"Valid audio files (first 5): {valid_audio_files[:5]}\")\n",
        "        logging.info(\"Valid audio files (first 5): %s\", valid_audio_files[:5])\n",
        "\n",
        "    feature_columns = ['track_id'] + [f'mfcc_{i+1}' for i in range(13)] + [f'chroma_{i+1}' for i in range(12)] + ['spectral_centroid', 'tempo']\n",
        "    df_features = pd.DataFrame(features, columns=feature_columns).dropna()\n",
        "    if df_features.empty or valid_audio_count == 0:\n",
        "        logging.warning(\"No valid audio features. Using synthetic features.\")\n",
        "        print(\"No valid audio features. Using synthetic features.\")\n",
        "        df_features = pd.DataFrame({\n",
        "            'track_id': df_metadata['track_id'],\n",
        "            **{col: [0.0] * len(df_metadata) for col in feature_columns[1:]}\n",
        "        })\n",
        "    else:\n",
        "        df_features = df_features[df_features['track_id'].isin(valid_track_ids)]\n",
        "        logging.info(\"Features shape: %s\", df_features.shape)\n",
        "        print(f\"Features shape: {df_features.shape}\")\n",
        "\n",
        "    lyrics_dict = {}\n",
        "    for lyric_file in Path(lyrics_path).glob(\"*.txt\"):\n",
        "        track_id = lyric_file.stem\n",
        "        if track_id in df_metadata['track_id'].values:\n",
        "            try:\n",
        "                with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                    lyrics_dict[track_id] = f.read().strip() or 'music'\n",
        "            except Exception as e:\n",
        "                logging.warning(\"Error reading lyrics %s: %s\", track_id, str(e))\n",
        "                lyrics_dict[track_id] = 'music'\n",
        "\n",
        "    if tags_path and os.path.exists(tags_path):\n",
        "        try:\n",
        "            df_tags = pd.read_csv(tags_path)\n",
        "            for _, row in df_tags.iterrows():\n",
        "                track_id = str(row['track_id']).zfill(6)\n",
        "                if track_id in df_metadata['track_id'].values:\n",
        "                    tag = str(row['tag'])\n",
        "                    lyrics_dict[track_id] = lyrics_dict.get(track_id, '') + \" \" + tag\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error loading tags: %s\", str(e))\n",
        "\n",
        "    if not lyrics_dict:\n",
        "        logging.warning(\"No lyrics found. Using synthetic lyrics.\")\n",
        "        print(\"No lyrics found. Using synthetic lyrics.\")\n",
        "        for track_id in df_metadata['track_id']:\n",
        "            lyrics_dict[track_id] = f\"Synthetic lyrics for {track_id} in {df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]}\"\n",
        "\n",
        "    logging.info(\"Lyrics dict size: %d\", len(lyrics_dict))\n",
        "    print(f\"Lyrics dict size: {len(lyrics_dict)}\")\n",
        "    return df_metadata, df_features, lyrics_dict\n",
        "\n",
        "def generate_text_embeddings(lyrics_dict):\n",
        "    \"\"\"Task A1.2, A3: Generate semantic embeddings for lyrics using Sentence-Transformers.\"\"\"\n",
        "    logging.info(\"Generating text embeddings...\")\n",
        "    print(\"Generating text embeddings...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    embeddings = {}\n",
        "    for track_id, text in tqdm(lyrics_dict.items(), desc=\"Generating text embeddings\"):\n",
        "        embeddings[track_id] = model.encode(text, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    logging.info(\"Text embeddings generated for %d tracks.\", len(embeddings))\n",
        "    print(f\"Text embeddings generated for {len(embeddings)} tracks.\")\n",
        "    return embeddings\n",
        "\n",
        "def analyze_linguistic_patterns(df_metadata, lyrics_dict):\n",
        "    \"\"\"Task A1.2: Analyze linguistic patterns across genres with preprocessing.\"\"\"\n",
        "    print(\"\\nLinguistic Analysis: Top 5 words per genre\")\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    genre_words = {g: [] for g in df_metadata['genre'].unique()}\n",
        "    for track_id, text in lyrics_dict.items():\n",
        "        genre = df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "        genre_words[genre].extend(tokens)\n",
        "    for genre, words in genre_words.items():\n",
        "        top_words = Counter(words).most_common(5)\n",
        "        print(f\"{genre} top words: {top_words}\")\n",
        "        logging.info(\"%s top words: %s\", genre, top_words)\n",
        "\n",
        "class HybridRecommender(nn.Module):\n",
        "    \"\"\"Task A5: Content-based recommender combining audio and text features.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        return torch.sigmoid(self.fc(combined))\n",
        "\n",
        "def train_recommender(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Task A5: Train the content-based recommender.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, ratings in train_loader:\n",
        "        audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, ratings.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_recommender(model, test_loader, df_metadata, test_indices, genres, k=10):\n",
        "    \"\"\"Task C5: Evaluate recommender with Precision@K, MAP, NDCG, and diversity.\"\"\"\n",
        "    model.eval()\n",
        "    precisions, maps, ndcgs, diversities = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, ratings in test_loader:\n",
        "            audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            binary_ratings = (ratings > 0.5).float()\n",
        "            top_k = torch.topk(outputs, k=min(k, outputs.size(0)), dim=0).indices.flatten()\n",
        "            relevant = binary_ratings[top_k]\n",
        "            precision = relevant.mean().item()\n",
        "            precisions.append(precision)\n",
        "            if binary_ratings.sum() > 0:\n",
        "                map_score = average_precision_score(binary_ratings.cpu().numpy(), outputs.cpu().numpy())\n",
        "                ndcg = ndcg_score(binary_ratings.cpu().numpy().reshape(1, -1), outputs.cpu().numpy().reshape(1, -1), k=k)\n",
        "            else:\n",
        "                map_score, ndcg = 0.0, 0.0\n",
        "            maps.append(map_score)\n",
        "            ndcgs.append(ndcg)\n",
        "            top_k_ids = df_metadata.iloc[test_indices].iloc[top_k.cpu()]['track_id']\n",
        "            top_k_genres = df_metadata[df_metadata['track_id'].isin(top_k_ids)]['genre']\n",
        "            diversity = len(set(top_k_genres)) / len(genres) if len(genres) > 0 else 1.0\n",
        "            diversities.append(diversity)\n",
        "    return {\n",
        "        'precision@k': np.mean(precisions) if precisions else 0.0,\n",
        "        'map': np.mean(maps) if maps else 0.0,\n",
        "        'ndcg@k': np.mean(ndcgs) if ndcgs else 0.0,\n",
        "        'diversity': np.mean(diversities) if diversities else 1.0\n",
        "    }\n",
        "\n",
        "def collaborative_filtering(user_data, df_metadata, k=10):\n",
        "    \"\"\"Task A5: Collaborative filtering using NMF with denser ratings.\"\"\"\n",
        "    print(\"\\nTraining Collaborative Filtering Model...\")\n",
        "    valid_track_ids = set(df_metadata['track_id'].astype(str).str.zfill(6))\n",
        "    user_data = user_data[user_data['track_id'].isin(valid_track_ids)]\n",
        "\n",
        "    if user_data.empty:\n",
        "        logging.error(\"No valid user data after filtering. Returning zero metrics.\")\n",
        "        print(\"No valid user data after filtering. Returning zero metrics.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    users = [f\"user_{i+1}\" for i in range(10)]\n",
        "    tracks = list(valid_track_ids)\n",
        "    genre_rating_bias = {\n",
        "        'Hip-Hop': 0.15, 'Pop': 0.2, 'Folk': -0.05, 'Experimental': -0.1,\n",
        "        'Rock': 0.1, 'International': 0.05, 'Electronic': 0.1, 'Instrumental': -0.2\n",
        "    }\n",
        "    denser_ratings = []\n",
        "    for user in users:\n",
        "        for track in tracks:\n",
        "            genre = df_metadata[df_metadata['track_id'] == track]['genre'].iloc[0]\n",
        "            rating = min(max(np.random.normal(3.5 + genre_rating_bias.get(genre, 0), 0.5), 1), 5)\n",
        "            denser_ratings.append({'user_id': user, 'track_id': track, 'rating': rating})\n",
        "    user_data = pd.DataFrame(denser_ratings)\n",
        "\n",
        "    train_data, test_data = train_test_split(user_data, test_size=0.2, random_state=42, stratify=user_data['track_id'])\n",
        "    logging.info(\"CF data split: train=%d, test=%d\", len(train_data), len(test_data))\n",
        "    print(f\"CF data split: train={len(train_data)}, test={len(test_data)}\")\n",
        "\n",
        "    user_item_matrix = train_data.pivot(index='user_id', columns='track_id', values='rating').fillna(0)\n",
        "    if user_item_matrix.empty:\n",
        "        logging.error(\"Empty user-item matrix. Returning zero metrics.\")\n",
        "        print(\"Empty user-item matrix. Returning zero metrics.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    nmf = NMF(n_components=20, random_state=42)\n",
        "    user_features = nmf.fit_transform(user_item_matrix)\n",
        "    item_features = nmf.components_\n",
        "    predictions = np.dot(user_features, item_features)\n",
        "    predicted_ratings = pd.DataFrame(predictions, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
        "\n",
        "    test_matrix = test_data.pivot(index='user_id', columns='track_id', values='rating').fillna(0)\n",
        "    logging.info(\"Test matrix tracks: %d, Predicted tracks: %d\", len(test_matrix.columns), len(predicted_ratings.columns))\n",
        "    print(f\"Test matrix tracks: {len(test_matrix.columns)}, Predicted tracks: {len(predicted_ratings.columns)}\")\n",
        "\n",
        "    shared_tracks = set(test_matrix.columns).intersection(set(predicted_ratings.columns))\n",
        "    logging.info(\"Shared tracks between test and predicted: %d\", len(shared_tracks))\n",
        "    print(f\"Shared tracks between test and predicted: {len(shared_tracks)}\")\n",
        "\n",
        "    if not shared_tracks:\n",
        "        logging.warning(\"No shared tracks between test and predicted. Returning zero metrics.\")\n",
        "        print(\"No shared tracks between test and predicted. Returning zero metrics.\")\n",
        "        return {'precision@k': 0.0}\n",
        "\n",
        "    precisions = []\n",
        "    for user_id in test_matrix.index:\n",
        "        if user_id in predicted_ratings.index:\n",
        "            true_ratings = test_matrix.loc[user_id]\n",
        "            pred_ratings = predicted_ratings.loc[user_id]\n",
        "            valid_top_k = pred_ratings[pred_ratings.index.isin(shared_tracks)].sort_values(ascending=False).index[:k]\n",
        "            if len(valid_top_k) == 0:\n",
        "                logging.warning(\"No valid tracks for user %s in test set.\", user_id)\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "            relevant = (true_ratings[valid_top_k] > 3.5).astype(int)\n",
        "            precision = relevant.mean()\n",
        "            precisions.append(precision)\n",
        "            logging.info(\"User %s: %d valid tracks, precision=%f\", user_id, len(valid_top_k), precision)\n",
        "\n",
        "    metrics = {'precision@k': np.mean(precisions) if precisions else 0.0}\n",
        "    logging.info(\"CF Metrics: %s\", metrics)\n",
        "    return metrics\n",
        "\n",
        "class GenreClassifier(nn.Module):\n",
        "    \"\"\"Task A1.1: Custom genre classifier combining audio and text features.\"\"\"\n",
        "    def __init__(self, audio_dim, text_dim, num_classes, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "        return self.fc(combined)\n",
        "\n",
        "def train_classifier(model, train_loader, criterion, optimizer):\n",
        "    \"\"\"Task A1.1: Train the genre classifier.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, labels in train_loader:\n",
        "        audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_classifier(model, test_loader, genres):\n",
        "    \"\"\"Task C5: Evaluate classifier with precision, recall, F1, and confusion matrix.\"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(DEVICE), text_features.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    errors = [(i, y_true[i], y_pred[i]) for i in range(len(y_true)) if y_true[i] != y_pred[i]]\n",
        "    print(f\"Error Analysis: {len(errors)} misclassifications\")\n",
        "    for idx, true, pred in errors[:5]:\n",
        "        print(f\"Sample {idx}: True={genres[true]}, Predicted={genres[pred]}\")\n",
        "    logging.info(\"Test set class distribution: %s\", Counter(y_true))\n",
        "    print(f\"Test set class distribution: {Counter(y_true)}\")\n",
        "    return {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n",
        "\n",
        "def ast_classifier(audio_files, labels, genres, df_metadata):\n",
        "    \"\"\"Task A2: Audio classification with fine-tuned Wav2Vec2 model.\"\"\"\n",
        "    print(\"\\nSetting up Audio Classifier (Wav2Vec2)...\")\n",
        "    try:\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to load feature extractor: %s\", str(e))\n",
        "        print(f\"Failed to load feature extractor: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    genre_to_label = {genre: idx for idx, genre in enumerate(sorted(genres))}\n",
        "    label_to_genre = {idx: genre for genre, idx in genre_to_label.items()}\n",
        "\n",
        "    valid_audio_files = []\n",
        "    valid_labels = []\n",
        "    for audio_file, label in zip(audio_files, labels):\n",
        "        track_id = Path(audio_file).stem\n",
        "        if track_id in df_metadata['track_id'].values and os.path.exists(audio_file) and os.path.getsize(audio_file) > 0:\n",
        "            genre = df_metadata[df_metadata['track_id'] == track_id]['genre'].iloc[0]\n",
        "            if genre in genre_to_label:\n",
        "                valid_audio_files.append(audio_file)\n",
        "                valid_labels.append(genre_to_label[genre])\n",
        "            else:\n",
        "                logging.warning(\"Genre %s not in genre_to_label for track %s\", genre, track_id)\n",
        "        else:\n",
        "            logging.warning(\"Invalid or missing audio file: %s\", audio_file)\n",
        "\n",
        "    if not valid_audio_files:\n",
        "        print(\"No valid audio files for processing.\")\n",
        "        logging.error(\"No valid audio files for processing.\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    def preprocess_audio(audio_file, label):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_file, sr=16000, duration=5.0)\n",
        "            inputs = feature_extractor(\n",
        "                y,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"np\",\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=80000,\n",
        "                return_attention_mask=True\n",
        "            )\n",
        "            return {\n",
        "                \"input_values\": inputs.input_values[0],\n",
        "                \"attention_mask\": inputs.attention_mask[0] if inputs.attention_mask is not None else np.zeros(80000),\n",
        "                \"labels\": label\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(\"Failed to process audio file %s: %s\", audio_file, str(e))\n",
        "            return None\n",
        "\n",
        "    data = [preprocess_audio(f, l) for f, l in zip(valid_audio_files, valid_labels)]\n",
        "    data = [d for d in data if d is not None]\n",
        "    if not data:\n",
        "        print(\"No valid audio data after preprocessing.\")\n",
        "        logging.error(\"No valid audio data after preprocessing.\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"input_values\": [d[\"input_values\"] for d in data],\n",
        "        \"attention_mask\": [d[\"attention_mask\"] for d in data],\n",
        "        \"labels\": [d[\"labels\"] for d in data]\n",
        "    })\n",
        "\n",
        "    dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForAudioClassification.from_pretrained(\n",
        "            \"facebook/wav2vec2-base-960h\",\n",
        "            num_labels=len(genre_to_label),\n",
        "            label2id=genre_to_label,\n",
        "            id2label=label_to_genre\n",
        "        ).to(DEVICE)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Failed to load Wav2Vec2 model: %s\", str(e))\n",
        "        print(f\"Failed to load Wav2Vec2 model: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(OUTPUT_DIR, \"wav2vec2-finetuned\"),\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        logging_steps=100,\n",
        "        learning_rate=3e-5,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = pred.predictions.argmax(-1)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
        "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"Fine-tuning Wav2Vec2 model...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model(os.path.join(OUTPUT_DIR, \"wav2vec2-finetuned\"))\n",
        "        print(\"Model fine-tuning completed and saved.\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Fine-tuning failed: %s\", str(e))\n",
        "        print(f\"Fine-tuning failed: {str(e)}\")\n",
        "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Wav2Vec2 Metrics: Precision: {eval_results['eval_precision']:.4f}, Recall: {eval_results['eval_recall']:.4f}, F1: {eval_results['eval_f1']:.4f}\")\n",
        "\n",
        "    predictions = trainer.predict(dataset[\"test\"])\n",
        "    y_true = predictions.label_ids\n",
        "    y_pred = predictions.predictions.argmax(-1)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=list(genre_to_label.keys()), yticklabels=list(genre_to_label.keys()))\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Wav2Vec2 Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'wav2vec2_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return {'precision': eval_results['eval_precision'], 'recall': eval_results['eval_recall'], 'f1': eval_results['eval_f1']}\n",
        "\n",
        "def music_search(query, df, text_embeddings, audio_features_df, model, scaler):\n",
        "    \"\"\"Task A3: Enhanced content-based music discovery with cross-modal retrieval and indexing.\"\"\"\n",
        "    import faiss\n",
        "    logging.info(\"Performing enhanced music search for query: %s\", query)\n",
        "    print(f\"Performing enhanced music search for query: {query}\")\n",
        "\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "\n",
        "    audio_features = audio_features_df[[col for col in audio_features_df.columns if col != 'track_id']].values\n",
        "    audio_features = scaler.transform(np.nan_to_num(audio_features))\n",
        "    audio_features = audio_features / np.linalg.norm(audio_features, axis=1, keepdims=True)\n",
        "\n",
        "    text_embedding_matrix = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df['track_id']])\n",
        "    text_embedding_matrix = text_embedding_matrix / np.linalg.norm(text_embedding_matrix, axis=1, keepdims=True)\n",
        "    text_index = faiss.IndexFlatIP(text_embedding_matrix.shape[1])\n",
        "    text_index.add(text_embedding_matrix.astype(np.float32))\n",
        "\n",
        "    audio_index = faiss.IndexFlatIP(audio_features.shape[1])\n",
        "    audio_index.add(audio_features.astype(np.float32))\n",
        "\n",
        "    text_scores, text_indices = text_index.search(query_embedding.reshape(1, -1).astype(np.float32), k=20)\n",
        "    audio_scores, audio_indices = audio_index.search(query_embedding.reshape(1, -1).astype(np.float32), k=20)\n",
        "\n",
        "    combined_scores = {}\n",
        "    for idx, score in zip(text_indices[0], text_scores[0]):\n",
        "        track_id = df['track_id'].iloc[idx]\n",
        "        genre_weight = 1.5 if df[df['track_id'] == track_id]['genre'].iloc[0] == 'Rock' else 1.0\n",
        "        combined_scores[track_id] = 0.7 * score * genre_weight\n",
        "    for idx, score in zip(audio_indices[0], audio_scores[0]):\n",
        "        track_id = df['track_id'].iloc[idx]\n",
        "        genre_weight = 1.5 if df[df['track_id'] == track_id]['genre'].iloc[0] == 'Rock' else 1.0\n",
        "        combined_scores[track_id] = combined_scores.get(track_id, 0) + 0.3 * score * genre_weight\n",
        "\n",
        "    top_tracks = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    result_ids = [track_id for track_id, _ in top_tracks]\n",
        "    results = df[df['track_id'].isin(result_ids)][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Music Search Results</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "            table {{ border-collapse: collapse; width: 100%; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "            th {{ background-color: #f2f2f2; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h2>Search Results for \"{query}\"</h2>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <th>Track ID</th>\n",
        "                <th>Artist</th>\n",
        "                <th>Title</th>\n",
        "                <th>Genre</th>\n",
        "            </tr>\n",
        "    \"\"\"\n",
        "    for _, row in results.iterrows():\n",
        "        html_content += f\"\"\"\n",
        "            <tr>\n",
        "                <td>{row['track_id']}</td>\n",
        "                <td>{row['artist_name']}</td>\n",
        "                <td>{row['title']}</td>\n",
        "                <td>{row['genre']}</td>\n",
        "            </tr>\n",
        "        \"\"\"\n",
        "    html_content += \"\"\"\n",
        "        </table>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    html_path = os.path.join(OUTPUT_DIR, f\"search_results_{uuid.uuid4().hex}.html\")\n",
        "    with open(html_path, 'w') as f:\n",
        "        f.write(html_content)\n",
        "    logging.info(\"Search results saved to %s\", html_path)\n",
        "    print(f\"Search results saved to {html_path}\")\n",
        "\n",
        "    logging.info(\"Search returned %d results.\", len(results))\n",
        "    return results\n",
        "\n",
        "def zero_shot_classification(df_metadata, lyrics_dict):\n",
        "    \"\"\"Task A1.2: Zero-shot genre classification using BART.\"\"\"\n",
        "    print(\"\\nZero-Shot Genre Classification:\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if torch.cuda.is_available() else -1)\n",
        "    genres = df_metadata['genre'].unique().tolist()\n",
        "    results = []\n",
        "    for _, row in df_metadata.head(5).iterrows():\n",
        "        text = row['title'] + \" \" + lyrics_dict.get(row['track_id'], \"\")\n",
        "        scores = classifier(text, candidate_labels=genres, multi_label=False)\n",
        "        results.append((row['track_id'], scores['labels'][0], scores['scores'][0]))\n",
        "        print(f\"Track {row['track_id']}: Predicted={scores['labels'][0]} ({scores['scores'][0]:.4f}), True={row['genre']}\")\n",
        "    return results\n",
        "\n",
        "def analyze_feature_contribution(model, test_loader, feature_columns):\n",
        "    \"\"\"Task A1.1: Analyze contribution of audio features to genre classification.\"\"\"\n",
        "    print(\"\\nFeature Contribution Analysis:\")\n",
        "    model.eval()\n",
        "    contributions = {col: [] for col in feature_columns}\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, _ in test_loader:\n",
        "            audio_features, text_features = audio_features.to(DEVICE), text_features.to(DEVICE)\n",
        "            baseline_output = model(audio_features, text_features)\n",
        "            for i, col in enumerate(feature_columns):\n",
        "                modified_features = audio_features.clone()\n",
        "                modified_features[:, i] = 0\n",
        "                modified_output = model(modified_features, text_features)\n",
        "                diff = torch.mean(torch.abs(baseline_output - modified_output)).item()\n",
        "                contributions[col].append(diff)\n",
        "    for col in feature_columns:\n",
        "        print(f\"{col}: Mean contribution = {np.mean(contributions[col]):.4f}\")\n",
        "    return contributions\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function orchestrating all tasks.\"\"\"\n",
        "    logging.info(\"Starting main execution...\")\n",
        "    print(\"Starting main execution...\")\n",
        "\n",
        "    # Install dependencies\n",
        "    print(\"Installing required dependencies...\")\n",
        "    try:\n",
        "        import faiss\n",
        "    except ImportError:\n",
        "        subprocess.run([\"pip\", \"install\", \"faiss-cpu\"], check=True)\n",
        "        print(\"Installed faiss-cpu\")\n",
        "    try:\n",
        "        from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "    except ImportError:\n",
        "        subprocess.run([\"pip\", \"install\", \"transformers\"], check=True)\n",
        "        print(\"Installed transformers\")\n",
        "    try:\n",
        "        import sentence_transformers\n",
        "    except ImportError:\n",
        "        subprocess.run([\"pip\", \"install\", \"sentence-transformers\"], check=True)\n",
        "        print(\"Installed sentence-transformers\")\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "    except ImportError:\n",
        "        subprocess.run([\"pip\", \"install\", \"tqdm\"], check=True)\n",
        "        print(\"Installed tqdm\")\n",
        "\n",
        "    # Task A1, A3, A5: Setup dataset\n",
        "    setup_fma_dataset()\n",
        "    df_metadata, df_features, lyrics_dict = load_fma_data(AUDIO_PATH, METADATA_PATH, ARTISTS_PATH, GENRES_PATH, LYRICS_PATH, TAGS_PATH)\n",
        "\n",
        "    if df_metadata.empty or df_features.empty or not lyrics_dict:\n",
        "        logging.error(\"Data loading failed. Exiting.\")\n",
        "        print(\"Data loading failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Task A1.2: Generate text embeddings\n",
        "    text_embeddings = generate_text_embeddings(lyrics_dict)\n",
        "\n",
        "    # Task A1.2: Linguistic analysis\n",
        "    analyze_linguistic_patterns(df_metadata, lyrics_dict)\n",
        "\n",
        "    # Task A1.2: Zero-shot classification\n",
        "    zero_shot_classification(df_metadata, lyrics_dict)\n",
        "\n",
        "    # Prepare features\n",
        "    valid_track_ids = df_features['track_id'].tolist()\n",
        "    if len(valid_track_ids) < 50:\n",
        "        logging.warning(\"Only %d valid audio tracks found. Consider increasing MAX_TRACKS or checking audio files.\", len(valid_track_ids))\n",
        "        print(f\"Warning: Only {len(valid_track_ids)} valid audio tracks found. Using available tracks.\")\n",
        "\n",
        "    df_metadata = df_metadata[df_metadata['track_id'].isin(valid_track_ids)]\n",
        "    text_features = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df_metadata['track_id']])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    audio_features = scaler.fit_transform(df_features[[col for col in df_features.columns if col != 'track_id']].values)\n",
        "    audio_features = np.nan_to_num(audio_features)\n",
        "    logging.info(\"Audio feature stats: mean=%s, std=%s\", audio_features.mean(), audio_features.std())\n",
        "    print(f\"Audio feature stats: mean={audio_features.mean():.4f}, std={audio_features.std():.4f}\")\n",
        "\n",
        "    if audio_features.shape[0] == 0 or text_features.shape[0] == 0:\n",
        "        logging.error(\"No valid features after filtering. Exiting.\")\n",
        "        print(\"No valid features after filtering. Exiting.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        user_data = pd.read_csv(USER_DATA_PATH)\n",
        "        user_data['track_id'] = user_data['track_id'].astype(str).str.zfill(6)\n",
        "        user_data = user_data[user_data['track_id'].isin(df_metadata['track_id'])]\n",
        "        ratings = np.clip(user_data['rating'].values / 5.0, 0.0, 1.0)\n",
        "        logging.info(\"Ratings stats: min=%s, max=%s, mean=%s\", ratings.min(), ratings.max(), ratings.mean())\n",
        "        print(f\"Ratings stats: min={ratings.min():.4f}, max={ratings.max():.4f}, mean={ratings.mean():.4f}\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading ratings: %s. Using synthetic ratings.\", str(e))\n",
        "        print(f\"Error loading ratings: {str(e)}. Using synthetic ratings.\")\n",
        "        ratings = np.random.uniform(0.2, 1.0, len(df_metadata))\n",
        "        ratings = np.clip(ratings, 0.0, 1.0)\n",
        "        logging.info(\"Synthetic ratings stats: min=%s, max=%s, mean=%s\", ratings.min(), ratings.max(), ratings.mean())\n",
        "        print(f\"Synthetic ratings stats: min={ratings.min():.4f}, max={ratings.max():.4f}, mean={ratings.mean():.4f}\")\n",
        "\n",
        "    if len(ratings) != len(df_metadata):\n",
        "        logging.warning(\"Ratings length mismatch. Truncating to match metadata.\")\n",
        "        print(\"Ratings length mismatch. Truncating to match metadata.\")\n",
        "        ratings = ratings[:len(df_metadata)]\n",
        "\n",
        "    indices = np.arange(len(df_metadata))\n",
        "    X_train_rec, X_test_rec, y_train_rec, y_test_rec, train_idx, test_idx = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), ratings, indices, test_size=0.2, random_state=42\n",
        "    )\n",
        "    logging.info(\"Train-test split for recommender: train=%d, test=%d\", len(X_train_rec), len(X_test_rec))\n",
        "    print(f\"Train-test split for recommender: train={len(X_train_rec)}, test={len(X_test_rec)}\")\n",
        "\n",
        "    train_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_rec, dtype=torch.float32)\n",
        "    )\n",
        "    train_loader_rec = torch.utils.data.DataLoader(train_dataset_rec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    test_dataset_rec = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_rec, dtype=torch.float32)\n",
        "    )\n",
        "    test_loader_rec = torch.utils.data.DataLoader(test_dataset_rec, batch_size=BATCH_SIZE)\n",
        "\n",
        "    recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=384).to(DEVICE)\n",
        "    criterion_rec = nn.BCELoss()\n",
        "    optimizer_rec = torch.optim.Adam(recommender.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"\\nTraining Recommender Model...\")\n",
        "    for epoch in range(NUM_EPOCHS_REC):\n",
        "        loss = train_recommender(recommender, train_loader_rec, criterion_rec, optimizer_rec)\n",
        "        logging.info(\"Recommendation Epoch %d, Loss: %.4f\", epoch+1, loss)\n",
        "        print(f\"Recommendation Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    rec_metrics = evaluate_recommender(recommender, test_loader_rec, df_metadata, test_idx, df_metadata['genre'].unique())\n",
        "    logging.info(\"Recommendation Metrics: %s\", rec_metrics)\n",
        "    print(f\"Recommendation Metrics: {rec_metrics}\")\n",
        "\n",
        "    cf_metrics = collaborative_filtering(user_data, df_metadata)\n",
        "    print(f\"Collaborative Filtering Metrics: {cf_metrics}\")\n",
        "    print(f\"Comparison: Content-Based Precision@K={rec_metrics['precision@k']:.4f}, CF Precision@K={cf_metrics['precision@k']:.4f}\")\n",
        "\n",
        "    genres = df_metadata['genre'].unique()\n",
        "    genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
        "    labels = df_metadata['genre'].map(genre_to_idx).values\n",
        "\n",
        "    X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    logging.info(\"Train-test split for classifier: train=%d, test=%d\", len(X_train_cls), len(X_test_cls))\n",
        "    print(f\"Train-test split for classifier: train={len(X_train_cls)}, test={len(X_test_cls)}\")\n",
        "\n",
        "    train_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_train_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_cls, dtype=torch.long)\n",
        "    )\n",
        "    train_loader_cls = torch.utils.data.DataLoader(train_dataset_cls, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    test_dataset_cls = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(X_test_cls[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_cls[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_cls, dtype=torch.long)\n",
        "    )\n",
        "    test_loader_cls = torch.utils.data.DataLoader(test_dataset_cls, batch_size=BATCH_SIZE)\n",
        "\n",
        "    classifier = GenreClassifier(audio_dim=audio_features.shape[1], text_dim=384, num_classes=len(genres)).to(DEVICE)\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    print(\"\\nTraining Genre Classifier Model...\")\n",
        "    for epoch in range(NUM_EPOCHS_CLS):\n",
        "        loss = train_classifier(classifier, train_loader_cls, criterion_cls, optimizer_cls)\n",
        "        logging.info(\"Classification Epoch %d, Loss: %.4f\", epoch+1, loss)\n",
        "        print(f\"Classification Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    cls_metrics = evaluate_classifier(classifier, test_loader_cls, genres)\n",
        "    logging.info(\"Classification Metrics: %s\", cls_metrics)\n",
        "    print(f\"Classification Metrics: Precision: {cls_metrics['precision']:.4f}, Recall: {cls_metrics['recall']:.4f}, F1: {cls_metrics['f1']:.4f}\")\n",
        "\n",
        "    analyze_feature_contribution(classifier, test_loader_cls, df_features.columns[1:])\n",
        "\n",
        "    audio_files = [os.path.join(AUDIO_PATH, f\"{tid}.mp3\") for tid in df_metadata['track_id'] if os.path.exists(os.path.join(AUDIO_PATH, f\"{tid}.mp3\"))]\n",
        "    ast_metrics = ast_classifier(audio_files, labels[:len(audio_files)], genres, df_metadata)\n",
        "    print(f\"Wav2Vec2 vs Custom Classifier: Wav2Vec2 F1={ast_metrics['f1']:.4f}, Custom F1={cls_metrics['f1']:.4f}\")\n",
        "\n",
        "    search_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    query = \"upbeat rock songs\"\n",
        "    results = music_search(query, df_metadata, text_embeddings, df_features, search_model, scaler)\n",
        "    print(\"\\nSearch Results:\")\n",
        "    print(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ssl9XfPLe2w2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "4b482214-1969-450c-b811-ae40e21fc759"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-38-2695139063.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-38-2695139063.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}