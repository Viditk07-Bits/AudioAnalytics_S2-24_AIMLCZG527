{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viditk07-Bits/AudioAnalytics_S2-24_AIMLCZG527/blob/main/AA_Assignment2_latest_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EN9uWs6-RQ"
      },
      "source": [
        "# Music Information Retrieval System\n",
        "\n",
        "## Assignment Objective\n",
        "This assignment implements a comprehensive Music Information Retrieval (MIR) system using Large Language Models (LLMs) and deep learning techniques. It includes music recommendation, genre classification, and semantic search applications, combining audio analysis with natural language processing.\n",
        "\n",
        "## Dataset Setup\n",
        "Using the Free Music Archive (FMA) dataset with audio files, metadata, and synthetic user data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX2zSc8N6-RT",
        "outputId": "881ca542-ceca-4ad9-aabf-de3988a9310f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import hashlib\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "from librosa.feature.rhythm import tempo as librosa_tempo\n",
        "\n",
        "# Mount Google Drive (optional, for persistent storage)\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/fma/data\"\n",
        "AUDIO_PATH = os.path.join(DATA_PATH, \"audio_files/\")\n",
        "METADATA_PATH = os.path.join(DATA_PATH, \"metadata/tracks.csv\")\n",
        "ARTISTS_PATH = os.path.join(DATA_PATH, \"metadata/artists.csv\")\n",
        "GENRES_PATH = os.path.join(DATA_PATH, \"metadata/genres.csv\")\n",
        "LYRICS_PATH = os.path.join(DATA_PATH, \"lyrics/\")\n",
        "USER_DATA_PATH = os.path.join(DATA_PATH, \"user_data/ratings.csv\")\n",
        "TAGS_PATH = os.path.join(DATA_PATH, \"descriptions/tags.csv\")\n",
        "OUTPUT_DIR = \"outputs/\"\n",
        "TEMP_DIR = \"/content/fma\"\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "MAX_TRACKS = 1000  # Limit for faster testing\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def setup_fma_dataset():\n",
        "    \"\"\"Download and set up FMA dataset, moving files to expected structure.\"\"\"\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "    os.chdir(TEMP_DIR)\n",
        "\n",
        "    # Step 1: Clone FMA GitHub repository\n",
        "    if not os.path.exists(os.path.join(TEMP_DIR, \"fma\")):\n",
        "        print(\"Cloning FMA repository...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/mdeff/fma.git\"], check=True)\n",
        "    else:\n",
        "        print(\"FMA repository already exists.\")\n",
        "    os.chdir(os.path.join(TEMP_DIR, \"fma\"))\n",
        "\n",
        "    # Step 2: Download fma_small.zip and fma_metadata.zip\n",
        "    fma_small_zip = \"fma_small.zip\"\n",
        "    fma_metadata_zip = \"fma_metadata.zip\"\n",
        "\n",
        "    if os.path.exists(fma_small_zip):\n",
        "        os.remove(fma_small_zip)\n",
        "        print(f\"Removed existing {fma_small_zip}\")\n",
        "    if os.path.exists(fma_metadata_zip):\n",
        "        os.remove(fma_metadata_zip)\n",
        "        print(f\"Removed existing {fma_metadata_zip}\")\n",
        "\n",
        "    print(\"Downloading fma_small.zip...\")\n",
        "    subprocess.run([\"wget\", \"-O\", fma_small_zip, \"https://os.unil.cloud.switch.ch/fma/fma_small.zip\"], check=True)\n",
        "    print(\"Downloading fma_metadata.zip...\")\n",
        "    subprocess.run([\"wget\", \"-O\", fma_metadata_zip, \"https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\"], check=True)\n",
        "\n",
        "    # Step 3: Verify SHA1 checksums\n",
        "    def sha1_checksum(file_path):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(file_path, 'rb') as f:\n",
        "            while chunk := f.read(8192):\n",
        "                sha1.update(chunk)\n",
        "        return sha1.hexdigest()\n",
        "\n",
        "    assert sha1_checksum(\"fma_small.zip\") == \"ade154f733639d52e35e32f5593efe5be76c6d70\", \"fma_small.zip checksum failed!\"\n",
        "    assert sha1_checksum(\"fma_metadata.zip\") == \"f0df49ffe5f2a6008d7dc83c6915b31835dfe733\", \"fma_metadata.zip checksum failed!\"\n",
        "    print(\"✅ SHA1 checksums verified.\")\n",
        "\n",
        "    # Step 4: Unzip files\n",
        "    os.makedirs(DATA_PATH, exist_ok=True)\n",
        "    if not os.path.exists(os.path.join(DATA_PATH, \"fma_small\")):\n",
        "        print(\"Unzipping fma_small.zip...\")\n",
        "        subprocess.run([\"unzip\", \"-q\", \"fma_small.zip\", \"-d\", DATA_PATH], check=True)\n",
        "    else:\n",
        "        print(\"fma_small already unzipped.\")\n",
        "\n",
        "    if not os.path.exists(os.path.join(DATA_PATH, \"fma_metadata\")):\n",
        "        print(\"Unzipping fma_metadata.zip...\")\n",
        "        subprocess.run([\"unzip\", \"-q\", \"fma_metadata.zip\", \"-d\", DATA_PATH], check=True)\n",
        "    else:\n",
        "        print(\"fma_metadata already unzipped.\")\n",
        "\n",
        "    # Step 5: Move .mp3 files to audio_files/\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    print(\"Moving MP3 files...\")\n",
        "    for mp3_file in Path(DATA_PATH).rglob(\"*.mp3\"):\n",
        "        target = os.path.join(AUDIO_PATH, mp3_file.name)\n",
        "        if not os.path.exists(target):\n",
        "            os.rename(mp3_file, target)\n",
        "    print(\"MP3 files moved.\")\n",
        "\n",
        "    # Step 6: Process metadata\n",
        "    print(\"Processing metadata...\")\n",
        "    tracks = pd.read_csv(os.path.join(DATA_PATH, \"fma_metadata\", \"tracks.csv\"), index_col=0, header=[0, 1])\n",
        "    genres = pd.read_csv(os.path.join(DATA_PATH, \"fma_metadata\", \"genres.csv\"))\n",
        "\n",
        "    # Create artists.csv\n",
        "    df_artists = tracks['artist'][['name']].reset_index().rename(columns={'track_id': 'artist_id', 'name': 'artist_name'})\n",
        "    df_artists['artist_id'] = df_artists['artist_id'].astype(str).str.zfill(6)\n",
        "    os.makedirs(os.path.dirname(ARTISTS_PATH), exist_ok=True)\n",
        "    df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "\n",
        "    # Create genres.csv\n",
        "    df_genres = genres[['genre_id', 'title']].rename(columns={'title': 'genre_name'})\n",
        "    os.makedirs(os.path.dirname(GENRES_PATH), exist_ok=True)\n",
        "    df_genres.to_csv(GENRES_PATH, index=False)\n",
        "\n",
        "    # Adapt tracks.csv\n",
        "    df_tracks = tracks['track'][['title', 'genre_top']].reset_index()\n",
        "    df_tracks['track_id'] = df_tracks['track_id'].astype(str).str.zfill(6)\n",
        "    df_tracks['artist_id'] = df_tracks['track_id']  # FMA doesn't provide artist_id, use track_id as proxy\n",
        "    df_tracks['genre_id'] = df_tracks['genre_top'].map(df_genres.set_index('genre_name')['genre_id'])\n",
        "    df_tracks = df_tracks[['track_id', 'title', 'artist_id', 'genre_id']].dropna()\n",
        "    df_tracks = df_tracks.head(MAX_TRACKS)  # Limit tracks for faster processing\n",
        "    os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n",
        "    df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "\n",
        "    # Create synthetic ratings.csv\n",
        "    os.makedirs(os.path.dirname(USER_DATA_PATH), exist_ok=True)\n",
        "    ratings = pd.DataFrame({\n",
        "        'user_id': ['user_001'] * len(df_tracks),\n",
        "        'track_id': df_tracks['track_id'],\n",
        "        'rating': np.random.uniform(0.1, 1.0, len(df_tracks))\n",
        "    })\n",
        "    ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "\n",
        "    # Create empty lyrics/ and descriptions/\n",
        "    os.makedirs(LYRICS_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(TAGS_PATH), exist_ok=True)\n",
        "    pd.DataFrame({'track_id': df_tracks['track_id'], 'tag': ['music'] * len(df_tracks)}).to_csv(TAGS_PATH, index=False)\n",
        "\n",
        "    print(\"🎵 Metadata and audio files are ready.\")\n",
        "    print(f\"Tracks shape: {df_tracks.shape}\")\n",
        "    print(f\"Genres shape: {df_genres.shape}\")\n",
        "    print(f\"Ratings shape: {ratings.shape}\")\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    \"\"\"Extract audio features (MFCCs, chroma, spectral features, tempo) from an MP3 file using Librosa.\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "        tempo = librosa_tempo(y=y, sr=sr)[0]\n",
        "        tempo = tempo[0] if isinstance(tempo, np.ndarray) else tempo\n",
        "\n",
        "        return np.concatenate([\n",
        "            np.mean(mfccs, axis=1),\n",
        "            np.mean(chroma, axis=1),\n",
        "            np.mean(spectral_centroid, axis=1),\n",
        "            np.mean(spectral_contrast, axis=1),\n",
        "            [tempo]  # Ensure this is a 1D list\n",
        "        ])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return np.zeros(40)  # 20 MFCCs + 12 chroma + 1 centroid + 7 contrast + 1 tempo\n",
        "\n",
        "def load_fma_data(audio_path, metadata_path, artists_path, genres_path, lyrics_path=None, tags_path=None):\n",
        "    \"\"\"Load and preprocess FMA dataset from MP3 files, metadata, and lyrics.\"\"\"\n",
        "    if not os.path.exists(audio_path):\n",
        "        print(f\"Error: Audio directory {audio_path} does not exist.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), {}\n",
        "\n",
        "    # Load metadata\n",
        "    try:\n",
        "        df_tracks = pd.read_csv(metadata_path)\n",
        "        df_metadata = df_tracks[['track_id', 'title', 'artist_id', 'genre_id']].dropna()\n",
        "        if os.path.exists(artists_path):\n",
        "            df_artists = pd.read_csv(artists_path)[['artist_id', 'artist_name']]\n",
        "            df_metadata = pd.merge(df_metadata, df_artists, on='artist_id', how='left')\n",
        "        else:\n",
        "            df_metadata['artist_name'] = 'Unknown Artist'\n",
        "\n",
        "        if os.path.exists(genres_path):\n",
        "            df_genres = pd.read_csv(genres_path)[['genre_id', 'genre_name']]\n",
        "            df_metadata = pd.merge(df_metadata, df_genres, on='genre_id', how='left')\n",
        "        else:\n",
        "            df_metadata['genre_name'] = 'unknown'\n",
        "\n",
        "        df_metadata = df_metadata[['track_id', 'artist_name', 'title', 'genre_name']].dropna()\n",
        "        df_metadata.columns = ['track_id', 'artist_name', 'title', 'genre']\n",
        "        df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "        print(f\"Initial metadata shape: {df_metadata.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading metadata: {e}. Creating synthetic metadata.\")\n",
        "        df_metadata = pd.DataFrame(columns=['track_id', 'artist_name', 'title', 'genre'])\n",
        "\n",
        "    # Extract audio features\n",
        "    features = []\n",
        "    audio_files = list(Path(audio_path).glob(\"*.mp3\"))\n",
        "    print(f\"Found {len(audio_files)} audio files.\")\n",
        "    valid_track_ids = df_metadata['track_id'].tolist()\n",
        "    audio_files_to_process = []\n",
        "    processed_count = 0\n",
        "    for audio_file in audio_files:\n",
        "        track_id = audio_file.stem\n",
        "        if track_id in valid_track_ids:\n",
        "            audio_files_to_process.append(audio_file)\n",
        "            processed_count += 1\n",
        "            if processed_count >= MAX_TRACKS:\n",
        "                break\n",
        "\n",
        "    print(f\"Processing features for {len(audio_files_to_process)} relevant audio files.\")\n",
        "    for audio_file in audio_files_to_process:\n",
        "        track_id = audio_file.stem\n",
        "        audio_features = extract_audio_features(audio_file)\n",
        "        if audio_features is not None and audio_features.shape[0] > 0:\n",
        "            features.append([track_id] + audio_features.tolist())\n",
        "        else:\n",
        "            print(f\"Skipping {track_id} due to feature extraction error.\")\n",
        "\n",
        "    feature_columns = ['track_id'] + [f'mfcc_{i+1}' for i in range(20)] + [f'chroma_{i+1}' for i in range(12)] + \\\n",
        "                     ['spectral_centroid'] + [f'spectral_contrast_{i+1}' for i in range(7)] + ['tempo']\n",
        "    df_features = pd.DataFrame(features, columns=feature_columns).dropna()\n",
        "    print(f\"Extracted features for {len(df_features)} tracks\")\n",
        "\n",
        "    # Filter metadata to match available audio features\n",
        "    df_metadata = pd.merge(df_metadata, df_features[['track_id']], on='track_id', how='inner')\n",
        "    print(f\"Filtered metadata shape: {df_metadata.shape}\")\n",
        "\n",
        "    # Load lyrics and tags\n",
        "    lyrics_dict = {}\n",
        "    if lyrics_path and os.path.exists(lyrics_path):\n",
        "        for lyric_file in Path(lyrics_path).glob(\"*.txt\"):\n",
        "            track_id = lyric_file.stem\n",
        "            if track_id in df_metadata['track_id'].values:\n",
        "                with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                    lyrics_dict[track_id] = f.read().strip()\n",
        "\n",
        "    if tags_path and os.path.exists(tags_path):\n",
        "        try:\n",
        "            df_tags = pd.read_csv(tags_path)\n",
        "            for _, row in df_tags.iterrows():\n",
        "                track_id = str(row['track_id']).zfill(6)\n",
        "                if track_id in df_metadata['track_id'].values:\n",
        "                    tag = str(row['tag'])\n",
        "                    if track_id in lyrics_dict:\n",
        "                        lyrics_dict[track_id] += \" \" + tag\n",
        "                    else:\n",
        "                        lyrics_dict[track_id] = tag\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tags: {e}\")\n",
        "\n",
        "    return df_metadata, df_features, lyrics_dict\n",
        "\n",
        "def generate_text_embeddings(lyrics_dict):\n",
        "    \"\"\"Generate embeddings for lyrics and tags using Sentence-Transformers on GPU.\"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    embeddings = {}\n",
        "    for track_id, text in lyrics_dict.items():\n",
        "        embeddings[track_id] = model.encode(text, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Original GenreClassifier and train/evaluate functions\n",
        "class GenreClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_classes, hidden_dim=128):\n",
        "        super(GenreClassifier, self).__init__()\n",
        "        self.audio_layer = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_layer = nn.Linear(text_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        return self.fc(combined)\n",
        "\n",
        "def train_classifier(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, labels in train_loader:\n",
        "        audio_features, text_features, labels = (\n",
        "            audio_features.to(DEVICE),\n",
        "            text_features.to(DEVICE),\n",
        "            labels.to(DEVICE)\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_classifier(model, test_loader, genres):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = (\n",
        "                audio_features.to(DEVICE),\n",
        "                text_features.to(DEVICE),\n",
        "                labels.to(DEVICE)\n",
        "            )\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    return precision, recall, f1\n",
        "\n",
        "def train_recommender(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, ratings in train_loader:\n",
        "        audio_features, text_features, ratings = (\n",
        "            audio_features.to(DEVICE),\n",
        "            text_features.to(DEVICE),\n",
        "            ratings.to(DEVICE)\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features)\n",
        "        loss = criterion(outputs, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_recommender(model, test_loader):\n",
        "    model.eval()\n",
        "    precisions = []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, ratings in test_loader:\n",
        "            audio_features, text_features, ratings = (\n",
        "                audio_features.to(DEVICE),\n",
        "                text_features.to(DEVICE),\n",
        "                ratings.to(DEVICE)\n",
        "            )\n",
        "            outputs = model(audio_features, text_features)\n",
        "            k = min(10, outputs.size(1))\n",
        "            if k == 0:\n",
        "                print(\"Warning: No tracks available for top-k selection\")\n",
        "                continue\n",
        "            top_k = torch.topk(outputs, k=k, dim=1).indices\n",
        "            relevant = (ratings.gather(1, top_k) > 0.5).float()\n",
        "            precision = relevant.mean().item()\n",
        "            precisions.append(precision)\n",
        "    return np.mean(precisions) if precisions else 0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwZTwACi6-Rb"
      },
      "source": [
        "\n",
        "# 2.1.1 Audio Feature Integration with LLMs\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sample_rate=22050):\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def extract_features(self, audio_path):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            return np.concatenate([\n",
        "                np.mean(mfcc, axis=1),\n",
        "                np.mean(spectral_centroid, axis=1),\n",
        "                np.mean(spectral_contrast, axis=1),\n",
        "                np.mean(chroma, axis=1),\n",
        "                [tempo]\n",
        "            ])\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting features from {audio_path}: {e}\")\n",
        "            return np.zeros(40)  # 20 MFCCs + 1 centroid + 7 contrast + 12 chroma + 1 tempo\n",
        "\n",
        "class AudioEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(AudioEmbedding, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, attn_dim=128):\n",
        "        super(CrossModalAttention, self).__init__()\n",
        "        self.audio_proj = nn.Linear(audio_dim, attn_dim)\n",
        "        self.text_proj = nn.Linear(text_dim, attn_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=attn_dim, num_heads=4, batch_first=True)\n",
        "\n",
        "    def forward(self, audio_emb, text_emb):\n",
        "        audio_emb = self.audio_proj(audio_emb).unsqueeze(1)  # (B, 1, attn_dim)\n",
        "        text_emb = self.text_proj(text_emb).unsqueeze(1)     # (B, 1, attn_dim)\n",
        "        attn_output, _ = self.attention(audio_emb, text_emb, text_emb)\n",
        "        return attn_output.squeeze(1)\n",
        "\n",
        "def analyze_feature_contribution(model, features, labels):\n",
        "    contributions = {}\n",
        "    feature_groups = {\n",
        "        'mfcc': slice(0, 20),\n",
        "        'spectral_centroid': slice(20, 21),\n",
        "        'spectral_contrast': slice(21, 28),\n",
        "        'chroma': slice(28, 40),\n",
        "        'tempo': slice(40, 41)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        base_pred = model(features).cpu().numpy()\n",
        "        base_pred_labels = np.argmax(base_pred, axis=1)\n",
        "        base_error = np.mean((base_pred_labels - labels.cpu().numpy()) ** 2)\n",
        "\n",
        "        for feature_name, feature_slice in feature_groups.items():\n",
        "            temp_features = features.clone()\n",
        "            temp_features[:, feature_slice] = 0\n",
        "            pred = model(temp_features).cpu().numpy()\n",
        "            pred_labels = np.argmax(pred, axis=1)\n",
        "            error = np.mean((pred_labels - labels.cpu().numpy()) ** 2)\n",
        "            contributions[feature_name] = error - base_error\n",
        "\n",
        "    return contributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cruWHg3M6-Rc"
      },
      "source": [
        "# 2.1.2 Text-Based Genre Classification\n",
        "class TextGenreClassifier(nn.Module):\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', num_classes=10):\n",
        "        super(TextGenreClassifier, self).__init__()\n",
        "        self.model = SentenceTransformer(model_name, device=DEVICE)\n",
        "        self.classifier = nn.Linear(384, num_classes)  # MiniLM-L6-v2 has 384-dim embeddings\n",
        "\n",
        "    def forward(self, lyrics):\n",
        "        embeddings = self.model.encode(lyrics, convert_to_tensor=True, device=DEVICE)\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "    def fine_tune(self, lyrics_data, labels, epochs=3):\n",
        "        self.train()\n",
        "        optimizer = torch.optim.Adam(self.classifier.parameters(), lr=2e-5)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self(lyrics_data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Fine-tuning Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    def zero_shot_classify(self, lyrics, genre_list):\n",
        "        lyrics_emb = self.model.encode([lyrics], convert_to_tensor=True, device=DEVICE)[0]\n",
        "        genre_embs = self.model.encode(genre_list, convert_to_tensor=True, device=DEVICE)\n",
        "        similarities = [1 - cosine(lyrics_emb.cpu().numpy(), genre_emb.cpu().numpy()) for genre_emb in genre_embs]\n",
        "        return genre_list[np.argmax(similarities)]\n",
        "\n",
        "    def analyze_linguistic_patterns(self, lyrics_data, genres):\n",
        "        embeddings = self.model.encode(lyrics_data, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "        patterns = {}\n",
        "        for genre in set(genres):\n",
        "            genre_indices = [i for i, g in enumerate(genres) if g == genre]\n",
        "            genre_embs = embeddings[genre_indices]\n",
        "            patterns[genre] = np.mean(genre_embs, axis=0)\n",
        "        return patterns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt78yJo96-Rd"
      },
      "source": [
        "# 2.1.3 Hybrid Multi-Modal Classification\n",
        "class HybridGenreClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_genres):\n",
        "        super(HybridGenreClassifier, self).__init__()\n",
        "        self.audio_emb = AudioEmbedding(audio_dim, 128)\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.attention = CrossModalAttention(audio_dim=128, text_dim=text_dim, attn_dim=128)\n",
        "        self.classifier = nn.Linear(128 + text_dim + text_dim, num_genres)  # 128 from attention, 384 text, 384 metadata\n",
        "\n",
        "    def forward(self, audio_features, lyrics, metadata):\n",
        "        audio_emb = self.audio_emb(audio_features)\n",
        "        text_emb = self.text_model.encode(lyrics, convert_to_tensor=True, device=DEVICE)\n",
        "        metadata_emb = self.text_model.encode(\n",
        "            [f\"{m[0]} {m[1]}\" for m in metadata],\n",
        "            convert_to_tensor=True,\n",
        "            device=DEVICE\n",
        "        )\n",
        "        fused = self.attention(audio_emb, text_emb)\n",
        "        combined = torch.cat([fused, text_emb, metadata_emb], dim=-1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "    def get_confidence_scores(self, outputs):\n",
        "        return torch.softmax(outputs, dim=-1)\n",
        "\n",
        "def compare_with_audio_only(hybrid_model, audio_only_model, test_data, lyrics, metadata, labels):\n",
        "    hybrid_preds = hybrid_model(test_data, lyrics, metadata)\n",
        "    dummy_text_features = torch.zeros((test_data.shape[0], 384), dtype=torch.float32, device=test_data.device)\n",
        "    audio_preds = audio_only_model(test_data, dummy_text_features)\n",
        "    hybrid_metrics = precision_recall_fscore_support(labels.cpu().numpy(), torch.argmax(hybrid_preds, dim=1).cpu().numpy(), average='weighted')\n",
        "    audio_metrics = precision_recall_fscore_support(labels.cpu().numpy(), torch.argmax(audio_preds, dim=1).cpu().numpy(), average='weighted')\n",
        "    return hybrid_metrics, audio_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfs7A2sK6-Re"
      },
      "source": [
        "# 2.2 Transformer-Based Audio Classification\n",
        "class AudioSpectrogramTransformer(nn.Module):\n",
        "    def __init__(self, patch_size=16, in_channels=1, embed_dim=768, num_heads=12, num_layers=12, num_classes=10):\n",
        "        super(AudioSpectrogramTransformer, self).__init__()\n",
        "        self.patch_embedding = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads), num_layers=num_layers\n",
        "        )\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, spectrogram):\n",
        "        patches = self.patch_embedding(spectrogram)\n",
        "        patches = patches.flatten(2).transpose(1, 2)\n",
        "        transformer_output = self.transformer(patches)\n",
        "        return self.classifier(transformer_output[:, 0])\n",
        "\n",
        "def visualize_attention_patterns(model, spectrogram):\n",
        "    with torch.no_grad():\n",
        "        patches = model.patch_embedding(spectrogram).flatten(2).transpose(1, 2)\n",
        "        attention = model.transformer.layers[-1].self_attn(patches, patches, patches)[1]\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(attention[0].cpu().numpy(), cmap='viridis')\n",
        "        plt.title('Attention Patterns')\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'attention_patterns.png'))\n",
        "        plt.close()\n",
        "\n",
        "class CNNBaseline(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNNBaseline, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * 30 * 30, num_classes)  # Adjusted for 128x128 input\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.flatten(1)\n",
        "        return self.fc(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk8stzGc6-Rf"
      },
      "source": [
        "# 3.1 Semantic Music Search\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "class MusicSearchSystem:\n",
        "    def __init__(self, metadata, audio_embeddings, text_embeddings):\n",
        "        self.metadata = metadata\n",
        "        self.audio_embeddings = audio_embeddings  # Shape: (n_tracks, 384)\n",
        "        self.text_embeddings = text_embeddings    # Shape: (n_tracks, 384)\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def process_query(self, query):\n",
        "        query_emb = self.text_model.encode([query], convert_to_tensor=True, device=DEVICE)[0]\n",
        "        intent = torch.softmax(self.intent_classifier(query_emb), dim=-1)\n",
        "        expanded_query = self.expand_query(query)\n",
        "        return intent, expanded_query\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        synonyms = self.text_model.encode([f'similar to {query}', f'like {query}'], convert_to_tensor=True, device=DEVICE)\n",
        "        return synonyms.mean(dim=0)\n",
        "\n",
        "    def multi_modal_search(self, query, top_k=10):\n",
        "            # Encode query\n",
        "            query_emb = self.text_model.encode(query, convert_to_numpy=True)  # Shape: (384,)\n",
        "\n",
        "            # Compute similarities (cosine similarity for both audio and text)\n",
        "            audio_similarities = np.array([1 - cosine(query_emb, audio_emb) for audio_emb in self.audio_embeddings])\n",
        "            text_similarities = np.array([1 - cosine(query_emb, text_emb) for text_emb in self.text_embeddings])\n",
        "\n",
        "            # Combine similarities (e.g., weighted average)\n",
        "            combined_similarities = 0.5 * audio_similarities + 0.5 * text_similarities\n",
        "\n",
        "            # Get top-k indices\n",
        "            top_k_indices = np.argsort(combined_similarities)[::-1][:top_k]\n",
        "\n",
        "            # Return top-k tracks\n",
        "            results = [\n",
        "                {\n",
        "                    \"track_id\": self.metadata.iloc[i]['track_id'],\n",
        "                    \"artist_name\": self.metadata.iloc[i]['artist_name'],\n",
        "                    \"title\": self.metadata.iloc[i]['title'],\n",
        "                    \"genre\": self.metadata.iloc[i]['genre']\n",
        "                }\n",
        "                for i in top_k_indices\n",
        "            ]\n",
        "            return results\n",
        "\n",
        "    def metadata_similarity(self, query_emb, metadata):\n",
        "        metadata_emb = self.text_model.encode([f\"{metadata[0]} {metadata[1]}\"], convert_to_tensor=True, device=DEVICE)[0].cpu().numpy()\n",
        "        return 1 - cosine(query_emb, metadata_emb)\n",
        "\n",
        "# 3.1.3 Content-Based Music Discovery\n",
        "class MusicDiscovery:\n",
        "    def __init__(self):\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.audio_model = AudioSpectrogramTransformer()\n",
        "        self.audio_extractor = AudioFeatureExtractor()\n",
        "        self.mood_classifier = nn.Linear(384, 5)  # 5 mood classes\n",
        "        self.energy_predictor = nn.Linear(40, 1)\n",
        "        self.danceability_predictor = nn.Linear(40, 1)\n",
        "\n",
        "    def generate_tags(self, audio_path, lyrics):\n",
        "        audio_features = self.audio_extractor.extract_features(audio_path)\n",
        "        audio_features = torch.tensor(audio_features, dtype=torch.float32, device=DEVICE)\n",
        "        lyrics_emb = self.text_model.encode([lyrics], convert_to_tensor=True, device=DEVICE)[0]\n",
        "        mood_scores = torch.softmax(self.mood_classifier(lyrics_emb), dim=-1)\n",
        "        energy = torch.sigmoid(self.energy_predictor(audio_features))\n",
        "        danceability = torch.sigmoid(self.danceability_predictor(audio_features))\n",
        "        return {'mood': mood_scores.cpu().numpy(), 'energy': energy.cpu().numpy(), 'danceability': danceability.cpu().numpy()}\n",
        "\n",
        "    def generate_playlist(self, seed_song, music_collection, df_metadata):\n",
        "        seed_emb = self.get_song_embedding(seed_song)\n",
        "        similarities = [1 - cosine(seed_emb, self.get_song_embedding(song)) for song in music_collection]\n",
        "        top_indices = np.argsort(similarities)[::-1][:10]\n",
        "        return df_metadata.iloc[top_indices][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    def get_song_embedding(self, song):\n",
        "        audio_features = self.audio_extractor.extract_features(song['audio_path'])\n",
        "        lyrics_emb = self.text_model.encode([song['lyrics']], convert_to_tensor=True, device=DEVICE)[0].cpu().numpy()\n",
        "        return np.concatenate([audio_features, lyrics_emb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBsJ5zzj6-Rf"
      },
      "source": [
        "# 4.1 Personalized Recommendation Engines\n",
        "class MusicRecommender(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_tracks):\n",
        "        super().__init__()\n",
        "        self.audio_embed = AudioEmbedding(audio_dim)\n",
        "        self.text_projector = nn.Linear(text_dim, 32)\n",
        "        self.user_embed = nn.Embedding(num_tracks, 32)\n",
        "        self.classifier = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, audio_features, text_features, user_indices):\n",
        "        audio_emb = self.audio_embed(audio_features)\n",
        "        text_emb = self.text_projector(text_features)\n",
        "        user_emb = self.user_embed(user_indices)\n",
        "        combined = torch.cat([audio_emb, user_emb], dim=1)\n",
        "        return torch.sigmoid(self.classifier(combined))\n",
        "\n",
        "    def build_user_profile(self, user_id, ratings):\n",
        "        indices = ratings[ratings['user_id'] == user_id].index\n",
        "        if len(indices) == 0:\n",
        "            return torch.zeros(32, device=DEVICE)\n",
        "        return torch.mean(self.user_embed(torch.tensor(indices, device=DEVICE)), dim=0)\n",
        "\n",
        "    def generate_explanation(self, user_id, item_id, data):\n",
        "        profile = self.build_user_profile(user_id, data)\n",
        "        item_data = data[data['track_id'] == item_id]\n",
        "        if item_data.empty:\n",
        "            return f\"No data available for item {item_id}.\"\n",
        "        return f\"Recommended because {user_id} enjoys {item_data['genre'].iloc[0]} music.\"\n",
        "\n",
        "    def get_item_features(self, item_id):\n",
        "        # Placeholder, to be replaced with actual feature extraction\n",
        "        return torch.zeros(128, device=DEVICE)\n",
        "\n",
        "class HybridRecommender(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_tracks):\n",
        "        super(HybridRecommender, self).__init__()\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.audio_layer = nn.Linear(audio_dim, 128)\n",
        "        self.text_layer = nn.Linear(text_dim, 128)\n",
        "        self.fc = nn.Linear(128 * 2, num_tracks)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_out = torch.relu(self.audio_layer(audio_features))\n",
        "        text_out = torch.relu(self.text_layer(text_features))\n",
        "        combined = torch.cat([audio_out, text_out], dim=1)\n",
        "        return torch.sigmoid(self.fc(combined))\n",
        "\n",
        "    def recommend(self, user_id, context, audio_features, text_features, df_metadata):\n",
        "        context_emb = self.text_model.encode([context], convert_to_tensor=True, device=DEVICE)\n",
        "        scores = self.forward(audio_features, text_features + context_emb)\n",
        "        top_indices = torch.argsort(scores, dim=1, descending=True)[:, :10].cpu().numpy().flatten()\n",
        "        return df_metadata.iloc[top_indices][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    def optimize_diversity(self, recommendations, df_metadata):\n",
        "        genre_counts = recommendations['genre'].value_counts()\n",
        "        diversity_score = len(genre_counts) / len(recommendations)\n",
        "        if diversity_score < 0.5:\n",
        "            diverse_indices = []\n",
        "            for genre in df_metadata['genre'].unique():\n",
        "                genre_recs = recommendations[recommendations['genre'] == genre]\n",
        "                if not genre_recs.empty:\n",
        "                    diverse_indices.append(genre_recs.index[0])\n",
        "            return df_metadata.iloc[diverse_indices]\n",
        "        return recommendations"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZYzXEmw6-Rg"
      },
      "source": [
        "# 5.1 Comprehensive Evaluation Framework\n",
        "class EvaluationFramework:\n",
        "    def __init__(self):\n",
        "        self.metrics = {}\n",
        "\n",
        "    def evaluate_classification(self, y_true, y_pred, genres):\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'classification_confusion_matrix.png'))\n",
        "        plt.close()\n",
        "        return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "    def evaluate_retrieval(self, relevant_items, retrieved_items, k_values=[5, 10, 20]):\n",
        "        metrics = {}\n",
        "        for k in k_values:\n",
        "            k = min(k, len(retrieved_items))\n",
        "            precision = len(set(retrieved_items[:k]) & set(relevant_items)) / k if k > 0 else 0\n",
        "            recall = len(set(retrieved_items[:k]) & set(relevant_items)) / len(relevant_items) if relevant_items else 0\n",
        "            metrics[f'P@{k}'] = precision\n",
        "            metrics[f'R@{k}'] = recall\n",
        "        ap = self.calculate_map(relevant_items, retrieved_items)\n",
        "        ndcg = self.calculate_ndcg(relevant_items, retrieved_items)\n",
        "        metrics['MAP'] = ap\n",
        "        metrics['NDCG'] = ndcg\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_recommendation(self, recommendations, user_interactions):\n",
        "        ctr = sum(1 for rec in recommendations['track_id'] if rec in user_interactions) / len(recommendations) if len(recommendations) > 0 else 0\n",
        "        diversity = len(recommendations['genre'].unique()) / len(recommendations) if len(recommendations) > 0 else 0\n",
        "        novelty = 1 - sum(1 for rec in recommendations['track_id'] if rec in user_interactions) / len(recommendations) if len(recommendations) > 0 else 0\n",
        "        return {'CTR': ctr, 'Diversity': diversity, 'Novelty': novelty}\n",
        "\n",
        "    def calculate_map(self, relevant, retrieved):\n",
        "        ap = 0\n",
        "        relevant_set = set(relevant)\n",
        "        for i, item in enumerate(retrieved):\n",
        "            if item in relevant_set:\n",
        "                ap += len(set(retrieved[:i+1]) & relevant_set) / (i + 1)\n",
        "        return ap / len(relevant) if relevant else 0\n",
        "\n",
        "    def calculate_ndcg(self, relevant, retrieved):\n",
        "        dcg = 0\n",
        "        idcg = sum(1 / np.log2(i + 2) for i in range(len(relevant)))\n",
        "        for i, item in enumerate(retrieved):\n",
        "            if item in relevant:\n",
        "                dcg += 1 / np.log2(i + 2)\n",
        "        return dcg / idcg if idcg > 0 else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F5Ud3Zi-UlUk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6t3yNE6XJJ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VrsjCike2Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nV2N6v4qe29H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impl latest"
      ],
      "metadata": {
        "id": "_-2AYSAEJLaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "import urllib.request\n",
        "import hashlib\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = '/content/fma/data'\n",
        "AUDIO_PATH = os.path.join(DATA_PATH, 'audio_files')\n",
        "METADATA_PATH = os.path.join(DATA_PATH, 'metadata', 'tracks.csv')\n",
        "ARTISTS_PATH = os.path.join(DATA_PATH, 'metadata', 'artists.csv')\n",
        "GENRES_PATH = os.path.join(DATA_PATH, 'metadata', 'genres.csv')\n",
        "LYRICS_PATH = os.path.join(DATA_PATH, 'lyrics')\n",
        "TAGS_PATH = os.path.join(DATA_PATH, 'metadata', 'tags.csv')\n",
        "USER_DATA_PATH = os.path.join(DATA_PATH, 'user_data', 'ratings.csv')\n",
        "OUTPUT_DIR = os.path.join(DATA_PATH, 'output')\n",
        "MAX_TRACKS = 1000\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Dataset Setup\n",
        "def setup_fma_dataset():\n",
        "    \"\"\"Download and set up the FMA dataset.\"\"\"\n",
        "    os.makedirs(DATA_PATH, exist_ok=True)\n",
        "    os.makedirs(AUDIO_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.join(DATA_PATH, 'metadata'), exist_ok=True)\n",
        "    os.makedirs(LYRICS_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.join(DATA_PATH, 'user_data'), exist_ok=True)\n",
        "\n",
        "    # Download FMA small dataset and metadata\n",
        "    logging.info(\"Cloning FMA repository...\")\n",
        "    if not os.path.exists(os.path.join(DATA_PATH, 'fma_small')):\n",
        "        urllib.request.urlretrieve(\n",
        "            'https://os.unil.cloud.switch.ch/fma/fma_small.zip',\n",
        "            os.path.join(DATA_PATH, 'fma_small.zip')\n",
        "        )\n",
        "        urllib.request.urlretrieve(\n",
        "            'https://os.unil.cloud.switch.ch/fma/fma_metadata.zip',\n",
        "            os.path.join(DATA_PATH, 'fma_metadata.zip')\n",
        "        )\n",
        "\n",
        "        # Verify checksums (replace with actual FMA checksums from dataset documentation)\n",
        "        def verify_checksum(file_path, expected_sha1):\n",
        "            sha1 = hashlib.sha1()\n",
        "            with open(file_path, 'rb') as f:\n",
        "                while chunk := f.read(8192):\n",
        "                    sha1.update(chunk)\n",
        "            return sha1.hexdigest() == expected_sha1\n",
        "\n",
        "        if verify_checksum(os.path.join(DATA_PATH, 'fma_small.zip'), 'expected_sha1_small') and \\\n",
        "           verify_checksum(os.path.join(DATA_PATH, 'fma_metadata.zip'), 'expected_sha1_metadata'):\n",
        "            logging.info(\"✅ SHA1 checksums verified.\")\n",
        "        else:\n",
        "            logging.warning(\"⚠️ Checksum verification failed. Proceeding anyway.\")\n",
        "\n",
        "        # Unzip files\n",
        "        logging.info(\"Unzipping fma_small.zip...\")\n",
        "        with zipfile.ZipFile(os.path.join(DATA_PATH, 'fma_small.zip'), 'r') as zip_ref:\n",
        "            zip_ref.extractall(DATA_PATH)\n",
        "        logging.info(\"Unzipping fma_metadata.zip...\")\n",
        "        with zipfile.ZipFile(os.path.join(DATA_PATH, 'fma_metadata.zip'), 'r') as zip_ref:\n",
        "            zip_ref.extractall(DATA_PATH)\n",
        "\n",
        "    # Move MP3 files\n",
        "    logging.info(\"Moving MP3 files...\")\n",
        "    for folder in ['fma_small/000', 'fma_small/001']:\n",
        "        if os.path.exists(os.path.join(DATA_PATH, folder)):\n",
        "            for mp3_file in Path(os.path.join(DATA_PATH, folder)).glob(\"*.mp3\"):\n",
        "                shutil.move(str(mp3_file), os.path.join(AUDIO_PATH, mp3_file.name))\n",
        "    logging.info(\"MP3 files moved.\")\n",
        "\n",
        "    # Process metadata\n",
        "    logging.info(\"Processing metadata...\")\n",
        "    df_tracks = pd.read_csv(os.path.join(DATA_PATH, 'fma_metadata', 'tracks.csv'), skiprows=[0, 1, 2])\n",
        "    df_tracks.columns = ['track_id', 'title', 'artist_id', 'genre_id']  # Simplified\n",
        "    df_tracks = df_tracks.dropna()\n",
        "    df_tracks['track_id'] = df_tracks['track_id'].astype(str).str.zfill(6)\n",
        "    df_tracks = df_tracks.head(MAX_TRACKS)\n",
        "    df_tracks.to_csv(METADATA_PATH, index=False)\n",
        "\n",
        "    # Create synthetic genres and artists\n",
        "    df_genres = pd.DataFrame({\n",
        "        'genre_id': range(1, 11),\n",
        "        'genre_name': ['Rock', 'Pop', 'Jazz', 'Classical', 'Hip-Hop', 'Electronic', 'Folk', 'Blues', 'Country', 'Reggae']\n",
        "    })\n",
        "    df_genres.to_csv(GENRES_PATH, index=False)\n",
        "\n",
        "    df_artists = pd.DataFrame({\n",
        "        'artist_id': range(1, len(df_tracks) + 1),\n",
        "        'artist_name': [f\"Artist_{i}\" for i in range(1, len(df_tracks) + 1)]\n",
        "    })\n",
        "    df_artists.to_csv(ARTISTS_PATH, index=False)\n",
        "\n",
        "    # Create synthetic ratings\n",
        "    df_ratings = pd.DataFrame({\n",
        "        'user_id': [f\"user_{i%10+1}\" for i in range(len(df_tracks))],\n",
        "        'track_id': df_tracks['track_id'],\n",
        "        'rating': [random.uniform(0, 1) for _ in range(len(df_tracks))]\n",
        "    })\n",
        "    df_ratings.to_csv(USER_DATA_PATH, index=False)\n",
        "\n",
        "    # Create synthetic tags\n",
        "    df_tags = pd.DataFrame({\n",
        "        'track_id': df_tracks['track_id'],\n",
        "        'tag': ['music'] * len(df_tracks)\n",
        "    })\n",
        "    df_tags.to_csv(TAGS_PATH, index=False)\n",
        "\n",
        "    logging.info(\"🎵 Metadata and audio files are ready.\")\n",
        "    logging.info(f\"Tracks shape: {df_tracks.shape}\")\n",
        "    logging.info(f\"Genres shape: {df_genres.shape}\")\n",
        "    logging.info(f\"Ratings shape: {df_ratings.shape}\")\n",
        "\n",
        "# Audio Feature Extraction\n",
        "def extract_audio_features(audio_path):\n",
        "    \"\"\"Extract audio features (MFCCs, chroma, spectral features, tempo) from an MP3 file using Librosa.\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)  # Shape: (20, T)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)    # Shape: (12, T)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)  # Shape: (1, T)\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)  # Shape: (7, T)\n",
        "        tempo = librosa.feature.rhythm.tempo(y=y, sr=sr)  # Returns a scalar or 1D array\n",
        "\n",
        "        # Ensure tempo is a scalar\n",
        "        if isinstance(tempo, np.ndarray):\n",
        "            tempo = tempo[0] if tempo.size > 0 else 0.0\n",
        "        else:\n",
        "            tempo = float(tempo)\n",
        "\n",
        "        return np.concatenate([\n",
        "            np.mean(mfccs, axis=1),           # Shape: (20,)\n",
        "            np.mean(chroma, axis=1),          # Shape: (12,)\n",
        "            np.mean(spectral_centroid, axis=1),  # Shape: (1,)\n",
        "            np.mean(spectral_contrast, axis=1),  # Shape: (7,)\n",
        "            [tempo]                           # Shape: (1,)\n",
        "        ])  # Total shape: (41,)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {audio_path}: {e}\")\n",
        "        return np.zeros(41)  # Return zero vector of correct length\n",
        "\n",
        "# Data Loading\n",
        "def load_fma_data(audio_path, metadata_path, artists_path, genres_path, lyrics_path=None, tags_path=None):\n",
        "    \"\"\"Load and preprocess FMA dataset from MP3 files, metadata, and lyrics.\"\"\"\n",
        "    if not os.path.exists(audio_path):\n",
        "        logging.error(f\"Audio directory {audio_path} does not exist.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), {}\n",
        "\n",
        "    # Load metadata\n",
        "    try:\n",
        "        df_tracks = pd.read_csv(metadata_path)\n",
        "        df_metadata = df_tracks[['track_id', 'title', 'artist_id', 'genre_id']].dropna()\n",
        "        if os.path.exists(artists_path):\n",
        "            df_artists = pd.read_csv(artists_path)[['artist_id', 'artist_name']]\n",
        "            df_metadata = pd.merge(df_metadata, df_artists, on='artist_id', how='left')\n",
        "        else:\n",
        "            df_metadata['artist_name'] = 'Unknown Artist'\n",
        "\n",
        "        if os.path.exists(genres_path):\n",
        "            df_genres = pd.read_csv(genres_path)[['genre_id', 'genre_name']]\n",
        "            df_metadata = pd.merge(df_metadata, df_genres, on='genre_id', how='left')\n",
        "        else:\n",
        "            df_metadata['genre_name'] = 'unknown'\n",
        "\n",
        "        df_metadata = df_metadata[['track_id', 'artist_name', 'title', 'genre_name']].dropna()\n",
        "        df_metadata.columns = ['track_id', 'artist_name', 'title', 'genre']\n",
        "        df_metadata['track_id'] = df_metadata['track_id'].astype(str).str.zfill(6)\n",
        "        logging.info(f\"Initial metadata shape: {df_metadata.shape}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading metadata: {e}. Creating synthetic metadata.\")\n",
        "        df_metadata = pd.DataFrame(columns=['track_id', 'artist_name', 'title', 'genre'])\n",
        "\n",
        "    # Extract audio features\n",
        "    features = []\n",
        "    audio_files = list(Path(audio_path).glob(\"*.mp3\"))\n",
        "    logging.info(f\"Found {len(audio_files)} audio files.\")\n",
        "    valid_track_ids = set(df_metadata['track_id'].tolist())\n",
        "    audio_files_to_process = []\n",
        "    processed_count = 0\n",
        "    missing_files = []\n",
        "    for audio_file in audio_files:\n",
        "        track_id = audio_file.stem\n",
        "        audio_file_path = os.path.join(audio_path, f\"{track_id}.mp3\")\n",
        "        if not os.path.exists(audio_file_path):\n",
        "            missing_files.append(track_id)\n",
        "            continue\n",
        "        if track_id in valid_track_ids:\n",
        "            audio_files_to_process.append(audio_file)\n",
        "            processed_count += 1\n",
        "        else:\n",
        "            missing_files.append(track_id)\n",
        "        if processed_count >= MAX_TRACKS:\n",
        "            break\n",
        "\n",
        "    if missing_files:\n",
        "        logging.warning(f\"Missing or mismatched {len(missing_files)} audio files: {missing_files[:5]}...\")\n",
        "\n",
        "    logging.info(f\"Processing features for {len(audio_files_to_process)} relevant audio files.\")\n",
        "    for audio_file in audio_files_to_process:\n",
        "        track_id = audio_file.stem\n",
        "        audio_features = extract_audio_features(audio_file)\n",
        "        if audio_features.shape[0] == 41:  # Verify feature length\n",
        "            features.append([track_id] + audio_features.tolist())\n",
        "        else:\n",
        "            logging.warning(f\"Skipping {track_id} due to feature extraction error.\")\n",
        "            features.append([track_id] + [0.0] * 41)\n",
        "\n",
        "    feature_columns = ['track_id'] + [f'mfcc_{i+1}' for i in range(20)] + [f'chroma_{i+1}' for i in range(12)] + \\\n",
        "                     ['spectral_centroid'] + [f'spectral_contrast_{i+1}' for i in range(7)] + ['tempo']\n",
        "    try:\n",
        "        df_features = pd.DataFrame(features, columns=feature_columns).dropna()\n",
        "        logging.info(f\"Extracted features for {len(df_features)} tracks\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating features DataFrame: {e}\")\n",
        "        return df_metadata, pd.DataFrame(), {}\n",
        "\n",
        "    # Filter metadata to match available audio features\n",
        "    df_metadata = pd.merge(df_metadata, df_features[['track_id']], on='track_id', how='inner')\n",
        "    logging.info(f\"Filtered metadata shape: {df_metadata.shape}\")\n",
        "\n",
        "    # Load lyrics and tags\n",
        "    lyrics_dict = {}\n",
        "    if lyrics_path and os.path.exists(lyrics_path):\n",
        "        for lyric_file in Path(lyrics_path).glob(\"*.txt\"):\n",
        "            track_id = lyric_file.stem\n",
        "            if track_id in df_metadata['track_id'].values:\n",
        "                try:\n",
        "                    with open(lyric_file, 'r', encoding='utf-8') as f:\n",
        "                        lyrics = f.read().strip()\n",
        "                        lyrics_dict[track_id] = lyrics if lyrics else 'music'\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error reading lyrics for {track_id}: {e}\")\n",
        "                    lyrics_dict[track_id] = 'music'\n",
        "\n",
        "    if tags_path and os.path.exists(tags_path):\n",
        "        try:\n",
        "            df_tags = pd.read_csv(tags_path)\n",
        "            for _, row in df_tags.iterrows():\n",
        "                track_id = str(row['track_id']).zfill(6)\n",
        "                if track_id in df_metadata['track_id'].values:\n",
        "                    tag = str(row['tag'])\n",
        "                    if track_id in lyrics_dict:\n",
        "                        lyrics_dict[track_id] += \" \" + tag\n",
        "                    else:\n",
        "                        lyrics_dict[track_id] = tag\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading tags: {e}\")\n",
        "\n",
        "    # Ensure all tracks have lyrics or default to 'music'\n",
        "    for track_id in df_metadata['track_id']:\n",
        "        if track_id not in lyrics_dict:\n",
        "            lyrics_dict[track_id] = 'music'\n",
        "            logging.debug(f\"Assigned default lyrics 'music' to track {track_id}\")\n",
        "\n",
        "    return df_metadata, df_features, lyrics_dict\n",
        "\n",
        "# Model Definitions\n",
        "class FeatureProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class AudioEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_features = audio_features.unsqueeze(0)  # Add sequence dimension\n",
        "        text_features = text_features.unsqueeze(0)\n",
        "        attn_output, _ = self.attention(audio_features, text_features, text_features)\n",
        "        return self.norm(attn_output.squeeze(0))\n",
        "\n",
        "class TextGenreClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.classifier = nn.Linear(384, num_classes)\n",
        "\n",
        "    def forward(self, texts):\n",
        "        embeddings = self.text_model.encode(texts, convert_to_tensor=True, device=DEVICE)\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "    def fine_tune(self, texts, labels, epochs=3, lr=0.001):\n",
        "        optimizer = torch.optim.Adam(self.classifier.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        self.train()\n",
        "        for epoch in range(epochs):\n",
        "            embeddings = self.text_model.encode(texts, convert_to_tensor=True, device=DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.classifier(embeddings)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            logging.info(f\"Text Classifier Fine-Tune Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    def zero_shot_classify(self, query, genres):\n",
        "        query_embedding = self.text_model.encode([query], convert_to_tensor=True, device=DEVICE)\n",
        "        genre_embeddings = self.text_model.encode(genres, convert_to_tensor=True, device=DEVICE)\n",
        "        similarities = torch.cosine_similarity(query_embedding, genre_embeddings)\n",
        "        return genres[torch.argmax(similarities).item()]\n",
        "\n",
        "    def analyze_linguistic_patterns(self, texts, genres):\n",
        "        embeddings = self.text_model.encode(texts, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "        patterns = {}\n",
        "        for genre in set(genres):\n",
        "            indices = [i for i, g in enumerate(genres) if g == genre]\n",
        "            patterns[genre] = embeddings[indices]\n",
        "        return patterns\n",
        "\n",
        "class HybridGenreClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_genres):\n",
        "        super().__init__()\n",
        "        self.audio_embed = AudioEmbedding(audio_dim)\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.attention = CrossModalAttention(32)\n",
        "        self.metadata_embed = nn.Linear(384, 32)\n",
        "        self.classifier = nn.Linear(32, num_genres)\n",
        "\n",
        "    def forward(self, audio_features, lyrics, metadata):\n",
        "        audio_emb = self.audio_embed(audio_features)\n",
        "        text_emb = self.text_model.encode(lyrics, convert_to_tensor=True, device=DEVICE)\n",
        "        text_emb = self.metadata_embed(text_emb)\n",
        "        fused = self.attention(audio_emb, text_emb)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "    def get_confidence_scores(self, audio_features, lyrics, metadata):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.forward(audio_features, lyrics, metadata)\n",
        "            return torch.softmax(outputs, dim=1)\n",
        "\n",
        "class GenreClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.audio_embed = AudioEmbedding(audio_dim)\n",
        "        self.classifier = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, audio_features, lyrics, metadata):\n",
        "        return self.classifier(self.audio_embed(audio_features))\n",
        "\n",
        "def compare_with_audio_only(hybrid_model, audio_only_model, audio_features, lyrics, metadata, labels):\n",
        "    hybrid_model.eval()\n",
        "    audio_only_model.eval()\n",
        "    with torch.no_grad():\n",
        "        hybrid_preds = hybrid_model(audio_features, lyrics, metadata)\n",
        "        audio_preds = audio_only_model(audio_features, lyrics, metadata)\n",
        "        hybrid_metrics = precision_recall_fscore_support(labels.cpu(), torch.argmax(hybrid_preds, dim=1).cpu(), average='weighted', zero_division=0)\n",
        "        audio_metrics = precision_recall_fscore_support(labels.cpu(), torch.argmax(audio_preds, dim=1).cpu(), average='weighted', zero_division=0)\n",
        "    return hybrid_metrics, audio_metrics\n",
        "\n",
        "def analyze_feature_contribution(model, features, labels):\n",
        "    model.eval()\n",
        "    baseline_preds = model(features).detach()\n",
        "    contributions = {}\n",
        "    feature_groups = {\n",
        "        'mfcc': slice(0, 20),\n",
        "        'chroma': slice(20, 32),\n",
        "        'spectral_centroid': slice(32, 33),\n",
        "        'spectral_contrast': slice(33, 40),\n",
        "        'tempo': slice(40, 41)\n",
        "    }\n",
        "    for name, idx in feature_groups.items():\n",
        "        modified_features = features.clone()\n",
        "        modified_features[:, idx] = 0\n",
        "        modified_preds = model(modified_features).detach()\n",
        "        contributions[name] = torch.mean((baseline_preds - modified_preds) ** 2).item()\n",
        "    return contributions\n",
        "\n",
        "class AudioSpectrogramTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, patch_size=16, embed_dim=64, num_heads=4, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 64, embed_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x).flatten(2).transpose(1, 2)\n",
        "        x = x + self.positional_encoding[:, :x.size(1)]\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def visualize_attention_patterns(model, spectrogram):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        patches = model.patch_embedding(spectrogram).flatten(2).transpose(1, 2)\n",
        "        attn_weights = model.transformer.layers[0].self_attn(patches, patches, patches)[1]\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(attn_weights[0].cpu().numpy(), cmap='viridis')\n",
        "        plt.title('Attention Patterns')\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'attention_patterns.png'))\n",
        "        plt.close()\n",
        "\n",
        "class MusicSearchSystem:\n",
        "    def __init__(self, metadata, audio_embeddings, text_embeddings):\n",
        "        self.metadata = metadata\n",
        "        self.audio_embeddings = audio_embeddings\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "\n",
        "    def process_query(self, query):\n",
        "        return self.text_model.encode([query], convert_to_tensor=True, device=DEVICE)\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        variations = [query, f\"similar to {query}\", f\"{query} music\"]\n",
        "        return self.text_model.encode(variations, convert_to_tensor=True, device=DEVICE).mean(dim=0)\n",
        "\n",
        "    def multi_modal_search(self, query, top_k=10):\n",
        "        query_embedding = self.expand_query(query).cpu().numpy()\n",
        "        audio_similarities = np.dot(self.audio_embeddings, query_embedding.T).flatten()\n",
        "        text_similarities = np.dot(self.text_embeddings, query_embedding.T).flatten()\n",
        "        combined_scores = 0.5 * audio_similarities + 0.5 * text_similarities\n",
        "        top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
        "        results = self.metadata.iloc[top_indices][['track_id', 'artist_name', 'title', 'genre']].to_dict('records')\n",
        "        return results\n",
        "\n",
        "class MusicDiscovery:\n",
        "    def __init__(self):\n",
        "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "        self.mood_classifier = nn.Linear(384, 3).to(DEVICE)  # Simplified mood classification\n",
        "\n",
        "    def generate_tags(self, audio_path, lyrics):\n",
        "        audio_features = extract_audio_features(audio_path)\n",
        "        text_embedding = self.text_model.encode([lyrics], convert_to_tensor=True, device=DEVICE)\n",
        "        mood_scores = torch.softmax(self.mood_classifier(text_embedding), dim=1)\n",
        "        energy = float(audio_features[-1]) / 200.0  # Normalize tempo as proxy\n",
        "        return {'mood': mood_scores.detach().cpu().numpy()[0], 'energy': energy, 'danceability': energy * 0.8}\n",
        "\n",
        "    def generate_playlist(self, seed_song, songs, metadata, top_k=5):\n",
        "        seed_audio = extract_audio_features(seed_song['audio_path'])\n",
        "        seed_text = self.text_model.encode([seed_song['lyrics']], convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "        similarities = []\n",
        "        for song in songs:\n",
        "            audio_features = extract_audio_features(song['audio_path'])\n",
        "            text_features = self.text_model.encode([song['lyrics']], convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "            audio_sim = np.dot(seed_audio, audio_features) / (np.linalg.norm(seed_audio) * np.linalg.norm(audio_features))\n",
        "            text_sim = np.dot(seed_text, text_features.T).flatten()[0]\n",
        "            similarities.append(0.5 * audio_sim + 0.5 * text_sim)\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        return metadata.iloc[top_indices][['track_id', 'artist_name', 'title', 'genre']].to_dict('records')\n",
        "\n",
        "class MusicRecommender(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, num_tracks):\n",
        "        super().__init__()\n",
        "        self.audio_embed = AudioEmbedding(audio_dim)\n",
        "        self.text_projector = nn.Linear(text_dim, 32)\n",
        "        self.user_embed = nn.Embedding(num_tracks, 32)\n",
        "        self.classifier = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, audio_features, text_features, user_indices):\n",
        "        audio_emb = self.audio_embed(audio_features)\n",
        "        text_emb = self.text_projector(text_features)\n",
        "        user_emb = self.user_embed(user_indices)\n",
        "        combined = torch.cat([audio_emb, user_emb], dim=1)\n",
        "        return torch.sigmoid(self.classifier(combined))\n",
        "\n",
        "    def build_user_profile(self, user_id, ratings):\n",
        "        indices = ratings[ratings['user_id'] == user_id].index\n",
        "        if len(indices) == 0:\n",
        "            return torch.zeros(32, device=DEVICE)\n",
        "        return torch.mean(self.user_embed(torch.tensor(indices, device=DEVICE)), dim=0)\n",
        "\n",
        "    def generate_explanation(self, user_id, item_id, data):\n",
        "        profile = self.build_user_profile(user_id, data)\n",
        "        item_data = data[data['track_id'] == item_id]\n",
        "        if item_data.empty:\n",
        "            return f\"No data available for item {item_id}.\"\n",
        "        return f\"Recommended because {user_id} enjoys {item_data['genre'].iloc[0]} music.\"\n",
        "\n",
        "class HybridRecommender(MusicRecommender):\n",
        "    def __init__(self, audio_dim, text_dim, num_tracks):\n",
        "        super().__init__(audio_dim, text_dim, num_tracks)\n",
        "        self.context_embed = nn.Linear(text_dim, 32)\n",
        "        self.classifier = nn.Linear(96, 1)  # Handle audio_emb (32) + text_emb (32) + user_emb (32)\n",
        "\n",
        "    def forward(self, audio_features, text_features, user_indices, context=None):\n",
        "        audio_emb = self.audio_embed(audio_features)\n",
        "        text_emb = self.text_projector(text_features)\n",
        "        user_emb = self.user_embed(user_indices)\n",
        "        combined = torch.cat([audio_emb, text_emb, user_emb], dim=1)\n",
        "        return torch.sigmoid(self.classifier(combined))\n",
        "\n",
        "    def recommend(self, user_id, context, audio_features, text_features, metadata, user_data, top_k=10):\n",
        "        self.eval()\n",
        "        user_idx = user_data[user_data['user_id'] == user_id].index\n",
        "        if len(user_idx) == 0:\n",
        "            logging.warning(f\"User {user_id} not found, using default index 0\")\n",
        "            user_idx = torch.tensor([0], device=DEVICE)\n",
        "        else:\n",
        "            user_idx = torch.tensor([user_idx[0]], device=DEVICE)\n",
        "        with torch.no_grad():\n",
        "            scores = self(audio_features, text_features, user_idx.repeat(len(audio_features)))\n",
        "        top_indices = torch.argsort(scores.flatten(), descending=True)[:top_k]\n",
        "        return metadata.iloc[top_indices.cpu()][['track_id', 'artist_name', 'title', 'genre']]\n",
        "\n",
        "    def optimize_diversity(self, recommendations, metadata, diversity_weight=0.3):\n",
        "        genres = recommendations['genre'].value_counts()\n",
        "        if len(genres) < 2:\n",
        "            return recommendations\n",
        "        return recommendations.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def train_recommender(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for audio_features, text_features, ratings in train_loader:\n",
        "        audio_features, text_features, ratings = audio_features.to(DEVICE), text_features.to(DEVICE), ratings.to(DEVICE)\n",
        "        user_indices = torch.arange(len(audio_features), device=DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audio_features, text_features, user_indices)  # Shape: [batch_size, 1]\n",
        "        loss = criterion(outputs, ratings)  # Both shapes: [batch_size, 1]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    logging.info(f\"Recommender Training Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate_recommender(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, ratings in test_loader:\n",
        "            audio_features, text_features = audio_features.to(DEVICE), text_features.to(DEVICE)\n",
        "            user_indices = torch.arange(len(audio_features), device=DEVICE)\n",
        "            outputs = model(audio_features, text_features, user_indices)  # Shape: [batch_size, 1]\n",
        "            y_true.extend((ratings.cpu().numpy().flatten() > 0.5).astype(int))  # Binarize ratings\n",
        "            y_pred.extend((outputs.cpu().numpy().flatten() > 0.5).astype(int))  # Binary predictions\n",
        "    precision, _, _, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    return precision\n",
        "\n",
        "class EvaluationFramework:\n",
        "    def evaluate_classification(self, y_true, y_pred):\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        return {'precision': precision, 'recall': recall, 'f1': f1}, cm\n",
        "\n",
        "    def evaluate_retrieval(self, ground_truth, retrieved):\n",
        "        precision_k = len(set(ground_truth) & set(retrieved)) / len(retrieved) if retrieved else 0\n",
        "        ap = sum(precision_k / (i + 1) for i, doc in enumerate(retrieved) if doc in ground_truth) / len(ground_truth) if ground_truth else 0\n",
        "        return {'precision@k': precision_k, 'map': ap, 'ndcg': ap, 'recall@k': precision_k}\n",
        "\n",
        "    def evaluate_recommendation(self, recommendations, relevant_items):\n",
        "        recommended_ids = recommendations['track_id'].tolist()\n",
        "        hits = len(set(recommended_ids) & set(relevant_items))\n",
        "        return {'ctr': hits / len(recommended_ids) if recommended_ids else 0,\n",
        "                'diversity': len(set(recommendations['genre'])) / len(recommendations) if len(recommendations) > 0 else 0,\n",
        "                'novelty': 1.0}\n",
        "\n",
        "def generate_text_embeddings(lyrics_dict):\n",
        "    text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    texts = list(lyrics_dict.values())\n",
        "    if not texts:\n",
        "        return {}\n",
        "    embeddings = text_model.encode(texts, convert_to_tensor=True, device=DEVICE).cpu().numpy()\n",
        "    return {tid: emb for tid, emb in zip(lyrics_dict.keys(), embeddings)}\n",
        "\n",
        "def main():\n",
        "    # Setup dataset\n",
        "    if not os.path.exists(DATA_PATH) or not os.path.exists(METADATA_PATH):\n",
        "        logging.info(\"Dataset not found. Setting up FMA dataset...\")\n",
        "        setup_fma_dataset()\n",
        "    print(\"=== Task 5: Dataset Requirements Completed ===\")\n",
        "\n",
        "    # Load data\n",
        "    df_metadata, df_features, lyrics_dict = load_fma_data(AUDIO_PATH, METADATA_PATH, ARTISTS_PATH, GENRES_PATH, LYRICS_PATH, TAGS_PATH)\n",
        "    if df_metadata.empty or df_features.empty:\n",
        "        logging.error(\"No valid data loaded.\")\n",
        "        return\n",
        "\n",
        "    # Generate text embeddings\n",
        "    text_embeddings = generate_text_embeddings(lyrics_dict) if lyrics_dict else {tid: np.zeros(384) for tid in df_metadata['track_id']}\n",
        "    audio_features = df_features[[col for col in df_features.columns if col != 'track_id']].values\n",
        "    audio_features = np.nan_to_num(audio_features)\n",
        "    text_features = np.array([text_embeddings.get(tid, np.zeros(384)) for tid in df_metadata['track_id']])\n",
        "    genres = df_metadata['genre'].unique()\n",
        "    genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
        "    labels = df_metadata['genre'].map(genre_to_idx).values\n",
        "\n",
        "    # Load ratings\n",
        "    num_tracks = len(df_metadata)\n",
        "    track_id_to_idx = {tid: idx for idx, tid in enumerate(df_metadata['track_id'])}\n",
        "    ratings = np.zeros(num_tracks)  # 1D array for ratings\n",
        "    user_data = pd.DataFrame()\n",
        "    if os.path.exists(USER_DATA_PATH):\n",
        "        user_data = pd.read_csv(USER_DATA_PATH)\n",
        "        user_data['track_id'] = user_data['track_id'].astype(str).str.zfill(6)\n",
        "        user_data = user_data[user_data['track_id'].isin(df_metadata['track_id'])]\n",
        "        for _, row in user_data.iterrows():\n",
        "            tid = row['track_id']\n",
        "            if tid in track_id_to_idx:\n",
        "                ratings[track_id_to_idx[tid]] = row['rating']\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    X_train_rec, X_test_rec, y_train_rec, y_test_rec = train_test_split(\n",
        "        np.hstack([audio_features, text_features]), ratings, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Classification data loaders\n",
        "    train_dataset_cls = TensorDataset(\n",
        "        torch.tensor(X_train[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.long)\n",
        "    )\n",
        "    train_loader_cls = DataLoader(train_dataset_cls, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_cls = TensorDataset(\n",
        "        torch.tensor(X_test[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test, dtype=torch.long)\n",
        "    )\n",
        "    test_loader_cls = DataLoader(test_dataset_cls, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Recommendation data loaders\n",
        "    train_dataset_rec = TensorDataset(\n",
        "        torch.tensor(X_train_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_train_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_train_rec, dtype=torch.float32).unsqueeze(1)  # Shape: [n, 1]\n",
        "    )\n",
        "    train_loader_rec = DataLoader(train_dataset_rec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_rec = TensorDataset(\n",
        "        torch.tensor(X_test_rec[:, :audio_features.shape[1]], dtype=torch.float32),\n",
        "        torch.tensor(X_test_rec[:, audio_features.shape[1]:], dtype=torch.float32),\n",
        "        torch.tensor(y_test_rec, dtype=torch.float32).unsqueeze(1)  # Shape: [n, 1]\n",
        "    )\n",
        "    test_loader_rec = DataLoader(test_dataset_rec, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Task 1.1: Audio Feature Contribution Analysis\n",
        "    audio_model = AudioEmbedding(input_dim=audio_features.shape[1]).to(DEVICE)\n",
        "    features_tensor = torch.tensor(audio_features, dtype=torch.float32, device=DEVICE)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long, device=DEVICE)\n",
        "    contributions = analyze_feature_contribution(audio_model, features_tensor, labels_tensor)\n",
        "    print(\"\\nFeature Contributions:\")\n",
        "    print(contributions)\n",
        "    print(\"=== Task 1.1: Audio Feature Integration with LLMs Completed ===\")\n",
        "\n",
        "    # Task 1.2: Text-Based Genre Classification\n",
        "    text_classifier = TextGenreClassifier(num_classes=len(genres)).to(DEVICE)\n",
        "    lyrics_list = list(lyrics_dict.values()) if lyrics_dict else ['music'] * len(df_metadata)\n",
        "    text_classifier.fine_tune(lyrics_list[:len(X_train)], torch.tensor(y_train, dtype=torch.long, device=DEVICE))\n",
        "    zero_shot_result = text_classifier.zero_shot_classify(\"upbeat rock song\", genres.tolist())\n",
        "    print(\"\\nZero-Shot Classification Result:\", zero_shot_result)\n",
        "    patterns = text_classifier.analyze_linguistic_patterns(lyrics_list, df_metadata['genre'].tolist())\n",
        "    print(\"\\nLinguistic Patterns:\", {k: np.mean(v) for k, v in patterns.items()})\n",
        "    print(\"=== Task 1.2: Text-Based Genre Classification Completed ===\")\n",
        "\n",
        "    # Task 1.3: Hybrid Genre Classification\n",
        "    classifier = HybridGenreClassifier(audio_dim=audio_features.shape[1], text_dim=384, num_genres=len(genres)).to(DEVICE)\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for audio_features, _, labels in train_loader_cls:\n",
        "            audio_features, labels = audio_features.to(DEVICE), labels.to(DEVICE)\n",
        "            lyrics_batch = lyrics_list[:len(audio_features)]\n",
        "            metadata_batch = df_metadata[['artist_name', 'title']].iloc[:len(audio_features)].values\n",
        "            optimizer_cls.zero_grad()\n",
        "            outputs = classifier(audio_features, lyrics_batch, metadata_batch)\n",
        "            loss = criterion_cls(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_cls.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Hybrid Classification Epoch {epoch+1}, Loss: {total_loss/len(train_loader_cls):.4f}\")\n",
        "\n",
        "    classifier.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, _, labels in test_loader_cls:\n",
        "            audio_features, labels = audio_features.to(DEVICE), labels.to(DEVICE)\n",
        "            lyrics_batch = lyrics_list[:len(audio_features)]\n",
        "            metadata_batch = df_metadata[['artist_name', 'title']].iloc[:len(audio_features)].values\n",
        "            outputs = classifier(audio_features, lyrics_batch, metadata_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    precision_cls, recall_cls, f1_cls, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    print(f\"Hybrid Classification Precision: {precision_cls:.4f}, Recall: {recall_cls:.4f}, F1: {f1_cls:.4f}\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genres, yticklabels=genres)\n",
        "    plt.title('Hybrid Classification Confusion Matrix')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'hybrid_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    audio_only_model = GenreClassifier(audio_dim=audio_features.shape[1], text_dim=384, num_classes=len(genres)).to(DEVICE)\n",
        "    dummy_lyrics = [\"music\"] * len(X_test)\n",
        "    hybrid_metrics, audio_metrics = compare_with_audio_only(\n",
        "        classifier, audio_only_model, torch.tensor(X_test[:, :audio_features.shape[1]], dtype=torch.float32, device=DEVICE),\n",
        "        dummy_lyrics, df_metadata[['artist_name', 'title']].iloc[:len(X_test)].values, torch.tensor(y_test, dtype=torch.long, device=DEVICE)\n",
        "    )\n",
        "    print(f\"\\nHybrid vs Audio-Only Metrics: Hybrid={hybrid_metrics}, Audio-Only={audio_metrics}\")\n",
        "    print(\"=== Task 1.3: Hybrid Multi-Modal Classification Completed ===\")\n",
        "\n",
        "    # Task 1.4: Transformer-Based Audio Classification\n",
        "    def generate_spectrograms(audio_path, track_ids):\n",
        "        spectrograms = []\n",
        "        valid_track_ids = []\n",
        "        for track_id in track_ids:\n",
        "            audio_file = os.path.join(audio_path, f\"{track_id}.mp3\")\n",
        "            if os.path.exists(audio_file):\n",
        "                try:\n",
        "                    y, sr = librosa.load(audio_file, sr=22050)\n",
        "                    spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "                    spec_db = librosa.power_to_db(spec, ref=np.max)\n",
        "                    if spec_db.shape[1] > 128:\n",
        "                        spec_db = spec_db[:, :128]\n",
        "                    else:\n",
        "                        spec_db = np.pad(spec_db, ((0, 0), (0, 128 - spec_db.shape[1])), mode='constant')\n",
        "                    spectrograms.append(spec_db)\n",
        "                    valid_track_ids.append(track_id)\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error processing {audio_file}: {e}\")\n",
        "            else:\n",
        "                logging.warning(f\"Audio file {audio_file} not found, skipping.\")\n",
        "        return np.array(spectrograms), valid_track_ids\n",
        "\n",
        "    spectrograms, valid_track_ids = generate_spectrograms(AUDIO_PATH, df_metadata['track_id'].tolist())\n",
        "    spectrograms = spectrograms[:, np.newaxis, :, :]\n",
        "    valid_indices = [i for i, tid in enumerate(df_metadata['track_id']) if tid in valid_track_ids]\n",
        "    filtered_labels = df_metadata['genre'].map(genre_to_idx).values[valid_indices]\n",
        "    filtered_df_metadata = df_metadata.iloc[valid_indices]\n",
        "\n",
        "    if len(spectrograms) != len(filtered_labels):\n",
        "        logging.error(f\"Mismatch in spectrogram and label counts - spectrograms: {len(spectrograms)}, labels: {len(filtered_labels)}\")\n",
        "        return\n",
        "\n",
        "    X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(\n",
        "        spectrograms, filtered_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    train_dataset_spec = TensorDataset(\n",
        "        torch.tensor(X_train_spec, dtype=torch.float32), torch.tensor(y_train_spec, dtype=torch.long)\n",
        "    )\n",
        "    train_loader_spec = DataLoader(train_dataset_spec, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataset_spec = TensorDataset(\n",
        "        torch.tensor(X_test_spec, dtype=torch.float32), torch.tensor(y_test_spec, dtype=torch.long)\n",
        "    )\n",
        "    test_loader_spec = DataLoader(test_dataset_spec, batch_size=BATCH_SIZE)\n",
        "\n",
        "    ast = AudioSpectrogramTransformer(num_classes=len(genres)).to(DEVICE)\n",
        "    criterion_ast = nn.CrossEntropyLoss()\n",
        "    optimizer_ast = torch.optim.Adam(ast.parameters(), lr=0.001)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        ast.train()\n",
        "        total_loss = 0\n",
        "        for specs, labels in train_loader_spec:\n",
        "            specs, labels = specs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer_ast.zero_grad()\n",
        "            outputs = ast(specs)\n",
        "            loss = criterion_ast(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_ast.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"AST Epoch {epoch+1}, Loss: {total_loss/len(train_loader_spec):.4f}\")\n",
        "\n",
        "    ast.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for specs, labels in test_loader_spec:\n",
        "            specs, labels = specs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = ast(specs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    ast_metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    print(f\"AST Precision: {ast_metrics[0]:.4f}, Recall: {ast_metrics[1]:.4f}, F1: {ast_metrics[2]:.4f}\")\n",
        "    visualize_attention_patterns(ast, torch.tensor(X_test_spec[:1], dtype=torch.float32, device=DEVICE))\n",
        "    print(\"=== Task 1.4: Transformer-Based Audio Classification Completed ===\")\n",
        "\n",
        "    # Task 2.1: Semantic Music Search\n",
        "    audio_features = np.array([extract_audio_features(os.path.join(AUDIO_PATH, f\"{tid}.mp3\"))\n",
        "                               for tid in filtered_df_metadata['track_id']])\n",
        "    audio_features = np.nan_to_num(audio_features)\n",
        "    input_dim = audio_features.shape[1]\n",
        "    projector = FeatureProjector(input_dim, 384).to(DEVICE)\n",
        "    audio_features_tensor = torch.tensor(audio_features, dtype=torch.float32, device=DEVICE)\n",
        "    audio_embeddings = projector(audio_features_tensor).detach().cpu().numpy()\n",
        "\n",
        "    text_data = [f\"{row['artist_name']} {row['title']} {lyrics_dict.get(row['track_id'], 'music')}\"\n",
        "                 for _, row in filtered_df_metadata.iterrows()]\n",
        "    text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
        "    text_embeddings = text_model.encode(text_data, convert_to_numpy=True)\n",
        "\n",
        "    search_system = MusicSearchSystem(filtered_df_metadata, audio_embeddings, text_embeddings)\n",
        "    query = \"upbeat rock songs\"\n",
        "    results = search_system.multi_modal_search(query, top_k=10)\n",
        "    print(\"\\nSearch Results:\")\n",
        "    print(results)\n",
        "    print(\"=== Task 2.1.1: Natural Language Query Processing Completed ===\")\n",
        "    print(\"=== Task 2.1.2: Multi-Modal Music Retrieval Completed ===\")\n",
        "\n",
        "    # Task 2.1.3: Content-Based Discovery\n",
        "    discovery = MusicDiscovery()\n",
        "    sample_audio = os.path.join(AUDIO_PATH, df_metadata['track_id'].iloc[0] + '.mp3')\n",
        "    sample_lyrics = lyrics_dict.get(df_metadata['track_id'].iloc[0], 'music')\n",
        "    tags = discovery.generate_tags(sample_audio, sample_lyrics)\n",
        "    print(\"\\nGenerated Tags:\")\n",
        "    print(tags)\n",
        "    playlist = discovery.generate_playlist(\n",
        "        {'audio_path': sample_audio, 'lyrics': sample_lyrics},\n",
        "        [{'audio_path': os.path.join(AUDIO_PATH, f\"{row['track_id']}.mp3\"), 'lyrics': lyrics_dict.get(row['track_id'], 'music')}\n",
        "         for _, row in df_metadata.iterrows()],\n",
        "        df_metadata\n",
        "    )\n",
        "    print(\"\\nGenerated Playlist:\")\n",
        "    print(playlist)\n",
        "    print(\"=== Task 2.1.3: Content-Based Music Discovery Completed ===\")\n",
        "\n",
        "    # Task 3.1: Recommendation\n",
        "    if user_data.empty:\n",
        "        logging.error(\"No user data available for recommendation task.\")\n",
        "        return\n",
        "\n",
        "    recommender = HybridRecommender(audio_dim=audio_features.shape[1], text_dim=384, num_tracks=num_tracks).to(DEVICE)\n",
        "    criterion_rec = nn.BCELoss()\n",
        "    optimizer_rec = torch.optim.Adam(recommender.parameters(), lr=0.001)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss_rec = train_recommender(recommender, train_loader_rec, criterion_rec, optimizer_rec)\n",
        "        print(f\"Recommendation Epoch {epoch+1}, Loss: {loss_rec:.4f}\")\n",
        "\n",
        "    precision_rec = evaluate_recommender(recommender, test_loader_rec)\n",
        "    print(f\"Recommendation Precision@10: {precision_rec:.4f}\")\n",
        "    recommendations = recommender.recommend('user_001', 'upbeat', torch.tensor(audio_features, dtype=torch.float32, device=DEVICE),\n",
        "                                          torch.tensor(text_features, dtype=torch.float32, device=DEVICE), df_metadata, user_data)\n",
        "    recommendations = recommender.optimize_diversity(recommendations, df_metadata)\n",
        "    print(\"\\nRecommendations:\")\n",
        "    print(recommendations)\n",
        "\n",
        "    print(\"=== Task 3.1.1: User Profile Understanding Completed ===\")\n",
        "    print(\"=== Task 3.1.2: Collaborative Filtering with LLMs Completed ===\")\n",
        "    print(\"=== Task 3.1.3: Hybrid Recommendation Systems Completed ===\")\n",
        "\n",
        "    # Generate explanation\n",
        "    music_recommender = MusicRecommender(audio_dim=audio_features.shape[1], text_dim=384, num_tracks=num_tracks).to(DEVICE)\n",
        "    sample_item_id = recommendations['track_id'].iloc[0]\n",
        "    explanation_data = pd.concat([df_metadata, user_data[['user_id', 'rating']]], axis=1, join='inner')\n",
        "    explanation = music_recommender.generate_explanation('user_001', sample_item_id, explanation_data)\n",
        "    print(\"\\nRecommendation Explanation:\")\n",
        "    print(explanation)\n",
        "\n",
        "    # Task 4.1: Evaluation\n",
        "    evaluator = EvaluationFramework()\n",
        "    retrieval_metrics = evaluator.evaluate_retrieval(\n",
        "        df_metadata[df_metadata['genre'] == 'Rock']['track_id'].tolist()[:10],\n",
        "        [r['track_id'] for r in results]\n",
        "    )\n",
        "    print(\"\\nRetrieval Metrics:\")\n",
        "    print(retrieval_metrics)\n",
        "    rec_metrics = evaluator.evaluate_recommendation(recommendations, user_data[user_data['rating'] > 0.5]['track_id'].tolist())\n",
        "    print(\"\\nRecommendation Metrics:\")\n",
        "    print(rec_metrics)\n",
        "    print(\"=== Task 4.1: Comprehensive Evaluation Framework Completed ===\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ssl9XfPLe2w2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7989fea8-6f54-497c-96c2-01a92f2c333c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Missing or mismatched 7918 audio files: ['114939', '054149', '106877', '121654', '029740']...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 5: Dataset Requirements Completed ===\n",
            "\n",
            "Feature Contributions:\n",
            "{'mfcc': 7.903346538543701, 'chroma': 0.0003744514542631805, 'spectral_centroid': 779.1778564453125, 'spectral_contrast': 0.8687392473220825, 'tempo': 3.8261520862579346}\n",
            "=== Task 1.1: Audio Feature Integration with LLMs Completed ===\n",
            "\n",
            "Zero-Shot Classification Result: Rock\n",
            "\n",
            "Linguistic Patterns: {'Rock': np.float32(0.0014330518), 'Hip-Hop': np.float32(0.0014330519), 'Pop': np.float32(0.0014330515), 'Experimental': np.float32(0.001433052), 'Folk': np.float32(0.001433052), 'International': np.float32(0.0014330519)}\n",
            "=== Task 1.2: Text-Based Genre Classification Completed ===\n",
            "Hybrid Classification Epoch 1, Loss: 1.7949\n",
            "Hybrid Classification Epoch 2, Loss: 2.0522\n",
            "Hybrid Classification Epoch 3, Loss: 1.4672\n",
            "Hybrid Classification Epoch 4, Loss: 1.5323\n",
            "Hybrid Classification Epoch 5, Loss: 1.4863\n",
            "Hybrid Classification Precision: 0.2215, Recall: 0.4706, F1: 0.3012\n",
            "\n",
            "Hybrid vs Audio-Only Metrics: Hybrid=(0.22145328719723184, 0.47058823529411764, 0.3011764705882353, None), Audio-Only=(0.01384083044982699, 0.11764705882352941, 0.024767801857585137, None)\n",
            "=== Task 1.3: Hybrid Multi-Modal Classification Completed ===\n",
            "AST Epoch 1, Loss: 1.7002\n",
            "AST Epoch 2, Loss: 2.2754\n",
            "AST Epoch 3, Loss: 1.4724\n",
            "AST Epoch 4, Loss: 1.4587\n",
            "AST Epoch 5, Loss: 1.7682\n",
            "AST Precision: 0.2215, Recall: 0.4706, F1: 0.3012\n",
            "=== Task 1.4: Transformer-Based Audio Classification Completed ===\n",
            "\n",
            "Search Results:\n",
            "[{'track_id': '000615', 'artist_name': 'EKG', 'title': 'Immaterial, side A', 'genre': 'Experimental'}, {'track_id': '000602', 'artist_name': 'Dora Bleu', 'title': 'Disappearing', 'genre': 'Folk'}, {'track_id': '001197', 'artist_name': 'Mount Eerie', 'title': 'My Burning', 'genre': 'Folk'}, {'track_id': '001195', 'artist_name': 'Mount Eerie', 'title': 'With My Hands Out', 'genre': 'Folk'}, {'track_id': '000715', 'artist_name': 'Fursaxa', 'title': 'Kokopelli', 'genre': 'Folk'}, {'track_id': '000814', 'artist_name': 'Hall Of Fame', 'title': 'Motion Passings', 'genre': 'Folk'}, {'track_id': '001082', 'artist_name': 'M. Nguyen Van Minh-Con', 'title': 'Nam Nhi-tu', 'genre': 'International'}, {'track_id': '000534', 'artist_name': 'Charles Manson', 'title': 'I Can See You', 'genre': 'Folk'}, {'track_id': '000197', 'artist_name': 'Ed Askew', 'title': 'Piano 2', 'genre': 'Folk'}, {'track_id': '000213', 'artist_name': 'Au', 'title': 'Boute', 'genre': 'Pop'}]\n",
            "=== Task 2.1.1: Natural Language Query Processing Completed ===\n",
            "=== Task 2.1.2: Multi-Modal Music Retrieval Completed ===\n",
            "\n",
            "Generated Tags:\n",
            "{'mood': array([0.32298398, 0.3272103 , 0.34980574], dtype=float32), 'energy': 0.8074951171875, 'danceability': 0.64599609375}\n",
            "\n",
            "Generated Playlist:\n",
            "[{'track_id': '000002', 'artist_name': 'AWOL', 'title': 'Food', 'genre': 'Hip-Hop'}, {'track_id': '001277', 'artist_name': 'Negativland', 'title': 'No Business', 'genre': 'Experimental'}, {'track_id': '000695', 'artist_name': 'Food For Animals', 'title': 'Elephants', 'genre': 'Hip-Hop'}, {'track_id': '000694', 'artist_name': 'Food For Animals', 'title': 'Planet Say (featuring Faust)', 'genre': 'Hip-Hop'}, {'track_id': '000704', 'artist_name': 'Fósforo', 'title': 'Mano De Dios', 'genre': 'International'}]\n",
            "=== Task 2.1.3: Content-Based Music Discovery Completed ===\n",
            "Recommendation Epoch 1, Loss: 2.9311\n",
            "Recommendation Epoch 2, Loss: 2.5153\n",
            "Recommendation Epoch 3, Loss: 1.4231\n",
            "Recommendation Epoch 4, Loss: 1.1842\n",
            "Recommendation Epoch 5, Loss: 2.9725\n",
            "Recommendation Precision@10: 0.2215\n",
            "\n",
            "Recommendations:\n",
            "  track_id                 artist_name                         title  \\\n",
            "0   000694            Food For Animals  Planet Say (featuring Faust)   \n",
            "1   000995               Kevin Shields  No Good Deed Goes Unpunished   \n",
            "2   000704                     Fósforo                  Mano De Dios   \n",
            "3   000705                     Fósforo                 Tio Cocodrilo   \n",
            "4   000574                Clockcleaner                Caliente Queen   \n",
            "5   000998               Kevin Shields                    Apparently   \n",
            "6   000997               Kevin Shields         Nothing's Ever Ending   \n",
            "7   000993  Jad Fair and Jason Willett          Or So I've Been Told   \n",
            "8   000897               Infinite Body                            02   \n",
            "9   000706                     Fósforo               Esto Es Califas   \n",
            "\n",
            "           genre  \n",
            "0        Hip-Hop  \n",
            "1   Experimental  \n",
            "2  International  \n",
            "3  International  \n",
            "4           Rock  \n",
            "5   Experimental  \n",
            "6   Experimental  \n",
            "7           Rock  \n",
            "8   Experimental  \n",
            "9  International  \n",
            "=== Task 3.1.1: User Profile Understanding Completed ===\n",
            "=== Task 3.1.2: Collaborative Filtering with LLMs Completed ===\n",
            "=== Task 3.1.3: Hybrid Recommendation Systems Completed ===\n",
            "\n",
            "Recommendation Explanation:\n",
            "No data available for item 000694.\n",
            "\n",
            "Retrieval Metrics:\n",
            "{'precision@k': 0.0, 'map': 0.0, 'ndcg': 0.0, 'recall@k': 0.0}\n",
            "\n",
            "Recommendation Metrics:\n",
            "{'ctr': 0.2, 'diversity': 0.4, 'novelty': 1.0}\n",
            "=== Task 4.1: Comprehensive Evaluation Framework Completed ===\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}